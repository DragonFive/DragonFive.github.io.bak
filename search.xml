<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CI从入门到放弃]]></title>
    <url>%2F2018-03-20%2Fcitest%2F</url>
    <content type="text"><![CDATA[呵呵哒嘿嘿嘿 哼哼哼 CADCAD]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>梯度下降法</tag>
        <tag>正则化</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow学习笔记]]></title>
    <url>%2F2017-11-20%2Ftensorflow_note%2F</url>
    <content type="text"><![CDATA[matplotlib.pyplot 的使用：12345678import matplotlibmatplotlib.use('Agg')import matplotlib.pyplot as plt fig = plt.figure()......plt.plot()fig.savefig("") referencegoogle group, kl 散度 讨论 KL Divergence in TensorFlow]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉问题小结]]></title>
    <url>%2F2017-11-05%2Fcomputer_vision%2F</url>
    <content type="text"><![CDATA[特征hoghistgram of gradient: 每个像素计算梯度方向和大小，然后8x8个像素组成一个cell, cell 之间不重叠，冲击cell里面各像素的梯度方向脂肪像，每20度算一个方向，累加这些方向上的梯度值得到9维的特征。然后每四个cell组成一个block，block可以重叠，一个block的特征向量是四个cell特征串联起来得到4x9=36维的特征。所有的block再合起来就得到整张图的hog特征 HOG特征算法 haar特征深度学习调参技巧调参优先级按照对结果的影响调整： 调整学习率 调整momentum一般为0.9 ，调整层数和minibatch size 学习率衰减率/adam的beta1(0.9) beta2(0.999),gama(10^(-8) ) 调参策略 不要使用网格法，而用随机选取法（不同参数对结果有不同的优先级） 又粗到细搜索参数 随机选取法不是均匀选取，而应该有一定的尺度，比如learning_rate的选择范围是0.0001～1,如果随机均匀选取，那么在0.0001～0.01采样数占整体的1%， 这时候应该用log形式的采样方法，如 12r = -4*np.random.rand()alpha = np.square(10,r) 那么alpha的取值范围在10^-4~1之间，且从0.0001到0.001之间样本概率是1/4 熊猫调参法，鱼子酱调参法与计算资源数量有关 BN层的真相BN层最原始的做法是将数据映射成均值为0方差为1的分布，但其实后面还有操作，gamma,beta两个参数来映射到其它的分布。所以有BN则前面的卷积层就不需要偏置项了。 BN层相当于在训练中的一个正则化，因为使用mini batch算出的平均值和方差跟整体的不一样，相当于加了一些噪声。所以batch_size越大，这个正则化的效果就不明显。 在测试阶段没有平均值和方差，就使用指数平均的 方式来计算。 为什么训练无法收敛一直保持 Loss: 2.303, Train acc 0.10, Test acc 0.10 可以从以下几个参数考虑：第一层的kernel_size，batch_size，learning_rate等 训练的基本流程 数据处理 定义网络结构 配置solver参数 训练：如 caffe train -solver solver.prototxt -gpu 0 冻结层冻结一层不参与训练：设置其blobs_lr=0 caffe训练时Loss变为nan的原因1. 梯度爆炸判定方法： 观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。 解决方法： 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight 2. 不当的输入原因：输入中就含有NaN 措施：重整你的数据集，确保训练集和验证集里面没有损坏的图片。 为什么Caffe中引入了这个inner_num，inner_num等于什么从FCN全卷积网络的方向去思考。FCN中label标签长度=图片尺寸caffe引入inner_num使得输入image的size可以是任意大小，innuer_num大小即为softmax层输入的heightxwidth BatchNorm层是否支持in place运算，为什么？BN是对输入那一层做归一化操作，要对每个元素-均值/标准差，且输入输出规格相当，是可以进行in place softmax问题在实际使用中，efj 常常因为指数太大而出现数值爆炸问题，两个非常大的数相除会出现数值不稳定问题，因此我们需要在分子和分母中同时进行以下处理： $$\frac{e^{f_{y_i}}}{\sum_j e^{f_j}} = \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}} = \frac{e^{f_{y_i}+logC}}{\sum_j e^{f_j+logC}}$$ 其中C 的设置是任意的，在实际变成中，往往把C设置成 $$logC = -max f_j$$ caffe+报错︱深度学习参数调优杂记+caffe训练时的问题+dropout/batch Normalization]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
        <tag>神经网络</tag>
        <tag>特征</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gluon学习笔记]]></title>
    <url>%2F2017-10-20%2Fball_detection%2F</url>
    <content type="text"><![CDATA[学到的新知识bn放在relu后面BN应该放在relu后 用于分类、检测和分割的移动网络 MobileNetV2 如何评价MobileNetV2 卷积核的数量卷积神经网络 — 从0开始 当输入数据有多个通道的时候，每个通道会有对应的权重，然后会对每个通道做卷积之后在通道之间求和。所以当输出只有一个的时候，卷积的channel数目和data的channel数目是一样的。 当输出需要多通道时，每个输出通道有对应权重，然后每个通道上做卷积。所以当输入有n个channel，输出有h个channel时，卷积核channel数目为n * h，每个输出channel对应一个bias ,卷积核的维度为(h,n,w,h) $$conv(data, w, b)[:,i,:,:] = conv(data, w[i,:,:,:], b[i])$$ gluon语法nn.Block与nn.sequential的嵌套使用12345678910111213141516class RecMLP(nn.Block): def __init__(self, **kwargs): super(RecMLP, self).__init__(**kwargs) self.net = nn.Sequential() with self.name_scope(): self.net.add(nn.Dense(256, activation="relu")) self.net.add(nn.Dense(128, activation="relu")) self.dense = nn.Dense(64) def forward(self, x): return nd.relu(self.dense(self.net(x)))rec_mlp = nn.Sequential()rec_mlp.add(RecMLP())rec_mlp.add(nn.Dense(10))print(rec_mlp) 初始化与参数访问123from mxnet import initparams.initialize(init=init.Normal(sigma=0.02), force_reinit=True)print(net[0].weight.data(), net[0].bias.data()) 我们也可以通过collect_params来访问Block里面所有的参数（这个会包括所有的子Block）。它会返回一个名字到对应Parameter的dict。 也可以自定义各层的初始化方法，没有自定义的按照net.initialize里面的方法进行定义12345678910111213141516from mxnet.gluon import nnfrom mxnet import ndfrom mxnet import initdef get_net(): net = nn.Sequential() with net.name_scope(): net.add(nn.Dense(4,activation="relu"))#,weight_initializer=init.Xavier())) net.add(nn.Dense(2,weight_initializer=init.Zero(),bias_initializer=init.Zero()) ) return netx = nd.random.uniform(shape=(3,5))net = get_net()net.initialize(init.One())net(x)print(net[1].weight.data GPU访问 删除cpu版本mxnet 1pip uninstall mxnet 更新GPU版本mxnet 1pip install -U --pre mxnet-cu80 查看版本号 123import pipfor pkg in ['mxnet', 'mxnet-cu75', 'mxnet-cu80']: pip.main(['show', pkg]) 使用jupyter的相关插件 notedown插件可以在jupyter 中查看markdown文件 nb_conda是conda的插件，可以在jupyter里面修改python内核版本 优化方法momentumgluon.Trainer的learning_rate属性和set_learning_rate函数可以随意调整学习率。12trainer = gluon.Trainer(net.collect_params(), 'sgd', &#123;'learning_rate': lr, 'momentum': mom&#125;) adagradAdagrad是一个在迭代过程中不断自我调整学习率，并让模型参数中每个元素都使用不同学习率的优化算法。12trainer = gluon.Trainer(net.collect_params(), 'adagrad', &#123;'learning_rate': lr&#125;) Adam 12trainer = gluon.Trainer(net.collect_params(), 'adam', &#123;'learning_rate': lr&#125;) 通过以上分析, 理论上可以说, 在数据比较稀疏的时候, adaptive 的方法能得到更好的效果, 例如, adagrad, adadelta, rmsprop, adam 等. 在数据稀疏的情况下, adam 方法也会比 rmsprop 方法收敛的结果要好一些, 所以, 通常在没有其它更好的理由的前框下, 我会选用 adam 方法, 可以比较快地得到一个预估结果. 但是, 在论文中, 我们看到的大部分还是最原始的 mini-batch 的 SGD 方法. 因为马鞍面的存在等问题, SGD 方法有时候较难收敛. 另外, SGD 对于参数的初始化要求也比较高. 所以, 如果要是想快速收敛的话, 建议使用 adam 这类 adaptive 的方法 延迟执行延后执行使得系统有更多空间来做性能优化。但我们推荐每个批量里至少有一个同步函数，例如对损失函数进行评估，来避免将过多任务同时丢进后端系统。1234567891011121314from mxnet import autogradmem = get_mem()total_loss = 0for x, y in get_data(): with autograd.record(): L = loss(y, net(x)) total_loss += L.sum().asscalar() L.backward() trainer.step(x.shape[0])nd.waitall()print('Increased memory %f MB' % (get_mem() - mem)) 多GPU训练123ctx = [gpu(i) for i in range(num_gpus)]data_list = gluon.utils.split_and_load(data, ctx)label_list = gluon.utils.split_and_load(label, ctx) fintune 微调gluon微调 一些可以重复使用的代码读取数据1234567from mxnet import gluonfrom mxnet import ndarray as nddef transform(data, label): return data.astype('float32')/255, label.astype('float32')mnist_train = gluon.data.vision.FashionMNIST(train=True, transform=transform)mnist_test = gluon.data.vision.FashionMNIST(train=False, transform=transform) 计算精度12def accuracy(output, label): return nd.mean(output.argmax(axis=1)==label).asscalar() 我们先使用Flatten层将输入数据转成 batch_size x ? 的矩阵，然后输入到10个输出节点的全连接层。照例我们不需要制定每层输入的大小，gluon会做自动推导。 激活函数sigmoid1234567from mxnet import nddef softmax(X): exp = nd.exp(X) # 假设exp是矩阵，这里对行进行求和，并要求保留axis 1， # 就是返回 (nrows, 1) 形状的矩阵 partition = exp.sum(axis=1, keepdims=True) return exp / partition relu12def relu(X): return nd.maximum(X, 0) 损失函数平方误差1square_loss = gluon.loss.L2Loss() 123def square_loss(yhat, y): # 注意这里我们把y变形成yhat的形状来避免矩阵形状的自动转换 return (yhat - y.reshape(yhat.shape)) ** 2 交叉熵损失 12def cross_entropy(yhat, y): return - nd.pick(nd.log(yhat), y) 1softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss() 取一个batch_size的代码scratch版本12345678import randombatch_size = 1def data_iter(num_examples): idx = list(range(num_examples)) random.shuffle(idx) for i in range(0, num_examples, batch_size): j = nd.array(idx[i:min(i+batch_size,num_examples)]) yield X.take(j), y.take(j) gluon版本1234batch_size = 1dataset_train = gluon.data.ArrayDataset(X_train, y_train)data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True) 初始化权值scratch版本 1234567def get_params(): w = nd.random.normal(shape=(num_inputs, 1))*0.1 b = nd.zeros((1,)) for param in (w, b): param.attach_grad() return (w, b) gluon版本1234net.initialize()net.collect_params().initialize(mx.init.Normal(sigma=1)) SGDscratch版本 123def SGD(params, lr): for param in params: param[:] = param - lr * param.grad L2正则 12def L2_penalty(w, b): return ((w**2).sum() + b**2) / 2 gluon版本12trainer = gluon.Trainer(net.collect_params(), &apos;sgd&apos;, &#123; &apos;learning_rate&apos;: learning_rate, &apos;wd&apos;: weight_decay&#125;) 这里的weight_decay表明这里添加了L2正则，正则化w = w -lr grad - wd w 训练过程scratch版本 12345678910for e in range(epochs): for data, label in data_iter(num_train): with autograd.record(): output = net(data, lambd, *params) loss = square_loss( output, label) + lambd * L2_penalty(*params) loss.backward() SGD(params, learning_rate) train_loss.append(test(params, X_train, y_train)) test_loss.append(test(params, X_test, y_test)) gluon版本 12345678910for e in range(epochs): for data, label in data_iter_train: with autograd.record(): output = net(data) loss = square_loss(output, label) loss.backward() trainer.step(batch_size) train_loss.append(test(net, X_train, y_train)) test_loss.append(test(net, X_test, y_test)) 12345678910111213141516171819202122232425262728293031323334353637383940414243%matplotlib inlineimport matplotlib as mplmpl.rcParams['figure.dpi']= 120import matplotlib.pyplot as pltdef train(X_train, X_test, y_train, y_test): # 线性回归模型 net = gluon.nn.Sequential() with net.name_scope(): net.add(gluon.nn.Dense(1)) net.initialize() # 设一些默认参数 learning_rate = 0.01 epochs = 100 batch_size = min(10, y_train.shape[0]) dataset_train = gluon.data.ArrayDataset(X_train, y_train) data_iter_train = gluon.data.DataLoader( dataset_train, batch_size, shuffle=True) # 默认SGD和均方误差 trainer = gluon.Trainer(net.collect_params(), 'sgd', &#123; 'learning_rate': learning_rate&#125;) square_loss = gluon.loss.L2Loss() # 保存训练和测试损失 train_loss = [] test_loss = [] for e in range(epochs): for data, label in data_iter_train: with autograd.record(): output = net(data) loss = square_loss(output, label) loss.backward() trainer.step(batch_size) train_loss.append(square_loss( net(X_train), y_train).mean().asscalar()) test_loss.append(square_loss( net(X_test), y_test).mean().asscalar()) # 打印结果 plt.plot(train_loss) plt.plot(test_loss) plt.legend(['train','test']) plt.show() return ('learned weight', net[0].weight.data(), 'learned bias', net[0].bias.data()) 最终版123456789101112131415161718192021222324252627282930313233def train(train_data, test_data, net, loss, trainer, ctx, num_epochs, print_batches=None): """Train a network""" print("Start training on ", ctx) if isinstance(ctx, mx.Context): ctx = [ctx] for epoch in range(num_epochs): train_loss, train_acc, n, m = 0.0, 0.0, 0.0, 0.0 if isinstance(train_data, mx.io.MXDataIter): train_data.reset() start = time() for i, batch in enumerate(train_data): data, label, batch_size = _get_batch(batch, ctx) losses = [] with autograd.record(): outputs = [net(X) for X in data] losses = [loss(yhat, y) for yhat, y in zip(outputs, label)] for l in losses: l.backward() train_acc += sum([(yhat.argmax(axis=1)==y).sum().asscalar() for yhat, y in zip(outputs, label)]) train_loss += sum([l.sum().asscalar() for l in losses]) trainer.step(batch_size) n += batch_size m += sum([y.size for y in label]) if print_batches and (i+1) % print_batches == 0: print("Batch %d. Loss: %f, Train acc %f" % ( n, train_loss/n, train_acc/m )) test_acc = evaluate_accuracy(test_data, net, ctx) print("Epoch %d. Loss: %.3f, Train acc %.2f, Test acc %.2f, Time %.1f sec" % ( epoch, train_loss/n, train_acc/m, test_acc, time() - start )) reference从零开始码一个皮卡丘检测器 图片标注工具 mxnet 使用自己的图片数据训练CNN模型 mxnet image API Create a Dataset Using RecordIO 基于MXNet gluon 的SSD模型训练 解决conda与ipython notebook的python版本问题 神经网络计算参数量的方法 神经网络计算特征图的大小的方法 BN应该放在relu后]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN中卷积计算的内存和速度优化]]></title>
    <url>%2F2017-09-20%2Fconvolution_improve%2F</url>
    <content type="text"><![CDATA[在现在的DNN中，不管是前向传播还是反向传播，绝大多数时间花费在卷积计算中。因此对于速度提升来说，优化卷积层意义重大。 虽说从参数量来讲，早期的一些网络(alexbnet,VGG，googlnet等)70%以上的参数都是全连接层的。但是现在从架构上的改进已经开始减少全连接层了，比如squeezenet,mobilenet已经使用global avg pooling层取代全连接层了。那么接下来再想提速那就得从卷积层下手了。当然还有一中思路是从量化的方式减少参数量和内存消耗的（如BNN，eBNN），对于提速来说意义并不大。 以往的卷积计算方法sum循环法时间复杂度最高，为 $O(HWMKKC)$ 最笨的方法，只是用来理解。12345678910111213input[C][H][W];kernels[M][K][K][C];output[M][H][W];for h in 1 to H do for w in 1 to W do for o in 1 to M do sum = 0; for x in 1 to K do for y in 1 to K do for i in 1 to C do sum += input[i][h+y][w+x] *kernels[o][x][y][i]; output[o][w][h] = sum; patch-building DNN convolution algorithmsbased on gemm convolution algorithm 优点是：比较简单，方便理解和计算 缺点是：需要大量的内存做中间存储 im2col过程图片来自 ：贾扬清的demo convolution in caffe在 Caffe 中如何计算卷积？ 把卷积的第一个感受野里的矩阵转化成一个vector，并把各个channel的feature连接起来。随着划窗的进行，把接下来的窗口都转化乘vector,并排放在下面 最后把有Cout个卷积核，每个卷积核有C个channel,那么转化乘Cout行的vector组。最后卷积就编程矩阵乘法了。 各种方法占用内存量 reference在 Caffe 中如何计算卷积？]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>神经网络</tag>
        <tag>网络优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法小结与对比]]></title>
    <url>%2F2017-09-10%2Fmachine_learning_deep_learning%2F</url>
    <content type="text"><![CDATA[机器学习是做NLP和计算机视觉这类应用算法的基础，虽然现在深度学习模型大行其道，但是懂一些传统算法的原理和它们之间的区别还是很有必要的。可以帮助我们做一些模型选择。本篇博文就总结一下各种机器学习算法的特点和应用场景。本文是笔者结合自身面试中遇到的问题和总结网络上的资源得到的，所有引用已给出链接，如侵删。 SVM与LR的区别从模型解决问题的方式来看Linear SVM直观上是trade-off两个量 a large margin，就是两类之间可以画多宽的gap ；不妨说是正样本应该在分界平面向左gap/2（称正分界），负样本应该在分解平面向右gap/2（称负分界） L1 error penalty，对所有不满足上述条件的点做L1 penalty 给定一个数据集，一旦完成Linear SVM的求解，所有数据点可以被归成两类 一类是落在对应分界平面外并被正确分类的点，比如落在正分界左侧的正样本或落在负分界右侧的负样本 第二类是落在gap里或被错误分类的点。 假设一个数据集已经被Linear SVM求解，那么往这个数据集里面增加或者删除更多的一类点并不会改变重新求解的Linear SVM平面。不受数据分布的影响。 求解LR模型过程中，每一个数据点对分类平面都是有影响的，它的影响力远离它到分类平面的距离指数递减。换句话说，LR的解是受数据本身分布影响的。在实际应用中，如果数据维度很高，LR模型都会配合参数的L1 regularization。 两者的区别两个模型对数据和参数的敏感程度不同，Linear SVM比较依赖penalty的系数和数据表达空间的测度，而（带正则项的）LR比较依赖对参数做L1 regularization的系数。但是由于他们或多或少都是线性分类器，所以实际上对低维度数据overfitting的能力都比较有限，相比之下对高维度数据，LR的表现会更加稳定，为什么呢？因为Linear SVM在计算margin有多“宽”的时候是依赖数据表达上的距离测度的，换句话说如果这个测度不好（badly scaled，这种情况在高维数据尤为显著），所求得的所谓Large margin就没有意义了，这个问题即使换用kernel trick（比如用Gaussian kernel）也无法完全避免。所以使用Linear SVM之前一般都需要先对数据做normalization，而求解LR（without regularization）时则不需要或者结果不敏感。 Linear SVM和LR都是线性分类器Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance一般需要先对数据做balancing。Linear SVM依赖数据表达的距离测度，所以需要对数据先做normalization；LR不受其影响Linear SVM依赖penalty的系数，实验中需要做validationLinear SVM和LR的performance都会收到outlier的影响，其敏感程度而言，谁更好很难下明确结论。 balance的方法 调整正、负样本在求cost时的权重，比如按比例加大正样本cost的权重。然而deep learning的训练过程是on-line的因此你需要按照batch中正、负样本的比例调整。 做训练样本选取：如hard negative mining，只用负样本中的一部分。 做训练样本选取：如通过data augmentation扩大正样本数量。 过拟合方面 LR容易欠拟合，准确度低。 SVM不太容易过拟合：松弛因子+损失函数形式 注意SVM的求解方法叫拉格朗日乘子法，而对于均方误差的优化方法是最小二乘法。 方法的选择在Andrew NG的课里讲到过： 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况 当你的数据非常非常非常非常非常大然后完全跑不动SVM的时候，跑LR。SVM适合于小样本学习。多大算是非常非常非常非常非常非常大？ 比如几个G，几万维特征，就勉强算大吧…而实际问题上几万个参数实在完全不算个事儿，太常见了。随随便便就得上spark。读一遍数据就老半天，一天能训练出来的模型就叫高效了。所以在新时代，LR其实反而比以前用的多了=. = 应用场景方面不同拟合程度，样本量， 距离测度，数据balance 模型简单易解释 如果数据特征维度高，svm要使用核函数来求解 Note：拉格朗日对偶没有改变最优解，但改变了算法复杂度：原问题—样本维度；对偶问题–样本数量。所以 线性分类&amp;&amp;样本维度&lt;样本数量：原问题求解（liblinear默认）； 非线性–升维—一般导致 样本维度&gt;样本数量：对偶问题求解 SVM适合处理什么样的数据？高维稀疏，样本少。【参数只与支持向量有关，数量少，所以需要的样本少，由于参数跟维度没有关系，所以可以处理高维问题】 机器学习常见算法总结机器学习常见算法个人总结（面试用） 朴素贝叶斯朴素贝叶斯的优点：对小规模的数据表现很好，适合多分类任务，适合增量式训练。缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的） 线性回归线性回归试图学得一个线性模型以尽可能准确地预测实值输出标记。均方误差是回归任务中最常用的性能度量，基于均方误差最小化来进行模型求解的方法成为最小二乘法。在线性回归中，最小二乘法就是试图找到一条直线，使得所有样本到直线上的欧式距离之和最小。这个想法和分类问题是正好相反的，分类问题是找到一个分界面离所有样本尽可能远。 优化方法 当x矩阵是列满秩的时候，可以用最小二乘法，但是求矩阵的逆比较慢 梯度下降法，以最大似然估计的结果对权值求梯度，sigmoid函数也是如此 均方无法的概率解释假设根据特征的预测结果与实际结果有误差∈ (i) ,那么预测结果θ T x (i) 和真实结果y (i) 满足下式:一般来讲,误差满足平均值为 0 的高斯分布,也就是正态分布。那么 x 和 y 的条件概率也就是 用条件概率最大似然估计法得到： LR回归回归用来分类 0/1 问题,也就是预测结果属于 0 或者 1 的二值分类问题 仍然求的是最大似然估计,然后求导,得到迭代公式结果为，梯度下降法： 优化问题的求解方法[Math] 常见的几种最优化方法大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。常见的最优化方法有梯度下降法、牛顿法和拟牛顿法、共轭梯度法等等。 梯度下降法优化思想当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。 缺点梯度下降法的最大问题就是会陷入局部最优，靠近极小值时收敛速度减慢。 批量梯度下降法最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。 随机梯度下降法最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。 随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。 牛顿法牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。 牛顿法比梯度下降法快牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。 但是牛顿法要算hessian矩阵的逆，比较费时间。 拟牛顿法 拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。 拉格朗日法 拉格朗日乘数法 拉格朗日乘子法主要用于解决约束优化问题，它的基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题。拉格朗日乘子背后的数学意义是其为约束方程梯度线性组合中每个向量的系数。 通过引入拉格朗日乘子建立极值条件，对n个变量分别求偏导对应了n个方程，然后加上k个约束条件（对应k个拉格朗日乘子）一起构成包含了（n+k）变量的（n+k）个方程的方程组问题，这样就能根据求方程组的方法对其进行求解。 机器学习算法选择机器学习算法选择 随机森林平均来说最强，但也只在9.9%的数据集上拿到了第一，优点是鲜有短板。SVM的平均水平紧随其后，在10.7%的数据集上拿到第一。神经网络（13.2%）和boosting（~9%）表现不错。数据维度越高，随机森林就比AdaBoost强越多，但是整体不及SVM2。数据量越大，神经网络就越强。 贝叶斯是相对容易理解的一个模型，至今依然被垃圾邮件过滤器使用。 K近邻典型的例子是KNN，它的思路就是——对于待判断的点，找到离它最近的几个数据点，根据它们的类型决定待判断点的类型。 它的特点是完全跟着数据走，没有数学模型可言。 三要素： k值的选择 距离的度量（常见的距离度量有欧式距离，马氏距离等） 分类决策规则 （多数表决规则） k值的选择 k值越小表明模型越复杂，更加容易过拟合 但是k值越大，模型越简单，如果k=N的时候就表明无论什么点都是训练集中类别最多的那个类 所以一般k会取一个较小的值，然后用过交叉验证来确定这里所谓的交叉验证就是将样本划分一部分出来为预测样本，比如95%训练，5%预测，然后k分别取1，2，3，4，5之类的，进行预测，计算最后的分类误差，选择误差最小的k 分类决策规则找到最近的k个实例之后，可以计算平均值作为预测值，也可以给这k个实例加上一个权重再求平均值，这个权重与度量距离成反比（越近权重越大） 优缺点：优点 思想简单 可用于非线性分类 训练时间复杂度为O(n) 准确度高，对outlier不敏感缺点 计算量大 样本不平衡问题不适用 需要大量的内存 KD树KD树是一个二叉树，表示对K维空间的一个划分，可以进行快速检索 构造KD树在k维的空间上循环找子区域的中位数进行划分的过程。 假设现在有K维空间的数据集： $T={x_1,x_2,x_3,…x_n}$, $xi={a_1,a_2,a_3..a_k}$ 首先构造根节点，以坐标$a_1$的中位数b为切分点，将根结点对应的矩形局域划分为两个区域，区域1中$a_1 &lt; b$,区域2中$a_1&gt;b$ 构造叶子节点，分别以上面两个区域中$a_2$的中位数作为切分点，再次将他们两两划分，作为深度1的叶子节点，（如果a2=中位数，则a2的实例落在切分面） 不断重复2的操作，深度为j的叶子节点划分的时候，索取的$a_i$ 的$i=j \% k+1$，直到两个子区域没有实例时停止 KD树的搜索 首先从根节点开始递归往下找到包含x的叶子节点，每一层都是找对应的xi 将这个叶子节点认为是当前的“近似最近点” 递归向上回退，如果以x圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界相交，则说明另一半子区域中存在与x更近的点，则进入另一个子区域中查找该点并且更新”近似最近点“ 重复3的步骤，直到另一子区域与球体不相交或者退回根节点 最后更新的”近似最近点“与x真正的最近点 log(n) 决策树决策树的特点是它总是在沿着特征做切分。随着层层递进，这个划分会越来越细。 因为它能够生成清晰的基于特征(feature)选择不同预测结果的树状结构 随机森林器学习岗位面试问题汇总 之 集成学习 基本概念 天池离线赛 - 移动推荐算法（四）：基于LR, RF, GBDT等模型的预测 它首先随机选取不同的特征(feature)和训练样本(training sample)bagging，生成大量的决策树，然后综合这些决策树的结果来进行最终的分类。 随机森林在现实分析中被大量使用，它相对于决策树，在准确性上有了很大的提升 适用场景：数据维度相对低（几十维），同时对准确性有较高要求时。 参数调节是一种基于决策树基模型的集成学习方法，其核心思想是通过特征采样来降低训练方差，提高集成泛化能力。 max_depth 属于基学习器参数，它控制着每个决策树的深度，一般来说，决策树越深，模型拟合的偏差越小，但同时拟合的开销也越大。一般地，需要保证足够的树深度，但也不宜过大。 RF与传统bagging的区别（1）样本采样：RF有放回选取和整体样本数目相同的样本，一般bagging用的样本&lt;总体样本数（2）特征采样：RF对特征进行采样，BAGGING用全部特征 RF的优点（1）在数据集上表现良好，在当先很多数据集上要优于现有的很多算法（2）可以并行，且不是对所有属性进行训练，训练速度相对较快（3）防止过拟合（4）能够处理高维特征，且不用做特征选择，可以给出特征重要性的评分，训练过程中，可以检测到feature的相互影响 缺点 ①树越多，随机森林的表现才会越稳定。所以在实际使用随机森林的时候需要注意如果树不够多的时候，可能会导致不稳定的情况。 ②不平衡数据集。分类结果会倾向于样本多的类别，所以训练样本中各类别的数据必须相同。Breiman在实际实现该算法的时候有考虑到了这个问题，采取了根据样本类别比例对决策树的判断赋予不同权值的方法 RF的学习算法ID3：离散C4.5：连续CART：离散或连续 GBDT基本概念GBDT（梯度迭代决策树）是一种基于决策回归树的Boosting模型，其核心思想是将提升过程建立在对“之前残差的负梯度表示”的回归拟合上，通过不断的迭代实现降低偏差的目的。 GBDT设置大量基学习器的目的是为了集成来降低偏差，所以 n_estimators （基决策器的个数）一般会设置得大一些。 对于GBDT模型来说，其每个基学习器是一个弱学习器(欠拟合)，决策树的深度一般设置得比较小，以此来降低方差（模型复杂度低），之后在经过残差逼近迭代来降低偏差，从而形成强学习器。 GBDT与传统Boosting（AdaBoost）的区别Boosting算法，但与传统boosting有区别、拟合上一步的残差，传统意义上说不能并行，只能用CART回归树，降低偏差 迭代思路不同：传统boosting对训练样本进行加权，GBDT则是拟合残差，下一棵树沿残差梯度下降的方向进行拟合 GBDT正则化的方式（1）同AdaBoost，通过步长（2）CART树的剪枝（3）子抽样，不放回，SGBT，可以实现一定程度上的并行 GBDT的优缺点优点：（1）调参少的情况下，准确率也高（SVM）（2）灵活处理各种数据，包括连续和离散，无需归一化处理（LR）（3）模型非线性变换多，特征不用经过复杂处理即可表达复杂信息（4）从一定程度上可以防止过拟合，小步而非大步拟合缺点：（1）一般来说传统的GBDT只能串行，但是也可以通过子采样比例（0.5~0.8）实现某种意义上的并行，但一般这就不叫GBDT了。（2）对异常值敏感，但是可以采取一些健壮的损失函数缓解，如Huber./Quantile损失函数 GBDT预测时每一棵树是否能并行？可以，训练需串行，预测可并行 GBDT和RF的区别与联系联系：多棵树进行训练+多棵树共同进行预测区别：（1）取样方式（2）预测时，RF多数投票，GBDT加权累加（3）样本的关系—&gt;并行和串行（4）学期器的种类，GBDT只能用CART回归树 (因为要计算连续梯度)（5）对异常值的敏感性（6）通过减少方差/偏差提高性能 XGBOOST相比于GBDT有何不同？XGBOOST为什么快？XGBOOST如何支持并行？（1）GBDT只能用CART，而XGBOOST可以用CART树（回归/分类）,还可以用用想LR之类的线性模型，相当于加入L1、L2正则项的LR或线性回归（2）列抽样，可以并行，不是树粒度上的，是特征粒度上的，block块，并行计算所有信息增益等信息（3）可处理多种特征，且对缺失值也不用进行处理（4）GBDT在残差梯度下降方向拟合，一阶导；XGBOOST泰勒展开至二阶导（5）近似直方图算法，高效生产候选分割点（6）shrink，缩减，叶子节点同时乘，防止过拟合（7）可以自己定义评价函数（8）代价函数含正则化项，防止过拟合 ababoostdaBoost的优缺点优点：（1）容易理解、实现简单（2）易编码（3）分类精度高（4）可以使用各种回归模型构建基分类器，非常灵活（5）作为二元分类器是，构造简单、结果可理解、少参数（6）相对来说，不宜过拟合缺点：（1）只能串行（2）对异常值敏感 boosting对异常值敏感 集成学习与方差偏差我觉得，避免偏差的话，首先我们需要尽量选择正确的模型，所谓“对症下药”。我觉得有位同行把机器学习算法的使用比作医生开药方，是非常不错的比喻。我们要根据数据的分布和特点，选择合适的算法。 其次，有了合适的算法，我们还要慎重选择数据集的大小。通常训练数据集越大越好，但是当大到数据集已经对整体所有数据有了一定的代表性之后，再多的数据已经不能提升模型的准确性，反而带来模型训练的计算量增加。但是，训练数据太少的话是一定不好的，这会带来过拟合的问题，过拟合就是模型复杂度太高，方差很大，不同的数据集训练出来的模型变化非常大 从集成学习到模型的偏差和方差的理解 使用sklearn进行集成学习——理论 GBDT算法特征重要程度计算 机器学习中，有哪些特征选择的工程方法？ 为什么说bagging是减少variance，而boosting是减少bias?从机制上讲为什么说bagging是减少variance，而boosting是减少bias 若各子模型独立，则有$$Var(\frac{\sum X_i}{n})=\frac{Var(X_i)}{n}$$，此时可以显著降低variance。若各子模型完全相同，则$$Var(\frac{\sum X_i}{n})=Var(X_i)$$，此时不会降低variance。 Bagging 是 Bootstrap Aggregating 的简称，意思就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均。，所以从偏差上看没有降低，但是由于各个子模型是单独训练的，有一定的独立性，所以方差降低比较多,提高泛化能力。特别是random forest这种方式，不仅对样本取样，还有特征取样。 boosting从优化角度来看，是用forward-stagewise这种贪心法去最小化损失函数，在这个过程中偏差是逐步减小的，而由于各阶段分类器之间相关性较强，方差降低得少。 举个例子gbdt是boosting的方式，它的决策树的深度比较小，模型会欠拟合，刚开始偏差大，后来就慢慢变小了。 为什么把特征组合之后还能提升反正这些基本都是增强了特征的表达能力，或者说更容易线性可分吧 总体性问题分类与回归的区别分类和回归的区别在于输出变量的类型。 定量输出称为回归，或者说是连续变量预测；定性输出称为分类，或者说是离散变量预测。 生成模型与判别模型的区别有监督机器学习方法可以分为生成方法和判别方法（常见的生成方法有混合高斯模型、朴素贝叶斯法和隐形马尔科夫模型等，常见的判别方法有SVM、LR等），生成方法学习出的是生成模型，判别方法学习出的是判别模型。 监督学习，预测时，一般都是在求p(Y|X)生成模型： 从数据中学习联合概率分布p(X,Y)，然后利用贝叶斯公式求：$$p(Y|X)=\frac{P(X,Y)}{\Sigma P(X,Y_{i} )} $$，比如说朴素贝叶斯 判别模型：直接学习P(Y|X)， 它直观输入什么特征X，就直接预测出最可能的Y; 典型的模型包括：LR, SVM,CRF,Boosting,Decision tree…. 生成方法的特点：生成方法可以还原联合概率分布，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学习的模型可以更快的收敛于真实的模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。 判别方法的特点：判别方法直接学习的是条件概率或者决策函数，直接面对预测，往往学习的准确率更高；由于直接学习或者，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。 精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？ 精确率（Precision）为TP/(TP+FP) 召回率（Recall）为TP/(TP+FN) F1值是精确率和召回率的调和均值，即F1=2PR/(P+R） ROC曲线（Receiver operating characteristic curve），ROC曲线其实是多个混淆矩阵的结果组合，如果在上述模型中我们没有定好阈值，而是将模型预测结果从高到低排序，将每个概率值依次作为阈值，那么就有多个混淆矩阵。对于每个混淆矩阵，我们计算两个指标TPR（True positive rate）和FPR（False positive rate），TPR=TP/(TP+FN)=Recall，TPR就是召回率，FPR=FP/(FP+TN)。 在画ROC曲线的过程中，若有一个阈值，高于此阈值的均为坏人，低于此阈值的均为好人，则认为此模型已完美的区分开好坏用户。此时坏用户的预测准确率（TPR）为1，同时好用户的预测错误率（FPR）为0，ROC曲线经过（0,1）点。AUC（Area Under Curve）的值为ROC曲线下面的面积，若如上所述模型十分准确，则AUC为1。但现实生活中尤其是工业界不会有如此完美的模型，一般AUC均在0.5到1之间，AUC越高，模型的区分能力越好 若AUC=0.5，即与上图中红线重合，表示模型的区分能力与随机猜测没有差别。 所以AUC表征的是模型的分类能力。 过拟合如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。 产生的原因因为参数太多，会导致我们的模型复杂度上升，容易过拟合 权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征. 解决方法 交叉验证法减少特征正则化权值衰减验证数据 线性分类器与非线性分类器的区别以及优劣如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归常见的非线性分类器：决策树、RF、GBDT、多层感知机SVM两种都有(看线性核还是高斯核) 线性分类器速度快、编程方便，但是可能拟合效果不会很好非线性分类器编程复杂，但是效果拟合能力强 特征比数据量还大时，选择什么样的分类器？线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分对于维度很高的特征，你是选择线性还是非线性分类器？理由同上对于维度极低的特征，你是选择线性还是非线性分类器？非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分 样本不均衡如何解决从重采样到数据合成 主要三个方面，数据，模型和评估方法。 数据上重采样和欠采样，使之均衡； 模型上选对样本不均衡问题不敏感的模型，和算法集成技术，如决策树，不能用KNN； 评估方法，用查全率，查准率之类 重采样（resampling）技术：(1). 随机欠采样随机欠采样的目标是通过随机地消除占多数的类的样本来平衡类分布。优点它可以提升运行时间；并且当训练数据集很大时，可以通过减少样本数量来解决存储问题。缺点它会丢弃对构建规则分类器很重要的有价值的潜在信息。被随机欠采样选取的样本可能具有偏差。它不能准确代表大多数。 (2). 随机过采样（Random Over-Sampling）过采样（Over-Sampling）通过随机复制少数类来增加其中的实例数量，从而可增加样本中少数类的代表性。优点与欠采样不同，这种方法不会带来信息损失。表现优于欠采样。缺点由于复制少数类事件，它加大了过拟合的可能性。 (3). 信息性过采样：合成少数类过采样技术直接复制少数类实例并将其添加到主数据集时。从少数类中把一个数据子集作为一个实例取走，接着创建相似的新合成的实例。这些合成的实例接着被添加进原来的数据集。新数据集被用作样本以训练分类模型。优点通过随机采样生成的合成样本而非实例的副本，可以缓解过拟合的问题。不会损失有价值信息。缺点当生成合成性实例时，SMOTE 并不会把来自其他类的相邻实例考虑进来。这导致了类重叠的增加，并会引入额外的噪音。 深度学习方面的问题机器学习岗位面试问题汇总 之 深度学习 深度学习的实质 及其 与浅层学习的区别深度学习实质：多隐层+海量数据——&gt;学习有用特征—–&gt;提高分类或预测准确性区别：（1）DL强调模型深度(2）DL突出特征学习的重要性：特征变换+非人工 BP算法为什么不能适应于深度学习BP为传统多层感知机的训练方法，&lt;=5层问题：（1）梯度越来越稀疏（梯度扩散&lt;—-非凸目标函数）（2）局部最小（3）一般，有标签NOTE：解决其中局部最小值的方法：（1）多组不同随机参数，取最好参数 （2）启发式优化算法：模拟退火 或 遗传 （3）随机梯度下降 CNN卷基层和pooling层的作用卷积层：特征提取子采样层/池化层：缩减输入数据的规模 DNN常用的激活函数有哪些，各有什么特点（1）sigmoid：易饱和（梯度消失），非0均值 （2）tanh，改进了sigmoid的第二个缺点，即它是0均值的 （3）ReLU，收敛快（不容易饱和），求梯度简单（没有指数计算，只需要阈值就可以），有稀疏特性。缺点是神经元容易坏死。1由于ReLU在x&lt;0时梯度为0，这样就导致负的梯度在这个ReLU被置零，而且这个神经元有可能再也不会被任何数据激活。如果这个情况发生了，那么这个神经元之后的梯度就永远是0了，也就是ReLU神经元坏死了，不再对任何数据有所响应。实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都坏死了 解决relu神经元坏死问题当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。 relu的变种leaky-relu: $$f(x)=\alpha x，(x &lt; 0)$$$$f(x)=x，(x&gt;=0)$$ 这里的 α 是一个很小的常数。这样，即修正了数据分布，又保留了一些负轴的值，使得负轴信息不会全部丢失。 Parametric ReLU： 对于 Leaky ReLU 中的α，通常都是通过先验知识人工赋值的。然而可以观察到，损失函数对α的导数我们是可以求得的，可不可以将它作为一个参数进行训练呢 Randomized ReLU：Randomized Leaky ReLU 是 leaky ReLU 的random 版本 ,核心思想就是，在训练过程中，α 是从一个高斯分布 U(l,u) 中 随机出来的，然后再测试过程中进行修正（有点像dropout的用法) 什么样的资料不适合用深度学习？（1）数据量小 （2）没有局部相关性 什么是共线性，跟过拟合有何关联？共线性：高度相关—&gt;冗余——&gt;过拟合解决：排除相关、加入权重正则 pooling技术有哪些，有什么作用,有什么区别pooling的结果是使得特征减少，参数减少，但pooling的目的并不仅在于此。pooling目的是为了保持某种不变性（平移），常用的有mean-pooling，max-pooling和Stochastic-pooling三种。 mean-pooling，即对邻域内特征点只求平均，max-pooling，即对邻域内特征点取最大。根据相关理论，特征提取的误差主要来自两个方面：（1）邻域大小受限造成的估计值方差增大；（2）卷积层参数误差造成估计均值的偏移。一般来说，mean-pooling能减小第一种误差，更多的保留图像的背景信息，max-pooling能减小第二种误差，更多的保留纹理信息。Stochastic-pooling则介于两者之间，通过对像素点按照数值大小赋予概率，再按照概率进行亚采样，在平均意义上，与mean-pooling近似，在局部意义上，则服从max-pooling的准则。 LeCun的“Learning Mid-Level Features For Recognition”对前两种pooling方法有比较详细的分析对比，如果有需要可以看下这篇论文。 其实pooling的目的就是为了使参数量减少，因为根本不需要那么多参数。pooling也只能做到在极小范围内的平移不变性，旋转和 伸缩是做不到的。其实不变性都是特征工程时代的概念了，现在在数据量极大的情况下，样本覆盖了足够多的variance，dnn自动就会把各种不变性学习出来 使用Pooling的目的之一是获取一定的特征不变性，目前用的比较多的是Max Pooling。max pooling是DCNN的非线性来源之一，然后在现代的深度神经网络中，最大的非线性来源是ReLU类的激活函数。因此，目前对使用Pooling也存在一定的争议，一些最新的工作已经不在网络的中间层使用pooling层了（或者只在最后一层使用average pooling，比如说network in network)。 缺点在于会丢失信息。 pooling的反向传播对于mean pooling，真的是好简单：假设pooling的窗大小是2x2, 在forward的时候啊，就是在前面卷积完的输出上依次不重合的取2x2的窗平均，得到一个值就是当前mean pooling之后的值。backward的时候，把一个值分成四等分放到前面2x2的格子里面就好了。如下forward: [1 3; 2 2] -&gt; 2backward: 2 -&gt; [0.5 0.5; 0.5 0.5] max pooling就稍微复杂一点，forward的时候你只需要把2x2窗子里面那个最大的拿走就好了，backward的时候你要把当前的值放到之前那个最大的位置，其他的三个位置都弄成0。如下forward: [1 3; 2 2] -&gt; 3backward: 3 -&gt; [0 3; 0 0] 特征选择的方法机器学习中，有哪些特征选择的工程方法？ 特征选择是特征工程中的重要问题（另一个重要的问题是特征提取），坊间常说：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征工程尤其是特征选择在机器学习中占有相当重要的地位。 特征选择方法举例 计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征 通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验； 训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型； 通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力. 特征选择方法分类 Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 Embedded：集成方法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 降维：PCA LDA等。Filter过滤法 方差选择法 使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征 相关系数法使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值 卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性 互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的 Embedded 集成方法 基于惩罚项的特征选择法L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个 基于树模型的特征选择法树模型中GBDT也可用来作为基模型进行特征选择 深度学习方法 降维将原始的样本映射到维度更低的样本空间中。 PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。 对抗过拟合的方法提前停止设定合适的模型训练停止条件，避免模型过程训练产生过拟合现象 交叉验证一个常用的交叉验证算法是k-fold交叉方法:把训练样例分成k份,然后进行k次交叉验证过程,每次使用不同的一份作为验证集合,其余k-1份合并作为训练集合.每个样例会在一次实验中被用作验证样例,在k-1次实验中被用作训练样例; 特征选择避免输入的数据维度过高，选择代表性的特征，删去重复特征。 正则化 正则项L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用，L1 正则是L0的最优凸近似。稀疏规则算子”（Lasso regularization）。L0是NP难的方法，所以使用L1方法进行最优化。 L1范数能实现稀疏任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。 数据稀疏的好处 自动特征选择：不重要的特征被设为0 可解释性：模型更容易解释 L2范数可以改善过拟合的状况 L1范数与L2范数的下降速度 LR相关问题LR与BPBP神经网络是否优于logistic回归？ 首先，神经网络的最后一层，也就是输出层，是一个 Logistic Regression （或者 Softmax Regression ），也就是一个线性分类器，中间的隐含层起到特征提取的作用，把隐含层的输出当作特征，然后再将它送入下一个 Logistic Regression，一层层变换。 神经网络的训练，实际上就是同时训练特征提取算法以及最后的 Logistic Regression的参数。为什么要特征提取呢，因为 Logistic Regression 本身是一个线性分类器，所以，通过特征提取，我们可以把原本线性不可分的数据变得线性可分。要如何训练呢，最简单的方法是（随机，Mini batch）梯度下降法 LR为什么使用sigmoid函数源于sigmoid，或者说exponential family所具有的最佳性质，即maximum entropy的性质。maximum entropy给了logistic regression一个很好的数学解释。为什么maximum entropy好呢？entropy翻译过来就是熵，所以maximum entropy也就是最大熵。熵用在概率分布上可以表示这个分布中所包含的不确定度，熵越大不确定度越大。均匀分布熵最大，因为基本新数据是任何值的概率都均等。而我们现在关心的是，给定某些假设之后，熵最大的分布。也就是说这个分布应该在满足我假设的前提下越均匀越好。比如大家熟知的正态分布，正是假设已知mean和variance后熵最大的分布。首先，我们在建模预测 Y|X，并认为 Y|X 服从bernoulli distribution，所以我们只需要知道 P(Y|X)；其次我们需要一个线性模型，所以 P(Y|X) = f(wx)。接下来我们就只需要知道 f 是什么就行了。而我们可以通过最大熵原则推出的这个 f，就是sigmoid。 面试问了如何在海量数据中查找给定部分数据最相似的top200向量，向量的维度也很高. 因为之前了解过其他面蚂蚁金服的朋友，也有问到这个题目的所以反应比较快，直接就说可以用KD树，聚类，hash,一天之内两连面，还是问了很多机器学习算法的东西 为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好 为什么把特征组合之后还能提升，反正这些基本都是增强了特征的表达能力，或者更容易线性可分吧 在logistic regression （LR）中，这个目标是什么呢？最大化条件似然度。考虑一个二值分类问题，训练数据是一堆（特征，标记）组合，（x1,y1), (x2,y2), …. 其中x是特征向量，y是类标记（y=1表示正类，y=0表示反类）。LR首先定义一个条件概率p(y|x；w）。 p(y|x；w）表示给定特征x，类标记y的概率分布，其中w是LR的模型参数（一个超平面）。有了这个条件概率，就可以在训练数据上定义一个似然函数，然后通过最大似然来学习w。这是LR模型的基本原理。 为什么LR把特征离散化后效果更好逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；(哑变量)特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。 连续特征的离散化：在什么情况下将连续的特征离散化之后可以获得更好的效果？ 在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。 李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。 如何用LR建立一个广告点击的模型：特征提取—&gt;特征处理（离散化、归一化、onehot等）—&gt;找出候选集—-&gt;模型训练，得到结果 LR的过拟合 减少feature个数（人工定义留多少个feature、算法选取这些feature） 正则化（为了方便求解，L2使用较多） 添加正则化后的损失函数变为： $$J(w)=-\frac{1}{N} \sum_{i=1}^N{\left(y_ilog(h_w(x_i))+(1-y_i)log(1-h_w(x_i))\right)} + \lambda ||w||_2$$ 同时w的更新变为： $$w:=w-\alpha \left(h_w(x_j)-y_j) x_i\right) -2\alpha*w_j$$ 关于LR的多分类：softmax$$P(Y=a|x)=\frac{exp(w_ax)}{(\sum_{i=1}^k(wix))} ; 1 &lt; a &lt; k$$ 这里会输出当前样本下属于哪一类的概率，并且满足全部概率加起来=1 关于softmax和k个LR的选择 如果类别之间是否互斥（比如音乐只能属于古典音乐、乡村音乐、摇滚月的一种）就用softmax否则类别之前有联系（比如一首歌曲可能有影视原声，也可能包含人声，或者是舞曲），这个时候使用k个LR更为合适Logistic回归优点：实现简单；分类时计算量非常小，速度很快，存储资源低；缺点：容易欠拟合，一般准确度不太高只能处理两分类问题 SVM相关问题解密SVM系列（一）：关于拉格朗日乘子法和KKT条件 svmw问题整理 SVM的主要特点（1）非线性映射-理论基础（2）最大化分类边界-方法核心（3）支持向量-计算结果（4）小样本学习方法 ，最终的决策函数只有少量支持向量决定，避免了“维数灾难” ，少数支持向量决定最终结果—-&gt;可“剔除”大量冗余样本+算法简单+具有鲁棒性（7）学习问题可表示为凸优化问题—-&gt;全局最小值（8）可自动通过最大化边界控制模型，但需要用户指定核函数类型和引入松弛变量（9）适合于小样本，优秀泛化能力（因为结构风险最小）（10）泛化错误率低，分类速度快，结果易解释 缺点：（1）大规模训练样本（m阶矩阵计算）（2）传统的不适合多分类（3）对缺失数据、参数、核函数/ 样本失衡敏感， 为什么要引入对偶问题（1）容易求解 （2）核函数Note：拉格朗日对偶没有改变最优解，但改变了算法复杂度：原问题—样本维度；对偶问题–样本数量。所以 线性分类&amp;&amp;样本维度&lt;样本数量：原问题求解（liblinear默认）； 非线性–升维—一般导致 样本维度&gt;样本数量：对偶问题求解 样本失衡的影响超平面会靠近样本少的类别。因为使用的是软间隔分类，而如果对所有类别都是使用同样的惩罚系数，则由于优化目标里面有最小化惩罚量，所以靠近少数样本时，其惩罚量会少一些。 对正例和负例赋予不同的C值，例如正例远少于负例，则正例的C值取得较大，这种方法的缺点是可能会偏离原始数据的概率分布； 对训练集的数据进行预处理即对数量少的样本以某种策略进行采样，增加其数量或者减少数量多的样本 样本失衡时，如何评价分类器的性能好坏？使用ROC曲线 样本没有规范化对SVM有什么影响？对偶问题的优化目标函数中有向量的内积计算(优化过程中也会有内积计算的，见SMO)，径向基核函数中有向量的距离计算，存在值域小的变量会被忽略的问题，影响算法的精度。参考 数据维度大于数据量的对SVM的影响？这种情况下一般采用线性核(即无核)，因为此时特征够用了(很大可能是线性问题)，没必要映射到更高维的特征空间。 拉格朗日乘子法 和KKT条件凸函数前提条件凸函数：下图左侧是凸函数。 凸的就是开口朝一个方向（向上或向下）。更准确的数学关系就是： 或者 对于凸问题，你去求导的话，是不是只有一个极点，那么他就是最优点，很合理。 等式条件约束当带有约束条件的凸函数需要优化的时候，一个带等式约束的优化问题就通过拉格朗日乘子法完美的解决了。 $$min \quad f = 2x_1^2+3x_2^2+7x_3^2 \s.t. \quad 2x_1+x_2 = 1 \ \quad \quad \quad 2x_2+3x_3 = 2$$ 可以使用 $$min \quad f = 2x_1^2+3x_2^2+7x_3^2 +\alpha _1(2x_1+x_2- 1)+\alpha _2(2x_2+3x_3 - 2)$$ 这里可以看到与α1,α2相乘的部分都为0，根原来的函数是等价的。所以α1,α2的取值为全体实数。现在这个优化目标函数就没有约束条件了吧。然后求导数。 不等式约束与KKT条件任何原始问题约束条件无非最多3种，等式约束，大于号约束，小于号约束，而这三种最终通过将约束方程化简化为两类：约束方程等于0和约束方程小于0。 $$min \quad f = x_1^2-2x_1+1+x_2^2+4x_2+4 \s.t. \quad x_1+10x_2 &gt; 10 \ \quad \quad \quad 10 x_1-10x_2 &lt; 10$$ 现在将约束拿到目标函数中去就变成：$$L(x,\alpha) = f(x) + \alpha_1g1(x)+\alpha_2g2(x)\ =x_1^2-2x_1+1+x_2^2+4x_2+4+ \alpha_1(10-x_1-10x_2 ) +\\alpha_2(10x_1-x_2 - 10)$$ 其中g是不等式约束，h是等式约束（像上面那个只有不等式约束，也可能有等式约束）。那么KKT条件就是函数的最优值必定满足下面条件： (1) L对各个x求导为零；(2) h(x)=0;(3) $\sum\alpha_ig_i(x)=0，\alpha_i\ge0$ 第三个式子不好理解，因为我们知道在约束条件变完后，所有的g(x)&lt;=0，且αi≥0，然后求和还要为0，无非就是告诉你，要么某个不等式gi(x)=0,要么其对应的αi=0。那么为什么KKT的条件是这样的呢？ SVM的原问题和对偶问题原问题 拉格朗日乘子法结果 求导得到 代入乘子算式得到 就得到的原问题的对偶问题 为什么要引入对偶算法 对偶问题往往更加容易求解(结合拉格朗日和kkt条件) 可以很自然的引用核函数（拉格朗日表达式里面有内积，而核函数也是通过内积进行映射的） SVM解决过拟合的方法决定SVM最优分类超平面的恰恰是那些占少数的支持向量，如果支持向量中碰巧存在异常点就会过拟合，解决的方法是加入松弛变量。 另一方面从损失函数角度看，引入了L2正则。 为什么要把原问题转换为对偶问题？ 因为原问题是凸二次规划问题，转换为对偶问题更加高效。 为什么求解对偶问题更加高效？ 因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0. alpha系数有多少个？ 样本点的个数 L1还可以用来选择特征 A 为什么L1可以用来选择特征 B 因为L1的话会把某些不重要的特征压缩为0 A 为什么L1可以把某些特征压缩为0 B 因为（画图）L1约束是正方形的，经验损失最有可能和L1的正方形的顶点相交，L1比较有棱角。所以可以把某些特征压缩为0 SVM优缺点优点： 使用核函数可以向高维空间进行映射 使用核函数可以解决非线性的分类 分类思想很简单，就是将样本与决策面的间隔最大化 分类效果较好 缺点： 对大规模数据训练比较困难 无法直接支持多分类，但是可以使用间接的方法来做 SMO算法SMOSMO是用于快速求解SVM的它选择凸二次规划的两个变量，其他的变量保持不变，然后根据这两个变量构建一个二次规划问题，这个二次规划关于这两个变量解会更加的接近原始二次规划的解，通过这样的子问题划分可以大大增加整个算法的计算速度 SVM多分类问题间接法一对多 其中某个类为一类，其余n-1个类为另一个类，比如A,B,C,D四个类，第一次A为一个类，{B,C,D}为一个类训练一个分类器，第二次B为一个类,{A,C,D}为另一个类,按这方式共需要训练4个分类器，最后在测试的时候将测试样本经过这4个分类器f1(x),f2(x),f3(x)和f4(x),取其最大值为分类器(这种方式由于是1对M分类，会存在偏置，很不实用) 一对一(libsvm实现的方式) 任意两个类都训练一个分类器，那么n个类就需要$n*(n-1)/2$个svm分类器。 还是以A,B,C,D为例,那么需要{A,B},{A,C},{A,D},{B,C},{B,D},{C,D}为目标共6个分类器，然后在预测的将测试样本通过这6个分类器之后进行投票选择最终结果。（这种方法虽好，但是需要$n*(n-1)/2$个分类器代价太大，不过有好像使用循环图来进行改进） referenceLinear SVM 和 LR 有什么异同？ SVM和logistic回归分别在什么情况下使用? 百度 – 机器学习面试 svmw问题整理 各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归 机器学习面试问题汇总 机器学习面试 如何准备机器学习工程师的面试 ？ 天池离线赛 - 移动推荐算法（四）：基于LR, RF, GBDT等模型的预测 机器学习常见算法个人总结（面试用） 机器学习面试问题汇总 cs229机器学习笔记及代码 腾讯17届校招面经合集]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow目标检测模型的压缩和ncnn转换]]></title>
    <url>%2F2017-08-24%2Fncnn_pi%2F</url>
    <content type="text"><![CDATA[ncnn 是一个为手机端极致优化的高性能神经网络前向计算框架。ncnn 从设计之初深刻考虑手机端的部署和使用。无第三方依赖，跨平台，手机端 cpu 的速度快于目前所有已知的开源框架。支持 8bit 量化和半精度浮点存储，可导入 caffe和tensorflow 模型 编译ncnn1234567git clone https://github.com/Tencent/ncnnsudo apt-get install libprotobuf-dev protobuf-compilercd ncnnmkdir build &amp;&amp; cd buildcmake ..make -jmake install 进入 ncnn/build/tools 目录下，如下所示，我们可以看到已经生成了 caffe2ncnn 可ncnn2mem这两个可执行文件，这两个可执行文件的作用是将caffe模型生成ncnn 模型，并且对模型进行加密。在ncnn/build/tools/tensorflow下面也有tensorflow2ncnn，可以把tensorflow模型转化乘ncnn模型 tensorflow在arm上的安装12wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v0.11.0/tensorflow-0.11.0-cp27-none-linux_armv7l.whlsudo pip install tensorflow-0.11.0-cp27-none-linux_armv7l.whl tensorflow的模型ckpt, pb，meta等文件 the .ckpt file is the old version output of saver.save(sess), which is the equivalent of your .ckpt-data (see below) the “checkpoint” file is only here to tell some TF functions which is the latest checkpoint file. .ckpt-meta contains the metagraph, i.e. the structure of your computation graph, without the values of the variables (basically what you can see in tensorboard/graph). .ckpt-data contains the values for all the variables, without the structure. To restore a model in python, you’ll usually use the meta and data files with (but you can also use the .pb file): 12saver = tf.train.import_meta_graph(path_to_ckpt_meta)saver.restore(sess, path_to_ckpt_data) I don’t know exactly for .ckpt-index, I guess it’s some kind of index needed internally to map the two previous files correctly. Anyway it’s not really necessary usually, you can restore a model with only .ckpt-meta and .ckpt-data. the .pb file can save your whole graph (meta + data). To load and use (but not train) a graph in c++ you’ll usually use it, created with freeze_graph, which creates the .pb file from the meta and data. Be careful, (at least in previous TF versions and for some people) the py function provided by freeze_graph did not work properly, so you’d have to use the script version. Tensorflow also provides a tf.train.Saver.to_proto() method, but I don’t know what it does exactly. TF0.12以来的模型类型Here’s my solution utilizing the V2 checkpoints introduced in TF 0.12. There’s no need to convert all variables to constants or freeze the graph. Just for clarity, a V2 checkpoint looks like this in my directory models:1234checkpoint # some information on the name of the files in the checkpointmy-model.data-00000-of-00001 # the saved weightsmy-model.index # probably definition of data layout in the previous filemy-model.meta # protobuf of the graph (nodes and topology info) Python part (saving)12with tf.Session() as sess: tf.train.Saver(tf.trainable_variables()).save(sess, &apos;models/my-model&apos;) If you create the Saver with tf.trainable_variables(), you can save yourself some headache and storage space. But maybe some more complicated models need all data to be saved, then remove this argument to Saver, just make sure you’re creating the Saver after your graph is created. It is also very wise to give all variables/layers unique names, otherwise you can run in different problems. Python part (inference)1234with tf.Session() as sess: saver = tf.train.import_meta_graph(&apos;models/my-model.meta&apos;) saver.restore(sess, tf.train.latest_checkpoint(&apos;models/&apos;)) outputTensors = sess.run(outputOps, feed_dict=feedDict) 导出pb文件12345python export_inference_graph.py \ --alsologtostderr \ --model_name=mobilenet_v1 \ --image_size=224 \ --output_file=/tmp/mobilenet_v1_224.pb Exporting the Inference Graph 作者的pretrained model frozen pb文件1234python tensorflow/tensorflow/python/tools/freeze_graph.py \ --input_graph=models/slim/mobilenet_v1_224.pb \--input_checkpoint=tmp_data/mobilenet_v1_1.0_224.ckpt \--input_binary=true --output_graph=tmp_data/frozen_mobilenet.pb --output_node_names=mobilenetv1/Predictions/Reshape_1 这里有个点在于怎么确定一个网络的output_node_names,参考 这个人自己搞的mobilenet image_label下载label数据 12curl -L &quot;https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz&quot; | tar -C tensorflow/examples/label_image/data -xz 1python tensorflow/tensorflow/examples/label_image/label_image.py --graph=tmp_data/inception_v3_2016_08_28_frozen.pb --image=cat123.jpg --input_layer=input --output_layer=InceptionV3/Predictions/Reshape_1 --input_mean=128 --input_std=128 --labels=tmp_data/imagenet_slim_labels.txt reference在树莓派上用TensorFlow玩深度学习 腾讯NCNN框架入门到应用 ncnn wiki tensorflow模型的各种版本 tensorfow各种版本的模型 mobilenet的使用 retrain_mobilenet MobileNet-SSD 从零开始码一个皮卡丘检测器-CNN目标检测入门教程 [Learning Note] Single Shot MultiBox Detector with Pytorch — Part 3 Building a Real-Time Object Recognition App with Tensorflow and OpenCV How to train your own Object Detector with TensorFlow’s Object Detector API Creating an Object Detection Application Using TensorFlow]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>tensorflow</tag>
        <tag>嵌入式</tag>
        <tag>模型压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow训练-finetune-压缩模型]]></title>
    <url>%2F2017-08-23%2Ftensorflow_object_detection%2F</url>
    <content type="text"><![CDATA[本博文主要介绍tensorflow训练模型并使用训练好的模型做图像分类，内容有:slim库，TensorFlow虚拟化环境安装，图像数据集的准备，训练网络模型的脚本，评估训练好的模型，调用训练好的模型来做分类。 环境配置docker tensorflow-gpudocker 方式运行tensorflow1nvidia-docker run -p 8088:8088 -p 6006:6006 -v /home/dragon/code:/root/code -it dragonfive/tensorflow:gpu bash 首先需要更新软件1apt-get update 然后安装git 等工具 12apt-get install git vim protobufprotobuf的配置 编译protobuf解压下载的tar.gz包，cd到protobuf的目录下，执行以下指令：1234./configuremakemake checkmake install 解决命令找不到 创建文件 /etc/ld.so.conf.d/libprotobuf.conf 包含内容/usr/local/lib 输入命令ldconfig 再运行protoc –version 就可以正常看到版本号了 slim-models编译proto的内容 1protoc object_detection/protos/*.proto --python_out=. 设置 PYTHONPATH 环境变量 1export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim 注意这里pwd外围的不是单引号，而是esc下面那个键 宠物数据集和标注数据下载下载训练数据里面的 图片1wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz 下载训练数据里面的标注信息1wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz 将训练数据格式转化为tfrecord格式需要一个id与类名的对应关系的txt文件：pet_label_map.pbtxt 12345678910111213141516171819item &#123; id: 1 name: 'Abyssinian'&#125;item &#123; id: 2 name: 'american_bulldog'&#125;item &#123; id: 3 name: 'american_pit_bull_terrier'&#125;item &#123; id: 4 name: 'basset_hound'&#125; 执行脚本需要一个把数据转化成 tfrecoder格式的脚本文件: 脚本参数这个脚本的使用方法是：123python object_detection/create_pet_tf_record.py --data_dir=`pwd`\ --label_map_path=object_detection/data/pet_lable_map.pbtxt \ --output_dir=`pwd` 需要指定data_dir :图像的源文件夹，output_dir：转化成tfrecord格式 –label_map_path：id与class_name的对应表。 脚本流程 读取anotations文件夹里面的 trainval.txt ，来确定用来训练的图片的文件名，然后通过shuffle,按比例分成训练集和验证集 然后根据训练集或测试集的文件名来读取anotations文件夹里面xmls下面的xml文件，获得图片对象 把图片对象写入文件 生成文件 pet_train.record pet_val.record 运行训练准备数据 这时候把它们拷贝到新创建的文件夹,比如 1234将 object_detection/data/pet_label_map.pbtxt 和 object_detection/samples/configs/faster_rcnn_resnet152_pets.config 两个文件也拷贝到这个目录。将faster_rcnn_resnet152_pets.config文件内容中几个```PATH_TO_BE_CONFIGURED``` 都替换为 ```_pet_20170822```，第 110 和 111 行内容改为： #fine_tune_checkpoint: “_pet_20170822/model.ckpt” from_detection_checkpoint: false1234567**开始训练**需要一个训练脚本 train.py ```cpppython object_detection/train.py --logtostderr --train_dir=_pet_20170822/ --pipeline_config_path=_pet_20170822/faster_rcnn_resnet152_pets.config 初次训练的时候用的是cpu /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory. “Converting sparse IndexedSlices to a dense Tensor of unknown shape. “2017-08-22 07:52:41.925484: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn’t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.2017-08-22 07:52:41.925511: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn’t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.2017-08-22 07:52:41.925517: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn’t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.2017-08-22 07:52:41.925523: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn’t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.2017-08-22 07:52:41.925529: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn’t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.2017-08-22 07:52:42.943663: I tensorflow/core/common_runtime/simple_placer.cc:697] Ignoring device specification /device:GPU:0 for node ‘prefetch_queue_Dequeue’ because the input edge from ‘prefetch_queue’ is a reference connection and already has a device field set to /device:CPU:0 解决这个问题，参考 https://github.com/tensorflow/models/issues/1695 监控训练 1tensorboard --logdir=_pet_20170822 评估训练好的网络12345python object_detection/eval.py \ --logtostderr \ --checkpoint_dir=_pet_20170822 \ --eval_dir=_pet_20170822 \ --pipeline_config_path=_pet_20170822/faster_rcnn_resnet152_pets.config preTrained model注意两点 restore参数的时候注意有些层不要初始化，因为跟原始层不一样 训练的时候之前训练过的参数不需要重新训练了，就frozen起来 123checkpoint_exclude_scopes trainable_scopes referenceprotobuf的配置 tf-slim的用法,用于图像检测和分割 tf-slim官方教程 TensorFlow Object Detection API 实践 Fine-tuning a model from an existing checkpoint Using pre-trained models 图像识别和分类竞赛，数据增强及优化算法 tensorflow固化模型 TensorFlow Mobile模型压缩 腾讯NCNN框架入门到应用 NCNN-squeezenet multi-tracker MobileNet-SSD tensorflow-xnor]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>目标检测</tag>
        <tag>tensorflow</tag>
        <tag>faster-rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发面试问题小结]]></title>
    <url>%2F2017-08-15%2Fcode_interview%2F</url>
    <content type="text"><![CDATA[大数据方面的问题 100亿数字，怎么统计前100大的？分治+堆海量数据处理：十道面试题与十个海量数据处理方法总结数据结构与算法（19）：海量数据处理 C++基础c++的存储区变量还有另一种属性——存储期(storage duration，也称生命期)。存储期是指变量在内存中的存在期间。这是从变量值存在的时间角度来分析的。存储期可以分为静态存储期(static storage duration)和动态存储期(dynamic storage duration)。这是由变量的静态存储方式和动态存储方式决定的。 所谓静态存储方式是指在程序运行期间，系统对变量分配固定的存储空间。而动态存储方式则是在程序运行期间，系统对变量动态地分配存储空间。 先看一下内存中的供用户使用的存储空间的情况。这个存储空间可以分为三部分，即：程序区静态存储区动态存储区 数据分别存放在静态存储区和动态存储区中。全局变量全部存放在静态存储区中，在程序开始执行时给全局变量分配存储单元，程序执行完毕就释放这些空间。在程序执行过程中它们占据固定的存储单元，而不是动态地进行分配和释放。 c++的set和unordered_setSTL中map、set的数据结构及底层实现 c++中的set是经过排序的数据，这里数据的值必须是唯一的，查找插入和删除的时间复杂度都是$O(logn)$ 当数据元素增多时（10000到20000个比较），map和set的插入和搜索速度变化如何？ 1000的数据时set最多比较14次，2000数据时最多比较15次。 unordered_set的查找时间是O(1)，但是建立时间比较长，数据是无序的，而且按照散列函数插入时间比较长。 c++的虚函数c++虚函数 虚函数是C++中用于实现多态(polymorphism)的机制。核心理念就是通过基类访问派生类定义的函数。 虚在所谓“推迟联编”或者“动态联编”上，一个类函数的调 用并不是在编译时刻被确定的，而是在运行时刻被确定的。由于编写代码的时候并不能确定被调用的是基类的函数还是哪个派生类的函数，所以被成为“虚”函数。 编译器发现一个类中有被声明为virtual的函数，就会为其搞一个虚函数表，也就是 VTABLE。VTABLE实际上是一个函数指针的数组，每个虚函数占用这个数组的一个slot。一个类只有一个VTABLE，不管它有多少个实例。派生 类有自己的VTABLE，但是派生类的VTABLE与基类的VTABLE有相同的函数排列顺序，同名的虚函数被放在两个数组的相同位置上。在创建类实例的 时候，编译器还会在每个实例的内存布局中增加一个vptr字段，该字段指向本类的VTABLE。通过这些手段，编译器在看到一个虚函数调用的时候，就会将 这个调用改写。 void bar(A * a){ a-&gt;foo();} 会被改写为： void bar(A * a){ (a-&gt;vptr[1])();} 线程安全智能指针reference大量面经总结(包括牛客网的和我听来的) 海量数据处理：十道面试题与十个海量数据处理方法总结]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>面试问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架的并行优化方法小结]]></title>
    <url>%2F2017-08-11%2Fmpi_parallel%2F</url>
    <content type="text"><![CDATA[目前的深度学习领域就是海量的数据加上大量的数学运算，所以计算量相当的大，训练一个模型跑上十天半个月啥的是常事。那此时分布式的意义就出现了，既然一张GPU卡跑得太慢就来两张，一台机器跑得太慢就用多台机器。 数据并行 每一个节点（或者叫进程）都有一份模型，然后各个节点取不同的数据，通常是一个batch_size，然后各自完成前向和后向的计算得到梯度，这些进行训练的进程我们成为worker，除了worker，还有参数服务器，简称ps server，这些worker会把各自计算得到的梯度送到ps server，然后由ps server来进行update操作，然后把update后的模型再传回各个节点。因为在这种并行模式中，被划分的是数据，所以这种并行方式叫数据并行。 数据并行有同步模式和异步模式之分。同步模式中，所有训练程序同时训练一个批次的训练数据，完成后经过同步，再同时交换参数。参数交换完成后所有的训练程序就有了共同的新模型作为起点，再训练下一个批次。而异步模式中，训练程序完成一个批次的训练数据，立即和参数服务器交换参数，不考虑其他训练程序的状态。异步模式中一个训练程序的最新结果不会立刻体现在其他训练程序中，直到他们进行下次参数交换。 卷积神经网络的并行化模型 parameter serverlimu的parameter server， MSRA的adam和google的tensorflow。 最近比较火的parameter server是什么？ 李沐：Parameter Server for Distributed Machine Learning 参数服务器是个编程框架，用于方便分布式并行程序的编写，其中重点是对大规模参数的分布式存储和协同的支持。 参数服务器就类似于MapReduce，是大规模机器学习在不断使用过程中，抽象出来的框架之一。重点支持的就是参数的分布式，毕竟巨大的模型其实就是巨大的参数。 架构：集群中的节点可以分为计算节点和参数服务节点两种。其中，计算节点负责对分配到自己本地的训练数据（块）计算学习，并更新对应的参数；参数服务节点采用分布式存储的方式，各自存储全局参数的一部分，并作为服务方接受计算节点的参数查询和更新请求。简而言之吧，计算节点负责干活和更新参数，参数服务节点则负责存储参数。 冗余和恢复：类似MapReduce，每个参数在参数服务器的集群中都在多个不同节点上备份（3个也是极好的），这样当出现节点失效时，冗余的参数依旧能够保证服务的有效性。当有新的节点插入时，把原先失效节点的参数从冗余参数那边复制过来，失效节点的接班人就加入队伍了。 并行计算：并行计算这部分主要在计算节点上进行。 类似于MapReduce，分配任务时，会将数据拆分给每个worker节点。参数服务器在开始学习前，也会把大规模的训练数据拆分到每个计算节点上。单个计算节点就对本地数据进行学习就可以了。学习完毕再把参数的更新梯度上传给对应的参数服务节点进行更新。 流程1.分发训练数据 -&gt; 节点1 节点2 节点3 … 节点i … 节点N2.节点i 学习过程：遍历本地的训练数据，统计所有需要的参数(key)向分布式的参数服务器查询需要的参数（注意，本地数据对应到的参数只是全局参数的一小部分）得到查询到的参数值，用于模型的本地训练一轮训练完毕，得到所有对应参数的更新，将更新上传给参数服务器3.参数服务器更新参数过程：参数服务器得到计算节点传过来的局部更新，汇总后更新本地数据 并行程序并行实现实现方式： 任务并行：将任务分配带若干计算核上; 数据并行：将数据进行分割，然后由不同的计算核进行处理，每个核在规模相当的数据集上大致采用相同的操作。这不由使我想到了CAFFE中的对GPU的运用来实现并行训练的思路，就是将数据集进行分割，每个GPU并行处理各自对应的数据集。 多指令多数据流又分为分布式内存系统和共享内存系统。分布式内存系统：每个处理器由独立的内存，通过消息传递函数来通信。共享式内存系统：多个处理器能访问内存系统中的相同内存，通过共享内存进行通信。MPI就是用来在分布式系统中为各处理器进行消息传递的API。 各个核能够直接访问自己的内存，而运行在不同核之间的进程需要交换内存数据的时候，只能通过消息传递API来实现。消息传递的API至少要提供一个发送函数和接收函数。进程之间通过它们的序号（rank）进行识别。 并行程序的流程a、任务或者数据划分，就是要识别出任务中可以进行并行执行的部分。b、不同任务之间的通信;c、聚合，将任务和通信进行集合，聚合成更大的任务;d、分配，将聚合的任务分配到进程或线程中。 1、MPI是进程级别的，通过通信在进程之间进行消息传递。2、编程模型复杂：a、需要进行任务划分;b、通信延迟和负载不均衡;通信延迟很好理解，负载不均衡是因为分布式的系统，每个处理的任务量不同？待进一步的解释 ；c、可靠性差，一个进程出错，整个程序崩溃。第一感觉就是这简直是MPI的命门。在分布式系统中某一个进程出错是很容易的，为MPI的命运担忧。 通信函数一般函数1int MPI_Send (void *buf, int count, MPI_Datatype datatype,int dest, int tag,MPI_Comm comm) 参数buf为发送缓冲区；count为发送的数据个数；datatype为发送的数据类型；dest为消息的目的地址(进程号)，其取值范围为0到np－1间的整数(np代表通信器comm中的进程数) 或MPI_PROC_NULL；tag为消息标签，其取值范围为0到MPI_TAG_UB间的整数；comm为通信器 1mpi_recv:接收信息 MPI_Probe：预测一下消息的size mpi聚合通信collective communication。聚合通信是在通信子中的所有的进程都参与的通信方式。 同步 MPI_BarrierMPI_Barrier就是这样的一个函数，他确保除非所有的进程同时调用，否则他不会允许任何进程通过这个节点对于所有的进程来说，聚合通信必然包含了一个同步点。也就是说所有的进程必须在他们又一次执行新动作之前都到达某个点。这跟GPU中线程同步的概念很相似，很好理解。 广播广播机制：一个进程将相同的数据发送给通信子中所有的进程。该机制最主要的应用是将输入数据发送给并行程序，或者将配置参数发送给所有的进程 1234567MPI_Bcast( void* data,//数据 int count,//数据个数 MPI_Datatype datatype, int root,//根进程编号 MPI_Comm communicator) MPI_Scatter 数据分发MPI_Scatter与MPI_Bcast非常相似，都是一对多的通信方式，不同的是后者的0号进程将相同的信息发送给所有的进程，而前者则是将一段array 的不同部分发送给所有的进程 1234567891011MPI_Scatter( void* send_data,//存储在0号进程的数据，array int send_count,//具体需要给每个进程发送的数据的个数 //如果send_count为1，那么每个进程接收1个数据；如果为2，那么每个进程接收2个数据 MPI_Datatype send_datatype,//发送数据的类型 void* recv_data,//接收缓存，缓存 recv_count个数据 int recv_count, MPI_Datatype recv_datatype, int root,//root进程的编号 MPI_Comm communicator) 通常send_count等于array的元素个数除以进程个数。 MPI_GatherMPI_Gather和MPI_scatter刚好相反，他的作用是从所有的进程中将每个进程的数据集中到根进程中，同样根据进程的编号对array元素排序 12345678910MPI_Gather( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count,//注意该参数表示的是从单个进程接收的数据个数，不是总数 MPI_Datatype recv_datatype, int root, MPI_Comm communicator) MPI_Allgather 多对多通信当数据分布在所有的进程中时，MPI_Allgather将所有的数据聚合到每个进程中。 数据归约 ReduceReduce——规约是来自函数式编程的一个经典概念。数据规约包含通过一个函数将一批数据分成较小的一批数据。比如将一个数组的元素通过加法函数规约为一个数字。 mpi_reduce与MPI_Gather类似，MPI_Reduce在每个进程上都有一组输入元素，并将一个输出元素数组返回给根进程。 输出元素包含被规约的结果。 12345678MPI_Reduce( void* send_data, void* recv_data, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm communicator) send_data参数指向的是每个进程想要规约的datatype类型的元素数组。recv_data仅与根进程相关。recv_data数组包含规约的结果，并具有sizeof（datatype）* count的大小的内存。op参数是要应用于数据的操作。 mpi支持的操作有 MPI_MAX - 返回最大值.MPI_MIN - 返回最小值.MPI_SUM -元素和.MPI_PROD - 元素乘积.MPI_LAND - 逻辑与.MPI_LOR - 逻辑或MPI_BAND -按位与MPI_BOR - 按位或MPI_MAXLOC - 返回最大值和拥有该值的进程编号MPI_MINLOC - 返回最小值和拥有该值的进程编号.1234567891011121314151617181920212223242526如果每个进程中的数组拥有两个元素，那么规约结果是对两个对位的元素进行规约的。![两个元素的归约结果][5]### mpi_allReduce![归约后分发给所有的进程][6]# parameter-server# CUDA C编程## cuda运行时函数cuda运行时提供了丰富的函数，功能涉及设备管理、存储管理、数据传输、线程管理、流管理、事件管理、纹理管理、执行控制等。### 设备管理函数 函数声明一般这样 extern host cudaError_t CUDARTAPI 函数名(参数列表)123456**cudaGetDeviceCount**获得计算能力大于等于1.0的GPU数量```cppint count;cudaGetDeviceCount(&amp;count); cudaSetDevice设置使用的GPU索引号，如果不设置默认使用0号GPU 12int gpuid = 0;cudaSetDevice(gpuid); cudaGetDevice获得当前线程的GPU设备号 12int gpuid;cudaGetDevice(&amp;gpuid); cudaSetValidDevices 设置多个device,len表示签名设备号数组的长度; 1cudaSetValidDevices(int &amp;device_arr, int len); 存储管理函数cudaMalloc 在GPU上分配大小为size的现行存储空间，起始地址为 *devPtr12cudaMalloc(void **devPtr,size_t size); cudaMallocPitch 在GPU上分配大小为PitchxHight的逻辑2D线性存储空间，首地址为其中Pitch是返回的width对齐后的存储空间的宽度123```cppcudaMallocPitch(void **devPtr, size_t *pitch, size_t width, size_t height); 1devPtr[x] = devPtr[rowid*pitch+column] cudaFree清空指定的GPU存储区域，可释放cudaMalloc和cudaMallocPitch分类的GPU存储区域 12cudaFree(void *devPtr); cudaMemset将GPU端的devPtr指针指向的count长度的存储空间赋值为value.1cudaMemset(void 8DevPTR， int value,size_t count); cudaHostAlloc在主机端(CPU)根据flag值来分配页锁定存储, 1cudaHostAlloc(void **pHost, size_t size, usigned int flags); flags可以有四种取值 1234cudaHostAllocDefault 分配默认存储cudaHostAllocPortable 分配的存储可以被cuda索引cudaHostAllocMapped 分配的存储映射到GPU。。。 数据传输函数cudaMemcpy1cudaMemcpy(void * dst, const void *src, size_t count, enum cudaMemcpyKind kind); 主机(cpu内存)与设备间的数据传输函数，源地址是12345```cppcudaMemcpyHostToHost = 0;cudaMemcpyHostToDevice = 0;cudaMemcpyDeviceToHost = 0;cudaMemcpyDeviceToDevice = 0; 还有其它的形式 线程管理函数cudaThreadSynchronize CPU与GPU之间的同步函数，保证该函数前的CPU和GPU上的任务均执行完成，并在该函数位置汇合。一般是CPU在该函数处等待GPU函数执行完。1cudaThreadSynchronize(void); reference《GPU编程与优化》——方民权 referenceMPI学习笔记之并行程序概述 卷积神经网络的并行化模型 知乎 parameter server 分布式机器学习系统笔记（一）——模型并行，数据并行，参数平均，ASGD 深度学习及并行化实现概述]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>caffe</tag>
        <tag>mpi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络的压缩优化方法总结]]></title>
    <url>%2F2017-08-05%2Fcnn_improve%2F</url>
    <content type="text"><![CDATA[这里回顾一下几个经典模型，我们主要看看深度和caffe模型大小，神经网络模型演化。并总结一些模型压缩优化的方法。 模型大小(参数量)和模型的深浅并非是正相关。 一些经典的模型-修改网络架构fully connect to local connect 全连接到卷积神经网络 1x1卷积Alexnet1是一个8层的卷积神经网络，有约60M个参数，如果采用32bit float存下来有200M。值得一提的是，AlexNet中仍然有3个全连接层，其参数量占比参数总量超过了90%。 下面举一个例子，假如输入为28×28×192，输出feature map通道数为128。那么，直接接3×3卷积，参数量为3×3×192×128=221184。 如果先用1×1卷积进行降维到96个通道，然后再用3×3升维到128，则参数量为：1×1×192×96+3×3×96×128=129024，参数量减少一半。虽然参数量减少不是很明显，但是如果1×1输出维度降低到48呢？则参数量又减少一半。对于上千层的大网络来说，效果还是很明显了。 移动端对模型大小很敏感。下载一个100M的app与50M的app，首先用户心理接受程度就不一样。 原则上降低通道数是会降低性能的，这里为什么却可以降维呢？我们可以从很多embedding技术，比如PCA等中得到思考，降低一定的维度可以去除冗余数据，损失的精度其实很多情况下都不会对我们解决问题有很大影响。 1×1卷积，在 GoogLeNet Inception v1以及后续版本，ResNet中都大量得到应用，有减少模型参数的作用。 卷积拆分(1) VGG VGG可以认为是AlexNet的增强版，两倍的深度，两倍的参数量。不过，也提出了一个模型压缩的trick，后来也被广泛借鉴。 那就是，对于5×5的卷积，使用两个3×3的卷积串联，可以得到同样的感受野，但参数量却有所降低，为3×3×2/(5×5)=0.72，同样的道理3个3×3卷积代替一个7×7，则参数压缩比3×3×3/(7×7)=0.55，降低一倍的参数量，也是很可观的。 (2) GoogLeNet GoogleLet Inception v2就借鉴了VGG上面的思想。而到了Inception V3网络，则更进一步，将大卷积分解(Factorization)为小卷积。 比如7×7的卷积，拆分成1×7和7×1的卷积后。参数量压缩比为1×7×2/(7×7)=0.29，比上面拆分成3个3×3的卷积，更加节省参数了。 问题是这种非对称的拆分，居然比对称地拆分成几个小卷积核改进效果更明显，增加了特征多样性。 后来的Resnet就不说了，也是上面这些trick。到现在，基本上网络中都是3×3卷积和1×1卷积，5×5很少见，7×7几乎不可见。 (3) SqueezeNet squeezenet将上面1×1降维的思想进一步拓展。通过减少3×3的filter数量，将其一部分替换为1×1来实现压缩。 具体的一个子结构如下：一个squeeze模块加上一个expand模块，使squeeze中的通道数量，少于expand通道数量就行。 假如输入为M维，如果直接接3×3卷积，输出为7个通道，则参数量：M×3×3×7。 如果按上图的做法，则参数量为M×1×1×3+3×4×1×1+3×4×3×3，压缩比为：(40+M)/21M，当M比较大时，约0.05。 文章最终将AlexNet压缩到原来1/50，而性能几乎不变。 SqueezeNet的核心指导思想是——在保证精度的同时使用最少的参数。而这也是所有模型压缩方法的一个终极目标。 基于这个思想，SqueezeNet提出了3点网络结构设计策略： 策略 1. 将3x3卷积核替换为1x1卷积核。 这一策略很好理解，因为1个1x1卷积核的参数是3x3卷积核参数的1/9，这一改动理论上可以将模型尺寸压缩9倍。 策略 2. 减小输入到3x3卷积核的输入通道数。 我们知道，对于一个采用3x3卷积核的卷积层，该层所有卷积参数的数量（不考虑偏置）为： N是卷积核的数量，也即输出通道数，C是输入通道数。因此，为了保证减小网络参数，不仅仅需要减少3x3卷积核的数量，还需减少输入到3x3卷积核的输入通道数量，即式中C的数量。 策略 3.尽可能的将降采样放在网络后面的层中。 分辨率越大的特征图（延迟降采样）可以带来更高的分类精度，而这一观点从直觉上也可以很好理解，因为分辨率越大的输入能够提供的信息就越多。 上述三个策略中，前两个策略都是针对如何降低参数数量而设计的，最后一个旨在最大化网络精度。 squeeze层借鉴了inception的思想，利用1x1卷积核来降低输入到expand层中3x3卷积核的输入通道数。 定义squeeze层中1x1卷积核的数量是$s_{11}$，类似的，expand层中1x1卷积核的数量是$e_{11}$， 3x3卷积核的数量是$e_33$。令$s_{11} &lt; e_{11}+ e_{33}$从而保证输入到3x3的输入通道数减小。SqueezeNet的网络结构由若干个 fire module 组成 速度考量SqueezeNet在网络结构中大量采用1x1和3x3卷积核是有利于速度的提升的，对于类似caffe这样的深度学习框架，在卷积层的前向计算中，采用1x1卷积核可避免额外的im2col操作，而直接利用gemm进行矩阵加速运算，因此对速度的优化是有一定的作用的。 (4) mobilenet mobilenet也是用卷积拆分的方法 作出更多共享的是有一个width Multiplier宽度参数和resolution Multiplier 分辨率参数 ，可以降低更多的参数。 没使用这两个参数的mobilenet是VGGNet的1/30. 权重参数量化与剪枝主要是通过权重剪枝，量化编码等方法来实现模型压缩。deep compression是float到uint的压缩,Binarized Neural Networks是 uint 到bool的压缩。 一些技巧： 网络修剪采用当网络权重非常小的时候(小于某个设定的阈值),把它置0,就像二值网络一般；然后屏蔽被设置为0的权重更新，继续进行训练；以此循环，每隔训练几轮过后，继续进行修剪。 权重共享对于每一层的参数,我们进行k-means聚类,进行量化,对于归属于同一个聚类中心的权重,采用共享一个权重,进行重新训练.需要注意的是这个权重共享并不是层之间的权重共享,这是对于每一层的单独共享 增加L2权重增加L2权重可以让更多的权重，靠近0，这样每次修剪的比例大大增加。 DeepCompresion 文章早期的工作，是Network Pruning，就是去除网络中权重低于一定阈值的参数后，重新finetune一个稀疏网络。在这篇文章中，则进一步添加了量化和编码，思路很清晰简单如下。 网络剪枝：移除不重要的连接（1） 普通网络训练； （2） 删除权重小于一定阈值的连接得到稀疏网络； （3） 对稀疏网络再训练； 权重量化与共享此处的权值量化基于权值聚类，将连续分布的权值离散化，从而减小需要存储的权值数量。 让许多连接共享同一权重，使原始存储整个网络权重变为只需要存储码本(有效的权重)和索引； 对于一个4×4的权值矩阵，量化权重为4阶（-1.0，0，1.5，2.0）。 对weights采用cluster index进行存储后，原来需要16个32bit float，现在只需要4个32bit float码字，与16个2bit uint索引，参数量为原来的(16×2+4×32)/(16×32)=0.31。 存储是没问题了，那如何对量化值进行更新呢？事实上，文中仅对码字进行更新。如上图：将索引相同的地方梯度求和乘以学习率，叠加到码字。 这样的效果，就等价于不断求取weights的聚类中心。原来有成千上万个weights，现在经过一个有效的聚类后，每一个weights都用其聚类中心进行替代. 看下表就知道最终的压缩效率非常可观，把500M的VGG压缩到到了11M，1/50。 霍夫曼编码：更高效利用了权重的有偏分布；Binarized Neural Networks 提出了一个BWN（Binary-Weight-Network）和XNOR-Network，前者只对网络参数做二值化，带来约32x的存储压缩和2x的速度提升，而后者对网络输入和参数都做了二值化，在实现32x存储压缩的同时带了58x的速度提升； 提出了一个新型二值化权值的算法； 第一个在大规模数据集如ImageNet上提交二值化网络结果的工作； 无需预训练，可实现training from scratch。 referenceCNN 模型压缩与加速算法综述 知乎:为了压榨CNN模型，这几年大家都干了什么 深度学习（六十）网络压缩简单总结 mobile_net的模型优化 squeeze_net的模型优化 神经网络模型演化]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>神经网络</tag>
        <tag>网络优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++使用7年后的经验总结]]></title>
    <url>%2F2017-07-31%2Fcpp_new_feature%2F</url>
    <content type="text"><![CDATA[[TOC] 一年前写了一篇c++11新特性详解, 这里总结了一些c++的编程经验。 c++11 new featurevariadic Templates可变的模板参数，参数量可以变化，举个例子： 对于函数1234567891011121314void print()//对于print没有参数的时候，调用这个函数。&#123;&#125;template &lt;typename T, typename... Types&gt;void print(const T&amp; firstArg, const Types&amp;... args)&#123; cout&lt;&lt;firstArg&lt;&lt;endl; print(args...);//这里使用了递归的方式&#125;//下面的调用方式print(7.5, "hello", 1.3, bitset&lt;16&gt;(377)); 注意了，上面的这种方式由于不知道有多少个参数，所以使用了递归的方式。为了处理最后的状态，上面声明了零参数的print; 如果想知道args有多少个，可以使用sizeof…(args); 这种方式可以把参数逐一分开 对于类tuple的实现就是依赖可变参数 12345678910111213template &lt;typename... Values&gt; class tuple;template&lt;&gt; class tuple&lt;&gt;(); // 处理终止条件template&lt;typename Head, typename... Tail&gt;class tuple&lt;Head, Tail...&gt; :private tuple&lt;Tail...&gt;//实现递归定义&#123; typedef tuple&lt;Tail...&gt; inherited;// 重新起个名字; public : tuple()&#123;&#125; tuple(Head v, Tail... vtail):m_head(v), inherited(vtail)&#123;&#125; protected: Head m_head;&#125;; 右值引用，移动构造函数，与 move函数C++札记 C++11中右值引用与移动构造函数 左值与右值 C++中，可以这么理解：对于一个表达式，凡是对其取地址（&amp;）操作可以成功的都是左值，否则就是右值。 需要注意的是：临时对象是右值 。 左值引用与右值引用 右值引用的形式为：类型 &amp;&amp; a= 被引用的对象 ，此外需要注意的是右值引用只能绑定到右值，如int &amp;&amp; x = 3;而形如 int x = 3; int &amp;&amp; y = x则是错误的，因为x是一个左值。 左值引用只能绑定到左值上。 拷贝构造函数与移动构造函数 拷贝构造函数就是需要拷贝一份样本给自己对象，而移动构造函数就是把样本的内存拿过来自己用。1234Test(Test &amp;&amp; rhs):m_p(rhs.m_p)&#123; rhs.m_p = nullptr;&#125; 但是移动构造函数只能接受右值，所以如果是左值想传入，需要使用std::move()函数。 智能指针C++智能指针 C++ STL为我们提供了四种智能指针：auto_ptr、unique_ptr、shared_ptr和weak_ptr；其中auto_ptr是C++98提供，在C++11中建议摒弃不用。 为什么不建议使用auto_ptr123auto_ptr&lt;int&gt; px(new int(8));auto_ptr&lt;int&gt; py;py = px; 为了避免上述的代码把内存delete两次，智能指针有两种机制。 建立所有权，一个对象同一时刻智能有一个智能指针可以拥有。只有拥有所有权的智能智能才能delete这个对象，unique_ptr和auto_ptr就是这种机制 引用计数，share_ptr就是这么做的 当py=px，后如果再想访问px, 对于auto_ptr会在运行时报segment_fault的错误，而对于unique_ptr则会在编译期就报错。 自己实现share_pre1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#include &lt;iostream&gt;using namespace std;template&lt;class T&gt;class SmartPtr&#123;public: SmartPtr(T *p); ~SmartPtr(); SmartPtr(const SmartPtr&lt;T&gt; &amp;orig); // 浅拷贝 SmartPtr&lt;T&gt;&amp; operator=(const SmartPtr&lt;T&gt; &amp;rhs); // 浅拷贝private: T *ptr; // 将use_count声明成指针是为了方便对其的递增或递减操作 int *use_count;&#125;;template&lt;class T&gt;SmartPtr&lt;T&gt;::SmartPtr(T *p) : ptr(p)&#123; try &#123; use_count = new int(1); &#125; catch (...) &#123; delete ptr; ptr = nullptr; use_count = nullptr; cout &lt;&lt; "Allocate memory for use_count fails." &lt;&lt; endl; exit(1); &#125; cout &lt;&lt; "Constructor is called!" &lt;&lt; endl;&#125;template&lt;class T&gt;SmartPtr&lt;T&gt;::~SmartPtr()&#123; // 只在最后一个对象引用ptr时才释放内存 if (--(*use_count) == 0) &#123; delete ptr; delete use_count; ptr = nullptr; use_count = nullptr; cout &lt;&lt; "Destructor is called!" &lt;&lt; endl; &#125;&#125;template&lt;class T&gt;SmartPtr&lt;T&gt;::SmartPtr(const SmartPtr&lt;T&gt; &amp;orig)&#123; ptr = orig.ptr; use_count = orig.use_count; ++(*use_count); cout &lt;&lt; "Copy constructor is called!" &lt;&lt; endl;&#125;// 重载等号函数不同于复制构造函数，即等号左边的对象可能已经指向某块内存。// 这样，我们就得先判断左边对象指向的内存已经被引用的次数。如果次数为1，// 表明我们可以释放这块内存；反之则不释放，由其他对象来释放。template&lt;class T&gt;SmartPtr&lt;T&gt;&amp; SmartPtr&lt;T&gt;::operator=(const SmartPtr&lt;T&gt; &amp;rhs)&#123; // 《C++ primer》：“这个赋值操作符在减少左操作数的使用计数之前使rhs的使用计数加1， // 从而防止自身赋值”而导致的提早释放内存 ++(*rhs.use_count); // 将左操作数对象的使用计数减1，若该对象的使用计数减至0，则删除该对象 if (--(*use_count) == 0) &#123; delete ptr; delete use_count; cout &lt;&lt; "Left side object is deleted!" &lt;&lt; endl; &#125; ptr = rhs.ptr; use_count = rhs.use_count; cout &lt;&lt; "Assignment operator overloaded is called!" &lt;&lt; endl; return *this;&#125; 一些小更新nullptr可以用来替代NULL,避免因为NULL=0带来的一些混淆。 1234567void f(int);void f(void *);//下面调用f(0);//调用f(int)f(NULL);//调用f(int)f(nullptr);//调用f(void *) auto当type比较长的时候，比如iterator定义的时候可以用auto替换，也可以定义返回值类型。 或者比较复杂：比如用1auto L = [](int x)-&gt;bool&#123;&#125;; 统一的初始化方法-大括号12int values[] &#123;1,2,3&#125;;vector&lt;int&gt; v&#123;2,3,4,5&#125;; effective_Cpp constructors（构造函数）被声明为 explicit（显式）通常比 non-explicit（非显式）更可取，因为它们可以防止编译器执行意外的（常常是无意识的）type conversions（类型转换）。 copy constructor（拷贝构造函数）被用来以一个 object（对象）来初始化同类型的另一个 object（新对象），copy assignment operator（拷贝赋值运算符）被用来将一个 object（对象）中的值拷贝到同类型的另一个 object（对象）。 如果 const 出现在星号左边，则指针 pointed to（指向）的内容为 constant（常量）；如果 const 出现在星号右边，则 pointer itself（指针自身）为 constant。声明一个 iterator 为 const 就类似于声明一个 pointer（指针）为 const（也就是说，声明一个 T* const pointer（指针））：不能将这个 iterator 指向另外一件不同的东西，但是它所指向的东西本身可以变化。如果你要一个 iterator 指向一个不能变化的东西（也就是一个 const T* pointer（指针）的 STL 对等物），你需要一个 const_iterator： 12const std::vector&lt;int&gt;::iterator iterstd::vector&lt;int&gt;::const_iterator cIter member functions（成员函数）在只有 constness（常量性）不同时是可以被 overloaded（重载）的，但这是 C++ 的一个重要特性。 12345const char&amp; operator[](std::size_t position) const // operator[] for&#123; return text[position]; &#125; // const objectschar&amp; operator[](std::size_t position) // operator[] for&#123; return text[position]; &#125; // non-const objects 改变一个返回 built-in type（内建类型）的函数的返回值总是非法的。 mutable 将 non-static data members（非静态数据成员）从 bitwise constness（二进制位常量性）的约束中解放出来： 12345678910111213141516171819202122class CTextBlock &#123;public: std::size_t length() const;private: char *pText; mutable std::size_t textLength; // these data members may mutable bool lengthIsValid; // always be modified, even in&#125;; // const member functionsstd::size_t CTextBlock::length() const&#123; if (!lengthIsValid) &#123; textLength = std::strlen(pText); // now fine lengthIsValid = true; // also fine &#125; return textLength;&#125; 根据 const member function（成员函数）实现它的 non-const 版本的技术却非常值得掌握 12345678910111213141516171819202122232425class TextBlock &#123;public: ... const char&amp; operator[](std::size_t position) const // same as before &#123; ... ... ... return text[position]; &#125; char&amp; operator[](std::size_t position) // now just calls const op[] &#123; return const_cast&lt;char&amp;&gt;( // cast away const on op[]'s return type; static_cast&lt;const TextBlock&amp;&gt;(*this) // add const to *this's type; [position] // call const version of op[] ); &#125;...&#125;; 通常它有更高的效率。assignment-based（基于赋值）的版本会首先调用 default constructors（缺省构造函数）初始化 theName，theAddress 和 thePhones，然而很快又在 default-constructed（缺省构造）的值之上赋予新值。那些 default constructions（缺省构造函数）所做的工作被浪费了。而 member initialization list（成员初始化列表）的方法避免了这个问题，因为initialization list（初始化列表中的 arguments（参数）就可以作为各种 data members（数据成员）的 constructor（构造函数）所使用的 arguments（参数）。在这种情况下，theName 从 name 中 copy-constructed（拷贝构造），theAddress 从 address 中 copy-constructed（拷贝构造），thePhones 从 phones 中 copy-constructed（拷贝构造）。对于大多数类型来说，只调用一次 copy constructor（拷贝构造函数）的效率比先调用一次 default constructor（缺省构造函数）再调用一次 copy assignment operator（拷贝赋值运算符）的效率要高（有时会高很多）。 12345678910ABEntry::ABEntry(const std::string&amp; name, const std::string&amp; address, const std::list&lt;PhoneNumber&gt;&amp; phones): theName(name), theAddress(address), // these are now all initializations thePhones(phones), numTimesConsulted(0)&#123;&#125; 在一个 class（类）内部，data members（数据成员）按照它们被声明的顺序被初始化。例如，在 ABEntry 中，theName 总是首先被初始化，theAddress 是第二个，thePhones 第三，numTimesConsulted 最后。即使它们在 member initialization list（成员初始化列表）中以一种不同的顺序排列（这不幸合法）. 赋值运算符函数需要注意的点，因为当前对象已经经过了初始化，所以已经有了存储空间，那么使用前就需要先释放之前申请的存储空间;另外，在赋值之前需要先判断这两个对象是否是同一个对象。 struct与class的区别，如果没有标明访问权限级别，在struct中默认的是public,而在class中默认的是private.如果 base classes（基类）将 copy assignment operator（拷贝赋值运算符）声明为 private，编译器拒绝为从它继承的 derived classes（派生类）生成 implicit copy assignment operators（隐式拷贝赋值运算符）。如果成员变量中有指针，引用或者const类型就需要自己定义默认构造函数、默认复制构造函数、默认赋值操作符。 将复制构造函数和赋值操作符函数声明为private，同时不给出定义就可以防止被复制。 polymorphic base classes（多态基类）应该声明 virtual destructor（虚拟析构函数）。如果一个 class（类）有任何 virtual functions（虚拟函数），它就应该有一个 virtual destructor（虚拟析构函数）。 高效的拷贝构造函数。 1234567Widget&amp; Widget::operator=(Widget rhs) // rhs is a copy of the object&#123; // passed in — note pass by val swap(rhs); // swap *this's data with // the copy's return *this;&#125; 当你写一个拷贝函数，需要保证（1）拷贝所有本地数据成员以及（2）调用所有基类中的适当的拷贝函数。 123456789PriorityCustomer::operator=(const PriorityCustomer&amp; rhs)&#123; logCall("PriorityCustomer copy assignment operator"); Customer::operator=(rhs); // assign base class parts priority = rhs.priority; return *this;&#125; 不要试图依据类内的一个拷贝函数实现同一类里的另一个拷贝函数。作为代替，将通用功能放入第三个供双方调用的函数 shared_ptr 和auto_ptr能够有效的管理堆上的资源，保证他们能够被释放，share_ptr更好用。而且支持拷贝资源管理。1234567891011class Lock &#123;public: explicit Lock(Mutex *pm) // init shared_ptr with the Mutex : mutexPtr(pm, unlock) // to point to and the unlock func &#123; // as the deleter lock(mutexPtr.get()); // see Item 15 for info on "get" &#125;private: std::tr1::shared_ptr&lt;Mutex&gt; mutexPtr; // use shared_ptr&#125;; shared_ptr 和 auto_ptr 都提供一个 get 成员函数进行显示转换。 在一个独立的语句中将 new 出来的对象存入智能指针。 绝不要返回一个局部栈对象的指针或引用，绝不要返回一个被分配的堆对象的引用，如果存在需要一个以上这样的对象的可能性时，绝不要返回一个局部 static 对象的指针或引用。 在子类中使用与父类public成员函数同名的函数会造成覆盖，继续使用父类函数有两种方式：a.using b. 转调 Item 33: 避免覆盖（hiding）“通过继承得到的名字” referencec++11新特性 effective-cpp c++单例模型]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测算法总结]]></title>
    <url>%2F2017-07-30%2Fobject_detection_with_dl%2F</url>
    <content type="text"><![CDATA[去年总结了一篇关于目标检测的博客 视频智能之——目标检测，今年到现在有了新的体会，所以就更新一篇。 目标检测 检测算法划分目标检测的算法大致可以如下划分： 传统方法： 基于Boosting框架：Haar/LBP/积分HOG/ICF/ACF等特征+Boosting等 基于SVM：HOG+SVM or DPM等 CNN方法： 基于region proposal：以Faster R-CNN为代表的R-CNN家族 基于回归区域划分的：YOLO/SSD 给予强化学习的：attentionNet等。 大杂烩：Mask R-CNN selective search 策略selective search的策略是，因为目标的层级关系，用到了multiscale的思想，那我们就尽可能遍历所有的尺度好了，但是不同于暴力穷举，可以先得到小尺度的区域，然后一次次合并得到大的尺寸就好了。既然特征很多，那就把我们知道的特征都用上，但是同时也要照顾下计算复杂度，不然和穷举法也没啥区别了。最后还要做的是能够对每个区域进行排序，这样你想要多少个候选我就产生多少个。 使用Efficient GraphBased Image Segmentation中的方法来得到region 得到所有region之间两两的相似度 合并最像的两个region 重新计算新合并region与其他region的相似度 重复上述过程直到整张图片都聚合成一个大的region 使用一种随机的计分方式给每个region打分，按照分数进行ranking，取出top k的子集，就是selective search的结果 区域划分与合并首先通过基于图的图像分割方法?初始化原始区域，就是将图像分割成很多很多的小块。然后我们使用贪心策略，计算每两个相邻的区域的相似度，然后每次合并最相似的两块，直到最终只剩下一块完整的图片。然后这其中每次产生的图像块包括合并的图像块我们都保存下来，这样就得到图像的分层表示了呢。 优先合并小的区域 颜色空间多样性作者采用了8中不同的颜色方式，主要是为了考虑场景以及光照条件等。这个策略主要应用于【1】中图像分割算法中原始区域的生成。主要使用的颜色空间有：（1）RGB，（2）灰度I，（3）Lab，（4）rgI（归一化的rg通道加上灰度），（5）HSV，（6）rgb（归一化的RGB），（7）C，（8）H（HSV的H通道） 使用L1-norm归一化获取图像每个颜色通道的25 bins的直方图，这样每个区域都可以得到一个75维的向量。，区域之间颜色相似度通过下面的公式计算： 在区域合并过程中使用需要对新的区域进行计算其直方图，计算方法： 优先合并小的区域. 距离计算多样性 这里的纹理采用SIFT-Like特征。具体做法是对每个颜色通道的8个不同方向计算方差σ=1的高斯微分（GaussianDerivative），每个通道每个方向获取10 bins的直方图（L1-norm归一化），这样就可以获取到一个240维的向量。 还有大小相似度(优先合并小的区域)和吻合相似度(boundingbox是否在一块儿) 给区域打分这篇文章做法是，给予最先合并的图片块较大的权重，比如最后一块完整图像权重为1，倒数第二次合并的区域权重为2以此类推。但是当我们策略很多，多样性很多的时候呢，这个权重就会有太多的重合了，排序不好搞啊。文章做法是给他们乘以一个随机数，毕竟3分看运气嘛，然后对于相同的区域多次出现的也叠加下权重，毕竟多个方法都说你是目标，也是有理由的嘛。 区域的分数是区域内图片块权重之和。 reference目标检测（1）-Selective Search 论文笔记 《Selective Search for Object Recognition》 bounding-box regression详解(回归/微调的对象是什么 referencebounding-box regression详解 RCNNRCNN作为第一篇目标检测领域的深度学习文章。这篇文章的创新点有以下几点：将CNN用作目标检测的特征提取器、有监督预训练的方式初始化CNN、在CNN特征上做BoundingBox 回归。 目标检测区别于目标识别很重要的一点是其需要目标的具体位置，也就是BoundingBox。而产生BoundingBox最简单的方法就是滑窗，可以在卷积特征上滑窗。但是我们知道CNN是一个层次的结构，随着网络层数的加深，卷积特征的步伐及感受野也越来越大。 对于每一个region proposal 都wrap到固定的大小的scale,227x227(AlexNet Input),对于每一个处理之后的图片，把他都放到CNN上去进行特征提取，得到每个region proposal的feature map,这些特征用固定长度的特征集合feature vector来表示。 本文的亮点在于网络结构和训练集 训练集经典的目标检测算法在区域中提取人工设定的特征（Haar，HOG）。本文则需要训练深度网络进行特征提取。可供使用的有两个数据库：一个较大的识别库（ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。一个较小的检测库（PASCAL VOC 2007）：标定每张图片中，物体的类别和位置。一万图像，20类。 整体结构 RCNN的输入为完整图片，首先通过区域建议算法产生一系列的候选目标区域，其中使用的区域建议算法为Selective Search,选择2K个置信度最高的区域候选。 然后对这些候选区域预处理成227 × 227 pixel size ，16 pixels of warped image context around the original box 然后对于这些目标区域候选提取其CNN特征AlexNet，并训练SVM分类这些特征。最后为了提高定位的准确性在SVM分类后区域基础上进行BoundingBox回归。 VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。 CNN目标特征提取 finetune物体检测的一个难点在于，物体标签训练数据少，如果要直接采用随机初始化CNN参数的方法，那么目前的训练数据量是远远不够的。这种情况下，最好的是采用某些方法，把参数初始化了，然后在进行有监督的参数微调，这边文献采用的是有监督的预训练。所以paper在设计网络结构的时候，是直接用Alexnet的网络，然后连参数也是直接采用它的参数，作为初始的参数值，然后再fine-tuning训练。网络优化求解：采用随机梯度下降法，学习速率大小为0.001；最后一层全连接层采用参数随机初始化的方法，其它网络层的参数不变。 RCNN使用ImageNet的有标签数据进行有监督的预训练Alexnet。直到现在，这种方法已经成为CNN初始化的标准化方法。但是训练CNN的样本量还是不能少的，为了尽可能获取最多的正样本，RCNN将IOU&gt;0.5（IoU 指重叠程度，计算公式为：A∩B/A∪B）的样本都称为正样本。每次ieration 的batch_size为128，其中正样本个数为32，负样本为96.其实这种设置是偏向于正样本的，因为正样本的数量实在是太少了。由于CNN需要固定大小的输入，因此对于每一个区域候选，首先将其防缩至227*227，然后通过CNN提取特征。 特定领域的fine-tuning 为了适应不同场合的识别需要，如VOC，对网络继续使用从VOC图片集上对region proposals归一化后的图片进行训练。网络只需要将最后的1000类的分类层换成21类的分类层（20个VOC中的类别和1个背景类），其他都不需要变。为了保证训练只是对网络的微调而不是大幅度的变化，网络的学习率只设置成了0.001。计算每个region proposal与人工标注的框的IoU，IoU重叠阈值设为0.5，大于这个阈值的作为正样本，其他的作为负样本。 svm训练数据是在经过微调的RCNN上取得Fc7层特征，然后训练SVM，并通过BoundingBox回归得到的最终结果。RCNN的SVM训练将ground truth样本作为正样本，而IOU &lt; 0.3的样本作为负样本，这样也是SVM困难样本挖掘的方法。 分类器对每一类目标，使用一个线性SVM二类分类器进行判别。输入为深度网络输出的4096维特征，输出是否属于此类。由于负样本很多，使用hard negative mining方法。正样本本类的真值标定框。负样本考察每一个候选框，如果和本类所有标定框的重叠都小于0.3，认定其为负样本 一旦CNN f7层特征被提取出来，那么我们将为每个物体累训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到2000x4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096xN点（Therefore，the pool5 need to be set as）乘(N为分类类别数目，因为我们训练的N个svm，每个svm包好了4096个W)，就可以得到结果了 贪婪非极大值抑制由于有多达2K个区域候选，我们如何筛选得到最后的区域呢？RCNN使用贪婪非极大值抑制的方法，假设ABCDEF五个区域候选，首先根据概率从大到小排列。假设为FABCDE。然后从最大的F开始，计算F与ABCDE是否IoU是否超过某个阈值，如果超过则将ABC舍弃。然后再从D开始，直到集合为空。而这个阈值是筛选得到的，通过这种处理之后一般只会剩下几个区域候选了。 定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。就这样一直重复，找到所有被保留下来的矩形框。 BoundingBox回归为了进一步提高定位的准确率，RCNN在贪婪非极大值抑制后进行BoundingBox回归，进一步微调BoundingBox的位置。不同于DPM的BoundingBox回归，RCNN是在Pool5层进行的回归。而BoundingBox是类别相关的，也就是不同类的BoundingBox回归的参数是不同的。例如我们的区域候选给出的区域位置为：也就是区域的中心点坐标以及宽度和高度。 目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 回归器对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000.输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 训练样本判定为本类的候选框中，和真值重叠面积大于0.6的候选框。 瓶颈 速度瓶颈：重复为每个region proposal提取特征是极其费时的，Selective Search对于每幅图片产生2K左右个region proposal，也就是意味着一幅图片需要经过2K次的完整的CNN计算得到最终的结果。 性能瓶颈：对于所有的region proposal放缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。 作者提到花费在region propasals和提取特征的时间是13s/张-GPU和53s/张-CPU。 r-cnn有点麻烦，他要先过一次classification得到分类的model，继而在得到的model上进行适当的改变又得到了detection的model，最后才开始在detection model cnn上进行边界检测。 reference目标检测（2）-RCNN RCNN学习笔记(0):rcnn简介 RCNN学习笔记(1): RCNN学习笔记(2) SPPNet论文：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition SPP的优点： 1）任意尺寸输入，固定大小输出2）层多3）可对任意尺度提取的特征进行池化。 图可以看出SPPnet和RCNN的区别，首先是输入不需要放缩到指定大小。其次是增加了一个空间金字塔池化层，使得fc层能够固定参数个数。 空间金字塔池化层是SPPNet的核心 其主要目的是对于任意尺寸的输入产生固定大小的输出。思路是对于任意大小的feature map首先分成16、4、1个块，然后在每个块上最大池化，池化后的特征拼接得到一个固定维度的输出。以满足全连接层的需要。不过因为不是针对于目标检测的，所以输入的图像为一整副图像。 为了简便，在fintune的阶段只修改fc层。 multi-size训练，输入尺寸在[180,224]之间，假设最后一个卷积层的输出大小为a×a，若给定金字塔层有n×n 个bins，进行滑动窗池化，窗口尺寸为win=⌈a/n⌉，步长为str=⌊a/n⌋，使用一个网络完成一个完整epoch的训练，之后切换到另外一个网络。只是在训练的时候用到多尺寸，测试时直接将SPPNet应用于任意尺寸的图像。 reference目标检测（3）-SPPNet fast RCNN无论是RCNN还是SPPNet，其训练都是多阶段的。首先通过ImageNet预训练网络模型，然后通过检测数据集微调模型提取每个区域候选的特征，之后通过softmax分类每个区域候选的种类，最后通过区域回归，精细化每个区域的具体位置。为了避免多阶段训练，同时在单阶段训练中提升识别准确率，Fast RCNN提出了多任务目标函数，将softmax分类以及区域回归的部分纳入了卷积神经网络中。 所有的层在finetune阶段都是可以更新的，使用了truncated SVD方法，MAP是66%. 网络结构 整体框架大致如上述所示,再次几句话总结： 1.用selective search在一张图片中生成约2000个object proposal，即RoI。2.把它们整体输入到全卷积的网络中，在最后一个卷积层上对每个ROI求映射关系，并用一个RoI pooling layer来统一到相同的大小－&gt; (fc)feature vector 即－&gt;提取一个固定维度的特征表示。3.继续经过两个全连接层（FC）得到特征向量。特征向量经由各自的FC层，得到两个输出向量：第一个是分类，使用softmax，第二个是每一类的bounding box回归。 ROI pooling对比SPPNet，首先是将SPP换成了ROI Poling。ROI Poling可以看作是空间金字塔池化的简化版本，它通过将区域候选对应的卷积层特征还分为H*W个块，然后在每个块上进行最大池化就好了。每个块的划分也简单粗暴，直接使用卷积特征尺寸除块的数目就可以了。空间金字塔池化的特征是多尺寸的，而ROI Pooling是单一尺度的。而对于H*W的设定也是参照网络Pooling层的，例如对于VGG-19，网络全连接层输入是7*7*512，因此对应于我们的H,W就分别设置为7，7就可以了。另外一点不同在于网络的输出端，无论是SPPNet还是RCNN，CNN网络都是仅用于特征提取，因此输出端只有网络类别的概率。而Fast RCNN的网络输出是包含区域回归的。 Rol pooling layer的作用主要有两个：1.是将image中的rol定位到feature map中对应patch2.是用一个单层的SPP layer将这个feature map patch下采样为大小固定的feature再传入全连接层。即RoI pooling layer来统一到相同的大小－&gt; (fc)feature vector 即－&gt;提取一个固定维度的特征表示。 muti-task将网络的输出改为两个子网络，一个用以分类（softmax）一个用于回归。最后更改网络的输入，网络的输入是图片集合以及ROI的集合 kx是真实类别，式中第一项是分类损失，第二项是定位损失，L由R个输出取均值而来。 1.对于分类loss，是一个N+1路的softmax输出，其中的N是类别个数，1是背景。为何不用SVM做分类器了？在5.4作者讨论了softmax效果比SVM好，因为它引入了类间竞争。（笔者觉得这个理由略牵强，估计还是实验效果验证了softmax的performance好吧 ）2.对于回归loss，是一个4xN路输出的regressor，也就是说对于每个类别都会训练一个单独的regressor的意思，比较有意思的是，这里regressor的loss不是L2的，而是一个平滑的L1，形式如下： Mini-Batch 采样Mini-Batch的设置基本上与SPPNet是一致的，不同的在于128副图片中，仅来自于两幅图片。其中25%的样本为正样本，也就是IOU大于0.5的，其他样本为负样本，同样使用了困难负样本挖掘的方法，也就是负样本的IOU区间为[0.1，0.5），负样本的u=0，[u&gt; 1]函数为艾弗森指示函数，意思是如果是背景的话我们就不进行区域回归了。在训练的时候，每个区域候选都有一个正确的标签以及正确的位置作为监督信息。 ROI Pooling的反向传播不同于SPPNet，我们的ROI Pooling是可以反向传播的，让我们考虑下正常的Pooling层是如何反向传播的，以Max Pooling为例，根据链式法则，对于最大位置的神经元偏导数为1，对于其他神经元偏导数为0。ROI Pooling 不用于常规Pooling，因为很多的区域建议的感受野可能是相同的或者是重叠的，因此在一个Batch_Size内，我们需要对于这些重叠的神经元偏导数进行求和，然后反向传播回去就好啦。 RoI pooling层计算损失函数对每个输入变量x的偏导数，如下 改算法的结构与思考的问题四个阶段1.Rol pooling layer(fc)2.Multi-task loss(one-stage)3.Scale invariance(trade off-&gt;single scale(compare with multi-scale for decreasing 1mAP) )4.SVD on fc layers(speed up training) 作者提出的问题1.Which layers to finetune?2.Data augment3.Are more proposals always better R-CNN、SPP-net的缺点以及fast-RCnn层的改进R-CNN 训练分多个阶段 每个proposal都要计算convNet特征，并保存在硬盘上。 SPPnet 训练分多个阶段 finetune的时候只微调FC层，一方面这样简单，另一方面spp层不方便做反向传播 fast-RCNN 把多个任务的损失函数写到一起，实现单级的训练过程； 小批量取样，以image为中心取样 在训练时可更新所有的层； 不需要在磁盘中存储特征。 RoI-centric sampling：从所有图片的所有RoI中均匀取样，这样每个SGD的mini-batch中包含了不同图像中的样本。（SPPnet采用）FRCN想要解决微调的限制,就要反向传播到spp层之前的层-&gt;(reason)反向传播需要计算每一个RoI感受野的卷积层，通常会覆盖整个图像，如果一个一个用RoI-centric sampling的话就又慢又耗内存。Fast RCNN:-&gt;改进了SPP-Net在实现上无法同时tuning在SPP layer两边的卷积层和全连接层 image-centric sampling：(solution)mini-batch采用层次取样，先对图像取样，再对RoI取样，同一图像的RoI共享计算和内存。 另外，FRCN在一次微调中联合优化softmax分类器和bbox回归。 reference目标检测（4）-Fast R-CNN RCNN学习笔记(4)：fast rcnn Faster RCNNfaster RCNN可以简单地看做“区域生成网络+fast RCNN“的系统,着重解决系统的三个问题： 如何设计区域生成网络 如何训练区域生成网络 如何让区域生成网络和fast RCNN网络共享特征提取网络 每张图只提300个proposals Fast RCNN提到如果去除区域建议算法的话，网络能够接近实时，而 selective search方法进行区域建议的时间一般在秒级。产生差异的原因在于卷积神经网络部分运行在GPU上，而selective search运行在CPU上，所以效率自然是不可同日而语。一种可以想到的解决策略是将selective search通过GPU实现一遍，但是这种实现方式忽略了接下来的检测网络可以与区域建议方法共享计算的问题。因此Faster RCNN从提高区域建议的速度出发提出了region proposal network 用以通过GPU实现快速的区域建议。通过共享卷积，RPN在测试时的速度约为10ms，相比于selective search的秒级简直可以忽略不计。Faster RCNN整体结构为RPN网络产生区域建议，然后直接传递给Fast RCNN。 faster rcnn 结构对于一幅图片的处理流程为：图片-卷积特征提取-RPN产生proposals-Fast RCNN分类proposals。 feature extraction 特征提取原始特征提取（上图灰色方框）包含若干层conv+relu，直接套用ImageNet上常见的分类网络，额外添加一个conv+relu层，输出5139256维特征（feature）。 region proposal networkRCNN,Fast RCNN,Faster RCNN 总结 区域建议算法一般分为两类：基于超像素合并的（selective search、CPMC、MCG等），基于滑窗算法的。由于卷积特征层一般很小，所以得到的滑窗数目也少很多。但是产生的滑窗准确度也就差了很多，毕竟感受野也相应大了很多。 RPN对于feature map的每个位置进行滑窗，通过不同尺度以及不同比例的K个anchor产生K个256维的向量，然后分类每一个region是否包含目标以及通过回归得到目标的具体位置。 供RPN网络输入的特征图经过RPN网络得到区域建议和区域得分，并对区域得分采用非极大值抑制【阈值为0.7】，输出其Top-N【文中为300】得分的区域建议给RoI池化层； 单个RPN网络结构 上图中卷积层/全连接层表示卷积层或者全连接层，作者在论文中表示这两层实际上是全连接层，但是网络在所有滑窗位置共享全连接层，可以很自然地用n×n卷积核【论文中设计为3×3】跟随两个并行的1×1卷积核实现 RPN的作用：RPN在CNN卷积层后增加滑动窗口操作以及两个卷积层完成区域建议功能，第一个卷积层将特征图每个滑窗位置编码成一个特征向量，第二个卷积层对应每个滑窗位置输出k个区域得分和k个回归后的区域建议，并对得分区域进行非极大值抑制后输出得分Top-N【文中为300】区域，告诉检测网络应该注意哪些区域 anchor Anchors是一组大小固定的参考窗口：三种尺度{ 1282，2562，51221282，2562，5122 }×三种长宽比{1:1，1:2，2:1}，如下图所示，表示RPN网络中对特征图滑窗时每个滑窗位置所对应的原图区域中9种可能的大小。 继而根据图像大小计算滑窗中心点对应原图区域的中心点，通过中心点和size就可以得到滑窗位置和原图位置的映射关系，由此原图位置并根据与Ground Truth重复率贴上正负标签，让RPN学习该Anchors是否有物体即可。因为ground truth 是在原图上的所以要做映射。 平移不变性 Anchors这种方法具有平移不变性，就是说在图像中平移了物体，窗口建议也会跟着平移。 RPN网络的训练 $$L({p_i},{t_i})=\frac{1}{N_{cls}}\sum i L{cls}(p_i,p_i^)+\lambda \frac{1}{N_{reg}}\sum _i p_i ^ L_{reg}(t_i,t_i^*)$$ Lcls是 分类的损失（classification loss），是一个二值分类器（是object或者不是）的softmax loss。其公式为 $$Lcls(p_i,p_i^)=−log[p_ip^*_i+(1−p^⋆_i)(1−pi)]$$ LregLreg 回归损失（regression loss）: RPN网络被ImageNet网络【ZF或VGG-16】进行了有监督预训练，利用其训练好的网络参数初始化； 用标准差0.01均值为0的高斯分布对新增的层随机初始化 PASCAL VOC 数据集中既有物体类别标签，也有物体位置标签； 正样本仅表示前景，负样本仅表示背景； 回归操作仅针对正样本进行； 训练时弃用所有超出图像边界的anchors，否则在训练过程中会产生较大难以处理的修正误差项，导致训练过程无法收敛； 对去掉超出边界后的anchors集采用非极大值抑制，最终一张图有2000个anchors用于训练。文中提到对于1000×600的一张图像，大约有20000(~60×40×9)个anchors，忽略超出边界的anchors剩下6000个anchors，利用非极大值抑制去掉重叠区域，剩2000个区域建议用于训练； 测试时在2000个区域建议中选择Top-N【文中为300】个区域建议用于Fast R-CNN检测。 训练方法4-Step Alternating Training交替训练的方法，思路和迭代的Alternating training有点类似，但是细节有点差别： 第一步：用ImageNet模型初始化，独立训练一个RPN网络；第二步：仍然用ImageNet模型初始化，但是使用上一步RPN网络产生的proposal作为输入，训练一个Fast-RCNN网络，至此，两个网络每一层的参数完全不共享；第三步：使用第二步的Fast-RCNN网络参数初始化一个新的RPN网络，但是把RPN、Fast-RCNN共享的那些卷积层的learning rate设置为0，也就是不更新，仅仅更新RPN特有的那些网络层，重新训练，此时，两个网络已经共享了所有公共的卷积层；第四步：仍然固定共享的那些网络层，把Fast-RCNN特有的网络层也加入进来，形成一个unified network，继续训练，fine tune Fast-RCNN特有的网络层，此时，该网络已经实现我们设想的目标，即网络内部预测proposal并实现检测的功能。 RPN的boundingbox和fast-rcnn的回归的区别 Fast R-CNN中基于RoI的bounding-box回归所输入的特征是在特征图上对任意size的RoIs进行Pool操作提取的，所有size RoI共享回归参数，而在RPN中，用来bounding-box回归所输入的特征是在特征图上相同的空间size【3×3】上提取的，为了解决不同尺度变化的问题，同时训练和学习了k个不同的回归器，依次对应为上述9种anchors，这k个回归量并不分享权重。 Faster R-CNN中三种尺度怎么解释：原始尺度：原始输入的大小，不受任何限制，不影响性能； 归一化尺度：输入特征提取网络的大小，在测试时设置，源码中opts.test_scale=600。anchor在这个尺度上设定，这个参数和anchor的相对大小决定了想要检测的目标范围； 网络输入尺度：输入特征检测网络的大小，在训练时设置，源码中为224×224。 reference目标检测（5）-Faster RCNN RCNN学习笔记(5) faster rcnn 简单粗暴地使用自己数据集训练Faster-RCNN模型 Faster-RCNN+ZF用自己的数据集训练模型(Python版本) 目标检测–Faster RCNN2 如何在faster—rcnn上训练自己的数据集（单类和多类 YoLoYOLO的核心思想就是利用整张图作为网络的输入，直接在输出层回归bounding box的位置和bounding box所属的类别。该方法采用单个神经网络直接预测物品边界和类别概率，实现端到端的物品检测。同时，该方法检测速非常快，基础版可以达到45帧/s的实时检测；FastYOLO可以达到155帧/s。由于可以看到图片的全局信息，所以YOLO的背景预测的假阳性优于当前最好的方法。 Resize成$448448$，图片分割得到$77$网格(cell)CNN提取特征和预测：卷积不忿负责提特征。全链接部分负责预测：a) $772=98$个bounding box(bbox) 的坐标$x_{center},y_{center},w,h$ 和是否有物体的conﬁdence 。b) $7*7=49个cell$所属20个物体的概率。过滤bbox（通过nms） yolo的实现方法网络结构类似于 GoogleNet 预训练分类网络： 在 ImageNet 1000-class competition dataset上预训练一个分类网络，这个网络是Figure3中的前20个卷机网络+average-pooling layer+ fully connected layer （此时网络输入是224*224）。 训练检测网络：转换模型去执行检测任务，《Object detection networks on convolutional feature maps》提到说在预训练网络中增加卷积和全链接层可以改善性能。在他们例子基础上添加4个卷积层和2个全链接层，随机初始化权重。检测要求细粒度的视觉信息，所以把网络输入也又$224224$变成$448448$。 将一幅图像分成SxS个网格(grid cell)，如果某个object的中心 落在这个网格中，则这个网格就负责预测这个object。 每个网格要预测B个bounding box，每个bounding box除了要回归自身的位置之外，还要附带预测一个confidence值。 这个confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息，其值是这样计算的：$$confidence = Pr(Object) \ast IOU^{truth}_{pred}$$ 其中如果有object落在一个grid cell里，第一项取1，否则取0。 第二项是预测的bounding box和实际的groundtruth之间的IoU值。 每个bounding box要预测(x, y, w, h)和confidence共5个值，每个网格还要预测一个类别信息，记为C类。则SxS个网格，每个网格要预测B个bounding box还要预测C个categories。输出就是S x S x (5xB+C)的一个tensor。注意：class信息是针对每个网格的，confidence信息是针对每个bounding box的。 PASCAL VOC中，图像输入为448x448，取S=7，B=2，一共有20个类别(C=20)。则输出就是7x7x30的一个tensor。 在test的时候，每个网格预测的class信息和bounding box预测的confidence信息相乘，就得到每个bounding box的class-specific confidence score:等式左边第一项就是每个网格预测的类别信息，第二三项就是每个bounding box预测的confidence。这个乘积即encode了预测的box属于某一类的概率，也有该box准确度的信息。 得到每个box的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果 实现细节 每个grid有30维，这30维中，8维是回归box的坐标，2维是box的confidence，还有20维是类别。 其中坐标的x,y用对应网格的offset归一化到0-1之间，w,h用图像的width和height归一化到0-1之间。 对不同大小的box预测中，相比于大box预测偏一点，小box预测偏一点肯定更不能被忍受的。而sum-square error loss中对同样的偏移loss是一样。为了缓和这个问题，作者用了一个比较取巧的办法，就是将box的width和height取平方根代替原本的height和width。这个参考下面的图很容易理解，小box的横轴值较小，发生偏移时，反应到y轴上相比大box要大。 作者向预训练模型中加入了4个卷积层和两层全连接层，提高了模型输入分辨率（224×224-&gt;448×448）。顶层预测类别概率和bounding box协调值。bounding box的宽和高通过输入图像宽和高归一化到0-1区间。顶层采用linear activation，其它层使用 leaky rectified linear。作者采用sum-squared error为目标函数来优化，增加bounding box loss权重，减少置信度权重。 更重视8维的坐标预测，给这些损失前面赋予更大的loss weight, 记为 $\lambda_{coord}$ 在pascal VOC训练中取5。对没有object的box的confidence loss，赋予小的loss weight，记为 $\lambda_noobj$ 在pascal VOC训练中取0.5。有object的box的confidence loss和类别的loss的loss weight正常取1。 这个损失函数中：只有当某个网格中有object的时候才对classification error进行惩罚。只有当某个box predictor对某个ground truth box负责的时候，才会对box的coordinate error进行惩罚，而对哪个ground truth box负责就看其预测值和ground truth box的IoU是不是在那个cell的所有box中最大。其他细节，例如使用激活函数使用leak RELU，模型用ImageNet预训练等等 yolo的缺点YOLO对相互靠的很近的物体，还有很小的群体 检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类。 对测试图像中，同一类物体出现的新的不常见的长宽比和其他情况是。泛化能力偏弱。 由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。 referenceYOLO：实时快速目标检测 RCNN学习笔记(6)：You Only Look Once(YOLO) YOLO有史以来讲的最清楚的PPT SSD采用 VGG16 的基础网络结构，使用前面的前 5 层，然后利用 astrous 算法将 fc6 和 fc7 层转化成两个卷积层。再格外增加了 3 个卷积层，和一个 average pool层。不同层次的 feature map 分别用于 default box 的偏移以及不同类别得分的预测，最后通过 nms得到最终的检测结果。 该论文是在 ImageNet 分类和定位问题上的已经训练好的 VGG16 模型中 fine-tuning 得到，使用 SGD，初始学习率为 10^{-3}, 冲量为 0.9，权重衰减为 0.0005，batchsize 为 32。不同数据集的学习率改变策略不同。新增加的卷积网络采用 xavier 的方式进行初始化，输入图片大小是300x300 这些增加的卷积层的 feature map 的大小变化比较大，允许能够检测出不同尺度下的物体： 在低层的feature map,感受野比较小，高层的感受野比较大，在不同的feature map进行卷积，可以达到多尺度的目的。 六尺度检测器 多尺度feature map得到 default boxs及其 4个位置偏移和21个类别置信度。 对于不同尺度feature map（ 上图中 38x38x512，19x19x512, 10x10x512, 5x5x512, 3x3x512, 1x1x256） 的上的所有特征点： 以5x5x256为例 它的#defalut_boxes = 6。 按照不同的 scale 和 ratio 生成，k 个 default boxes，这种结构有点类似于 Faster R-CNN 中的 Anchor。(此处k=6所以：5x5x6 = 150 boxes)scale: 假定使用 m 个不同层的feature map 来做预测，最底层的 feature map 的 scale 值为 $s_{min} = 0.2$，最高层的为 $s_{max} = 0.95$，其他层通过下面公式计算得到 $s_k = s_{min} + \frac{s_{max} - s_{min}}{m - 1}(k-1), k \in [1,m]$ratio: 使用不同的 ratio值$a_r \in \left{1, 2, \frac{1}{2}, 3, \frac{1}{3} \right }$ 计算 default box 的宽度和高度：$w_k^{a} = s_k\sqrt{a_r}，h_k^{a} = s_k/\sqrt{a_r}$。另外对于 ratio = 1 的情况，额外再指定 scale 为$s_k{‘} = \sqrt{s_ks_{k+1}}$ 也就是总共有 6 中不同的 default box。default box中心：上每个 default box的中心位置设置成 $( \frac{i+0.5}{ \left| f_k \right| },\frac{j+0.5}{\left| f_k \right| } )$ ，其中 $\left| f_k \right|$ 表示第k个特征图的大小 $i,j \in [0, \left| f_k \right| )$ 。因为有6个ratio，所以每个位置有6个defaul box. 新增加的每个卷积层的 feature map 都会通过一些小的卷积核操作，得到每一个 default boxes 关于物体类别的21个置信度 ($c_1,c_2 ,\cdots, c_p$ 20个类别和1个背景) 和4偏移 (shape offsets) 。假设feature map 通道数为 p 卷积核大小统一为 $33p$ （此处p=256）。个人猜想作者为了使得卷积后的feature map与输入尺度保持一致必然有 padding = 1， stride = 1 ： $$\frac{ inputFieldSize - kernelSize + 2 \cdot padding }{stride} + 1 = \frac{5 - 3 + 2 \cdot 1}{1} + 1 = 5 $$假如feature map 的size 为$mn$, 通道数为 p，使用的卷积核大小为 $33p$。每个 feature map 上的每个特征点对应 k 个 default boxes，物体的类别数为 c，那么一个feature map就需要使用 $k(c+4)$个这样的卷积滤波器，最后有 $(mn) k (c+4)$个输出。 训练策略正负样本 ==正负样本==： 给定输入图像以及每个物体的 ground truth,首先找到每个ground true box对应的default box中IOU最大的做为（与该ground true box相关的匹配）正样本。然后，在剩下的default box中找到那些与任意一个ground truth box 的 IOU 大于 0.5的default box作为（与该ground true box相关的匹配）正样本。下图的例子是：给定输入图像及 ground truth，分别在两种不同尺度(feature map 的大小为 88，44)下的匹配情况。有两个 default box 与猫匹配（$88$），一个 default box 与狗匹配（$44$）。 目标函数，和常见的 Object Detection 的方法目标函数相同，分为两部分：计算相应的 default box 与目标类别的 score(置信度)以及相应的回归结果（位置回归）。置信度是采用 Softmax Loss（Faster R-CNN是log loss），位置回归则是采用 Smooth L1 loss （与Faster R-CNN一样采用 offset_PTDF靠近 offset_GTDF的策略 referenceRCNN学习笔记(10)：SSD:Single Shot MultiBox Detector 晓雷机器学习笔记-SSD 最好的SSD课程 其它referenceCVPR2017-目标检测相关论文 深度学习实践经验：用Faster R-CNN训练行人检测数据集Caltech——准备工作 深度学习实践经验：用Faster R-CNN训练行人检测数据集Caltech]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mobile_net的模型优化]]></title>
    <url>%2F2017-07-23%2Fmobilenets%2F</url>
    <content type="text"><![CDATA[[TOC] 论文出自google的 MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications。源代码和训练好的模型: tensorflow版本 mobilenet 对于alexnet运行速度提高了10倍，参数量降低了50倍！而squeezenet虽然参数量也降低了50倍，但是速度提升很小。 在建立小型和有效的神经网络上，已经有了一些工作，比如SqueezeNet，Google Inception，Flattened network等等。大概分为压缩预训练模型和直接训练小型网络两种。MobileNets主要关注优化延迟，同时兼顾模型大小。 mobileNets模型结构只有一个avg pooling层，用来替换fc层，少用fc和pooling层就能减少参数量。 深度可分解卷积MobileNets模型基于深度可分解的卷积，它可以将标准卷积分解成一个深度卷积和一个点卷积（1 × 1卷积核）。标准卷积核为：a × a × c，其中a是卷积核大小，c是卷积核的通道数，本文将其一分为二，一个卷积核是a × a × 1，一个卷积核是1 ×1 × c。简单说，就是标准卷积同时完成了2维卷积计算和改变特征数量两件事，本文把这两件事分开做了。后文证明，这种分解可以有效减少计算量，降低模型大小。 标准的卷积核是一步到位，直接计算输出，跨通道的意思是：包含了图征途之间的加权混合。而可分离卷积层把标准卷积层分成两个步骤： 各个卷积层单独卷积 $1x1$卷积核心(1,1,M,N)跨通道结合 首先是标准卷积，假定输入F的维度是 DF×DF×M ，经过标准卷积核K得到输出G的维度 DG×DG×N ，卷积核参数量表示为 DK×DK×M×N 。如果计算代价也用数量表示，应该为 DK×DK×M×N×DF×DF 。 现在将卷积核进行分解，那么按照上述计算公式，可得深度卷积的计算代价为 DK×DK×M×DF×DF ，点卷积的计算代价为 M×N×DF×DF 。 模型结构和训练 MobileNet将95％的计算时间用于有75％的参数的1×1卷积。 宽度参数 Width Multiplier宽度乘数 α ，作用是改变输入输出通道数，减少特征图数量，让网络变瘦。α 取值是0~1，应用宽度乘数可以进一步减少计算量，大约有 $α^2$ 的优化空间。在 α 参数作用下，MobileNets某一层的计算量为：$D_K×D_K×αM×D_F×D_F+αM×αN×D_F×D_F$ 分辨率参数 Resolution Multiplier分辨率乘数用来改变输入数据层的分辨率，同样也能减少参数。在 α 和 ρ 共同作用下，MobileNets某一层的计算量为：$D_K×D_K×αM×ρD_F×ρD_F+αM×αN×ρD_F×ρD_F$ ρ 是隐式参数，ρ 如果为{1，6/7，5/7，4/7}，则对应输入分辨率为{224，192，160，128}，ρ 参数的优化空间同样是 $ρ^2$ 左右. 没有使用这两个参数的mobilenet是vGG的1/30 ,$\alpha = 0.5, \rho = \frac{5}{7}$时是alexnet的1/50,精度提升了0.3% tensorflow官网给出了部署方式，支持android,ios,raspberry Pi等。 持续更新中。。。。。。。。。。。 referencegithub源码 官方的部署方式 深度学习（六十五）移动端网络MobileNets MobileNets 论文笔记 MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications 论文理解 tensorflow训练好的模型怎么调用？ 如何用TensorFlow和TF-Slim实现图像分类与分割]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>神经网络</tag>
        <tag>网络优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[squeeze_net的模型优化]]></title>
    <url>%2F2017-07-20%2Fsqueeze_net%2F</url>
    <content type="text"><![CDATA[SqueezeNet主要是为了降低CNN模型参数数量而设计的。没有提高运行速度。 使用的squeezenet的pre-trained model来自SqueezeNet repo 设计原则（1）替换3x3的卷积kernel为1x1的卷积kernel 卷积模板的选择，从12年的AlexNet模型一路发展到2015年底Deep Residual Learning模型，基本上卷积大小都选择在3x3了，因为其有效性，以及设计简洁性。本文替换3x3的卷积kernel为1x1的卷积kernel可以让参数缩小9X。但是为了不影响识别精度，并不是全部替换，而是一部分用3x3，一部分用1x1。具体可以看后面的模块结构图。 （2）减少输入3x3卷积的input feature map数量如果是conv1-conv2这样的直连，那么实际上是没有办法减少conv2的input feature map数量的。所以作者巧妙地把原本一层conv分解为两层，并且封装为一个Fire Module。 （3）减少pooling 这个观点在很多其他工作中都已经有体现了，比如GoogleNet以及Deep Residual Learning。 同时也替换fc层为 global avg pooling层 Fire ModuleFire Module是本文的核心构件，思想非常简单，就是将原来简单的一层conv层变成两层：squeeze层+expand层，各自带上Relu激活层。在squeeze层里面全是1x1的卷积kernel，数量记为S11；在expand层里面有1x1和3x3的卷积kernel，数量分别记为E11和E33，要求S11 &lt; input map number即满足上面的设计原则（2）。expand层之后将1x1和3x3的卷积output feature maps在channel维度拼接起来。 总体网络架构 共有9层fire module，中间穿插一些max pooling，最后是global avg pooling代替了fc层（参数大大减少）。在开始和最后还有两层最简单的单层conv层，保证输入输出大小可掌握。 比较了alexnet，可以看到准确率差不多的情况下，squeezeNet模型参数数量显著降低了（下表倒数第三行），参数减少50X；如果再加上deep compression技术，压缩比可以达到461X！ 延迟下采样操作在alexnet里，第一层卷积层的stride = 4, 直接下采样了4倍。在一般的CNN中，一般卷积层、池化层都会有下采样(stride&gt;1), 甚至在前面几层网络的下采样比例会比较大，这样会导致后面基层的神经元的激活映射区域减少，为了提高精度设计下采样层延迟的慢一点，这也是squeezenet不能提高速度的真正原因。 持续更新中。。。。。。。。。。。 reference深度学习（六十二）SqueezeNet网络设计思想笔记 深度学习方法（七）：最新SqueezeNet 模型详解]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>梯度下降法</tag>
        <tag>正则化</tag>
        <tag>激活函数</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习理论问题小结]]></title>
    <url>%2F2017-07-10%2Fneural-network-And-DeepLearning%2F</url>
    <content type="text"><![CDATA[[TOC] Michael Nielsen的《Neural Network and Deep Learning》 神经网络基础sigmoid神经元假设在网络的一些权值（或偏移）上做一个小的改变。我们期望的结果是，这些在权值上的小改变，将会为网络的输出结果带来相应的改变，且这种改变也必须是轻微的。我们在后面将会看到，满足这样的性质才能使学习变得可能。我们就可以改变权值和偏移来使得网络的表现越来越接近我们预期。当使用σ函数时我们就得到了一个平滑的感知机。而且，σ函数的平滑属性才是其关键。sigmoid函数的代数形式保证了其在求微分的时候很方便。根据我的经验，很多layer后面都会跟上一个激活函数，激活函数用于将该层之前的输入的线性组合进行非线性运算，如果没有激活函数那么整个神经网络就相当于一层了。另外一些激活函数有梯度消失的问题，当层数过深的时候在后向传播的时候梯度趋近于0,relu能够缓解这个问题，同时能够让一些神经元的梯度为0,来减少运算量。 不同激活函数(activation function)的神经网络的表达能力是否一致？ - 回答作者: 纳米酱 我们也有一些存在回馈环路可能性的人工神经网络模型。这种模型被称为递归神经网络（recurrent neural networks）。该模型的关键在于，神经元在变为非激活态之前会在一段有限时间内均保持激活状态。这种激活状态可以激励其他的神经元，被激励的神经元在随后一段有限时间内也会保持激活状态。如此就会导致更多的神经元被激活，一段时间后我们将得到一个级联的神经元激活系统。 用简单的网络结构解决手写数字识别 损失函数 为什么要介绍平方代价（quadratic cost）呢？毕竟我们最初所感兴趣的内容不是对图像正确地分类么？为什么不增大正确输出的得分，而是去最小化一个像平方代价类似的间接评估呢？这么做是因为在神经网络中，被正确分类的图像的数量所关于权重、偏置的函数并不是一个平滑的函数。大多数情况下，对权重和偏置做出的微小变动并不会影响被正确分类的图像的数量。这会导致我们很难去刻画如何去优化权重和偏置才能得到更好的结果。一个类似平方代价的平滑代价函数能够更好地指导我们如何去改变权重和偏置来达到更好的效果。这就是为何我们集中精力去最小化平方代价，只有通过这种方式我们才能让分类器更精确。 神经网络与深度学习中对为什么梯度下降法能够work做了详尽的解释 ΔC≈∇C⋅Δv. (9)Δv=−η∇C, (10)这里的η是个很小的正数（就是我们熟知的学习速率）。等式（9）告诉我们ΔC≈−η∇C⋅∇C=−η||∇C||2。由于||∇C||2≥0，这保证了ΔC≤0，例如，如果我们以等式（10）的方式去改变v，那么C将一直会降低，不会增加。（当然，要在（9）式的近似约束下）。这就是我们想要的特性！ 为了使我们的梯度下降法能够正确地运行，我们需要选择足够小的学习速率η使得等式（9）能得到很好的近似。 反向传播如果输入神经元是低激活量的，或者输出神经元已经饱和（高激活量或低激活量），那么权重就会学习得缓慢。 反向传播背后的四个基本等式 反向传播的过程 方向传播算法就是将错误量从输出层反向传播。 改进神经网络的方式学习的速度（收敛速度）与很多因素有关，学习率、代价函数的形式、激活函数的形式都有关系。这是使用平方误差作为代价函数。 假设y=0是我们的输出。我们能够从图像看出当神经元输出接近Rendered by QuickLaTeX.com时，曲线变得非常平坦，因此激活函数的导数会变得很小。 交叉熵代价函数与sigmoid激活函数可以用不同的代价函数比如交叉熵（cross-entropy）代价函数来替代二次代价函数来让学习速度加快。 交叉熵函数为什么能作为代价函数交叉熵有两个特性能够合理地解释为何它能作为代价函数。首先，它是非负的; 其次，如果对于所有的训练输入x，这个神经元的实际输出值都能很接近我们期待的输出的话，那么交叉熵将会非常接近0。 。而且交叉熵有另一个均方代价函数不具备的特征，它能够避免学习速率降低的情况。为了理解这个，我们需要计算一下交叉熵关于权重的偏导。因为在计算代价函数关于权值的偏导的时候，sigmoid函数的导数会与交叉熵中导数的一部分抵消掉。 - 当我们的输出层是线性神经元（linear neurons）的时候使用均方误差，假设我们有一个多层神经网络。假设最后一层的所有神经元都是线性神经元（linear neurons）意味着我们不用sigmoid作为激活函数。 激活函数 函数 导数 特点 sigmoid 有饱和状态 tanh tanh保持非线性单调，延迟饱和 ，[-1,1] relu y=max(0,x) 导数为常数 节省计算量，避免梯度丢失，网络稀疏 softplus y=log(1+e^x) softplus可以看作是ReLu的平滑 如何选择为自己的模型选择合适的激活函数 通常使用tanh激活函数要比sigmoid收敛速度更快； 在较深层的神经网络中，选用relu激活函数能使梯度更好地传播回去 当使用softmax作为最后一层的激活函数时，其前一层最好不要使用relu进行激活，而是使用tanh作为替代，否则最终的loss很可能变成Nan； 当选用高级激活函数时，建议的尝试顺序为ReLU-&gt;ELU-&gt;PReLU-&gt;MPELU，因为前两者没有超参数，而后两者需要自己调节参数使其更适应构建的网络结构。 softmax 层的输出是一个概率分布softmax 的单调性，如果J=K，那么为正数， 如果J!=K,则为负数，这个表明了 z增大，则相应的a增大，其它a（输出概率）就见效 softmax 的非局部性，任何一个输出激活值a依赖于所有的输入值。 softmax函数的导数 参考softmax分类 Softmax 输出及其反向传播推导 知乎：多类分类下为什么用softmax而不是用其他归一化方法? free-mind博客，softmax-loss 当 j = k时， ，其中 y=1,所以可以写成 当j!=k, 这样的话当j = k 的时候, ,这个形式非常简洁，而且与线性回归（采用最小均方误差目标函数）、两类分类（采用cross-entropy目标函数）时的形式一致。 其中，损失函数对激活函数的结果a的偏导是top_diff, 而对激活函数的输入z的偏导是bottom_diff。 损失函数是最大似然也是交叉熵 因为softmax的输出是一个概率分布，所以它的损失函数可以使用 最大似然估计 过拟合与正则化检测过拟合一个显而易见的方法就是跟踪网络训练过程中测试数据的准确率。如果测试数据的精度不再提高，就应该停止训练。当然，严格地说，这也不一定就是过拟合的迹象，也许需要同时检测到测试数据和训练数据的精度都不再提高时才行。 在每一步训练之后，计算 validation_data 的分类精度。一旦 validation_data 的分类精度达到饱和，就停止训练。这种策略叫做提前终止（early stopping）。validation_data视为帮助我们学习合适超参数的一种训练数据。由于validation_data和test_data是完全分离开的，所以这种找到优秀超参数的方法被称为分离法（hold out method）。 避免过拟合的手段:增大训练集、减小模型的规模、L2正则化、L1 规范化、弃权(Dropout)。正则化的一种手段叫weight-decay 又叫L2正则。L2 正则的思想是，在代价函数中加入一个额外的正则化项。下面是加入L2正则项之后的交叉熵代价函数。 直观来说，正则化的作用是让网络偏向学习更小的权值，而在其它的方面保持不变。选择较大的权值只有一种情况，那就是它们能显著地改进代价函数的第一部分。换句话说，正则化可以视作一种能够折中考虑小权值和最小化原来代价函数的方法。 而在反向传播过程中，这个正则项只影响权重项。 如果代价函数没有正则化，那么权重向量的长度倾向于增长，而其它的都不变。随着时间推移，权重向量将会变得非常大。这可能导致权重向量被限制得或多或少指向同一个方向，因为当长度过长时，梯度下降只能带来很小的变化。我相信这一现象令我们的学习算法难于恰当地探索权重空间，因而难以给代价函数找到一个好的极小值。 L1正则 L1 规范化，这个方法是在未规范化的代价函数上加上一个权重绝对值的和：其中C0表示的是原来的损失函数，对上面的损失函数求导数得到： 那么给予L1规范化的网络的更新规则为：可以发现正则项的导数是常数。在 L1 规范化中，权重通过一个常量向 0 进行缩小。在 L2规范化中，权重通过一个和 |w| 成比例的量进行缩小的。所以，当一个特定的权重绝对值 |w| 很大时，L1 规范化的权重缩小得远比 L2 规范化要小得多。相反，当一个特定的权重绝对值 |w| 很小时，L1 规范化的权重缩小得要比 L2 规范化大得多。最终的结果就是：L1 规范化倾向于聚集网络的权重在相对少量的高重要度连接上，而其他权重就会被驱使向 0 接近。 弃权（Dropout）dropout并不修改代价函数，而是修改网络本身。主要是用在全连接层。 因为神经元不能依赖其他神经元特定的存在，这个技术其实减少了复杂的互适应的神经元。所以，强制要学习那些在神经元的不同随机子集中更加健壮的特征。 当我们弃权掉不同的神经元集合时，有点像我们在训练不同的神经网络。所以，弃权过程就如同大量不同网络的效果的平均那样。不同的网络会以不同的方式过度拟合了，所以，弃权过的网络的效果会减轻过度拟合。 人为扩展训练数据data augmentation 可以提高网络性能 batch normalization 用在relu前面效果好，是LRN的升级版，把结果映射到0-1之间。 即在每次SGD时，通过mini-batch来对相应的activation做规范化操作，使得结果（输出信号各个维度）的均值为0，方差为1. 而最后的“scale and shift”操作则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入，从而保证整个network的capacity。 1）直接对输入信号的每个维度做规范化（“normalize each scalar feature independently”）；2）在每个mini-batch中计算得到mini-batch mean和variance来替代整体训练集的mean和variance. BN的提出还是为了克服深度神经网络难以训练的弊病.说到底还是为了防止“梯度弥散”。关于梯度弥散，大家都知道一个简单的栗子：$ 0.9^30 = 0.04 $ ,在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。 知乎:深度学习中 Batch Normalization为什么效果好？ 参数初始化输出神经元在错误的值上饱和导致学习的下降。但是隐含层的饱和没法解决，需要用其它的初始化方法。假设我们有一个有个输入权重的神经元。我们会使用均值为 0 标准差为 的高斯随机分布初始化这些权重。也就是说，我们会向下挤压高斯分布，让我们的神经元更不可能饱和。我们会继续使用均值为0 标准差为 1 的高斯分布来对偏置进行初始化。 resnet如何解决了梯度消失问题梯度消失的问题出现的本质原因是梯度连乘问题 而residual 结构出现后，求梯度就成 $$\frac{\partial X_L}{\partial X_{l}} = \frac{\partial X_l + F(X_l,W_l,b_l)}{\partial X_l} = 1 + \frac{\partial F(X_L,W_L,b_L)}{\partial X_L}$$ referenceDeep Residual Network 与 梯度消失 Neural Networks and Deep Learning 解析深度学习中的激活函数 参考：《神经网络与深度学习》连载——哈工大]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>梯度下降法</tag>
        <tag>正则化</tag>
        <tag>激活函数</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络模型演化]]></title>
    <url>%2F2017-07-05%2Fdeep_learning_model%2F</url>
    <content type="text"><![CDATA[[TOC] Lenet，1986年 Alexnet，2012年 GoogleNet，2014年 VGG，2014年 Deep Residual Learning，2015年 网络结构的基础知识 下采样层的目的是为了降低网络训练参数及模型的过拟合程度。 LRN局部响应归一化有利于模型泛化。（过拟合） alexnet做了重叠池化，与lenet不同。也就是说它的pool kernel的步长比kernel size要小。 dropout在每个不同的样本进来的时候用不同的一半的神经元做fc层。但他们共享权重， relu是线性的，非饱和的。收敛速度比sigmoid和tanh快 inception的主要思路是用密集成分来近似局部稀疏结构。网络越到后面，特征越来越抽象，3x3、5x5的卷积的比例也在增加，但是5x5的卷积计算量会大，后续层的参数会多，因此需要用1x1的卷积进行降维 alexNet它证明了CNN在复杂模型下的有效性，然后GPU实现使得训练在可接受的时间范围内得到结果Alexnet有一个特殊的计算层，LRN层，做的事是对当前层的输出结果做平滑处理。 前面的结构 conv - relu - pool - LRN 全连接的结构 fc - relu - dropout googLeNet从这里开始pooling层其实变少了。要想提高CNN的网络能力，比如分类准确率，一般的想法就是增大网络，比如Alexnet确实比以前早期Lenet大了很多，但是纯粹的增大网络——比如把每一层的channel数量翻倍——但是这样做有两个缺点——参数太多容易过拟合，网络计算量也会越来越大。 inception v1 一分四，然后做一些不同大小的卷积，之后再堆叠feature map。这样提取不同尺度的特征，能够提高网络表达能力。 目前很多工作证明，要想增强网络能力，可以：增加网络深度，增加网络宽度；但是为了减少过拟合，也要减少自由参数。因此，就自然而然有了这个第一版的Inception网络结构——同一层里面，有卷积1x1, 3x 3,5x 5 不同的卷积模板，他们可以在不同size的感受野做特征提取，也算的上是一种混合模型了。因为Max Pooling本身也有特征提取的作用，而且和卷积不同，没有参数不会过拟合，也作为一个分支。但是直接这样做，整个网络计算量会较大，且层次并没有变深，因此，在3x3和5x5卷 积前面先做1x1的卷积，降低input的channel数量，这样既使得网络变深，同时计算量反而小了；（在每一个卷积之后都有ReLU） inception v2 v3用1x3和3x1卷积替代3x3卷积，计算量少了很多，深度变深，思路是一样的。（实际上是1xn和nx1替代nxn，n可以变）,使用的是不对称的卷积核 VGG特点也是连续conv多，计算量巨大 resnet特殊之处在于设计了“bottleneck”形式的block（有跨越几层的直连）。最深的model采用的152层 block的结构如下图 Global Average Pooling在Googlenet网络中，也用到了Global Average Pooling，其实是受启发于Network In Network。Global Average Pooling一般用于放在网络的最后，用于替换全连接FC层，为什么要替换FC？因为在使用中，例如alexnet和vgg网络都在卷积和softmax之间串联了fc层，发现有一些缺点： （1）参数量极大，有时候一个网络超过80~90%的参数量在最后的几层FC层中；（2）容易过拟合，很多CNN网络的过拟合主要来自于最后的fc层，因为参数太多，却没有合适的regularizer；过拟合导致模型的泛化能力变弱；（3）实际应用中非常重要的一点，paper中并没有提到：FC要求输入输出是fix的，也就是说图像必须按照给定大小，而实际中，图像有大有小，fc就很不方便； 作者提出了Global Average Pooling，做法很简单，是对每一个单独的feature map取全局average。要求输出的nodes和分类category数量一致，这样后面就可以直接接softmax了。 global avg pooling层没有参数所以不会过拟合。 reference深度学习方法（十一）：卷积神经网络结构变化 卷积神经网络结构变化 卷积神经网络总结/)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>alexnet</tag>
        <tag>googlenet</tag>
        <tag>caffenet</tag>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[heapter-ubfluxdb_grafaba实现k8s集群资源监控]]></title>
    <url>%2F2017-06-30%2Fheapter-ubfluxdb_grafaba%2F</url>
    <content type="text"><![CDATA[在k8s集群中，默认提供的资源监控方式是 cadvisor+influxdb+grafana,K8S已经将cAdvisor功能集成到kubelet组件中。每个Node节点可以直接进行web访问。cAdvisor web界面访问： http://&lt; Node-IP &gt;:4194 方案配置但是cadvisor只能搜集本node上面的资源信息，对于集群中其它结点的资源使用情况检查不了。而heapter是一个只需运行一份就能监控集群中所有node的资源信息，所以我们使用主流的方案：heapter+ubfluxdb+grafaba. heapter用来采集信息，ubfluxdb用来存储，而grafaba用来展示信息。 使用influxdbInfluxDB的不同版本，安装都是通过rpm直接安装，区别只是数据库的“表”不一样而已，所以会影响到Grafana过滤数据，这些不是重点，重点是Grafana数据的清理。 首先下载安装influxdb。在influxdb里找到适合的版本下载安装。 12wget https://repos.influxdata.com/centos/7/x86_64/stable/influxdb-1.2.4.x86_64.rpmrpm -vih influxdb-1.2.4.x86_64.rpm 安装之后发现influxdb需要8086和8088两个端口，但这两个端口经常被占用，所以我们打算使用容器来运行 发现influxdb1.0的才能被grafana使用 使用的镜像是tutum/influxdb:0.8.8,步骤和参数 Docker学习系列3-Influxdb使用入门 Connected to http://localhost:8086 version 0.10.3InfluxDB shell 0.10.3 SHOW DATABASES name: databasesname_internal 新建testdb数据库 CREATE DATABASE testdbSHOW DATABASES name: databasesname_internaltestdb use testdbUsing database testdb 新建root用户 CREATE USER “root” WITH PASSWORD ‘root’ WITH ALL PRIVILEGESshow usersuser adminroot true 插入一条测试数据 INSERT cpu,host=test,region=us_west value=0.64SELECT FROM /./ LIMIT 1 name: cputime host region value1458115488163303455 test us_west 0.64 influxdb的简单操作 influxdb最终的yaml文件为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: monitoring-influxdb namespace: kube-systemspec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: influxdb spec: nodeSelector: ip: six containers: - name: influxdb # image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 image: 10.10.31.26:5000/heapster_influxdb-amd64:0.12.2 volumeMounts: - mountPath: /data name: influxdb-storage volumes: - name: influxdb-storage hostPath: path: /data/kubenetes_data/influxdb---apiVersion: v1kind: Servicemetadata: labels: task: monitoring # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: monitoring-influxdb name: monitoring-influxdb namespace: kube-systemspec: type: NodePort ports: - port: 8086 nodePort: 30031 targetPort: 8086 #type: NodePort #ports: #- port: 8083 # nodePort: 30032 # targetPort: 8083 selector: k8s-app: influxdb 使用grafana使用的grafana版本grafana采用的镜像来自ist0ne的google container ist0ne的github k get deployment –all-namespaces 有两种连接influxdb的方式：一种是proxy,通过influxdb的service名字+port方式一种是direct,通过nodeip+nodeport方式，前者与influxdb具体所在的port无法，但是需要设置好 在yaml文件中指定参数的方式：configure-the-connection-to-influxdb另一个提供更多选项的image 其实我决定grafana的版本还是挺重要的，不同的版本提供了不同的配置项。 最终的yaml文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: monitoring-grafana namespace: kube-systemspec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: grafana spec: containers: - name: grafana # image: gcr.io/google_containers/heapster-grafana-amd64:v4.2.0 image: 10.10.31.26:5000/heapster_grafana-amd64:v4.0.2 ports: - containerPort: 3000 protocol: TCP volumeMounts: - mountPath: /var name: grafana-storage env: - name: INFLUXDB_HOST #value: monitoring-influxdb value: 10.10.31.26 - name: INFLUXDB_PORT value: "30031" - name: GF_SERVER_HTTP_PORT value: "3000" # The following env variables are required to make Grafana accessible via # the kubernetes api-server proxy. On production clusters, we recommend # removing these env variables, setup auth for grafana, and expose the grafana # service using a LoadBalancer or a public IP. - name: GF_AUTH_BASIC_ENABLED value: "false" - name: GF_AUTH_ANONYMOUS_ENABLED value: "true" - name: GF_AUTH_ANONYMOUS_ORG_ROLE value: Admin - name: GF_SERVER_ROOT_URL # If you're only using the API Server proxy, set this value instead: # value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ value: / volumes: - name: grafana-storage emptyDir: &#123;&#125;---apiVersion: v1kind: Servicemetadata: labels: # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: monitoring-grafana name: monitoring-grafana namespace: kube-systemspec: # In a production setup, we recommend accessing Grafana through an external Loadbalancer # or through a public IP. # type: LoadBalancer type: NodePort ports: - port: 80 targetPort: 3000 nodePort: 30030 selector: k8s-app: grafana heapster使用google_container遇到问题：heapster的镜像无法启动; Back-off restarting failed container 参考资料k8s的资源，source-config查看log:1kubectl logs -p --namespace=kube-system heapster-1014378573-6s75z Failed to create source provide: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory 解决方案：1--source=kubernetes:http://10.10.31.25:8080?inClusterConfig=false –sink的设置：https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md 接下来发现问题： Failed to create infuxdb: failed to ping InfluxDB server at “monitoring-influxdb.kube-system.svc:8086” - Get http://monitoring-influxdb.kube-system.svc:8086/ping: dial tcp: lookup monitoring-influxdb.kube-system.svc on 10.10.11.110:53: no such host 解决方法： –sink=influxdb:http://10.10.31.26:30031 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253---apiVersion: v1kind: ServiceAccountmetadata: name: heapster namespace: kube-system---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: heapster namespace: kube-systemspec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: heapster spec: serviceAccountName: heapster containers: - name: heapster # image: gcr.io/google_containers/heapster-amd64:v1.3.0 # image: 10.10.31.26:5000/heapster-amd64:v1.3.0-beta.1 image: 10.10.31.26:5000/heapster-amd64:v1.2.0 imagePullPolicy: IfNotPresent # command: # - /heapster # - --source=kubernetes:https://kubernetes.default # - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086 command: - /heapster - --source=kubernetes:http://10.10.31.25:8080?inClusterConfig=false - --sink=influxdb:http://10.10.31.26:30031---apiVersion: v1kind: Servicemetadata: labels: task: monitoring # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: Heapster name: heapster namespace: kube-systemspec: ports: - port: 80 targetPort: 8082 selector: k8s-app: heapster 其它信息记录grafana 在pod启动两分钟后大概可以画出一条线。grafana停掉重启不会丢失influxdb的数据。9点46断掉influxdb,10点10打开influxdb，发现并没有数据灌入，只有当heapster重启之后才有新数据灌入，而grafana为了描述变化，就使用直线连连接。 referenceKubernetes监控之Heapster介绍 Kubernetes技术分析之监控 部署分布式(cadvisor+influxdb+grafana) Try InfluxDB and Grafana by docker Run Heapster in a Kubernetes cluster with an InfluxDB backend and a Grafana UI]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>heapter</tag>
        <tag>资源监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手把手教你看懂goturn的源码]]></title>
    <url>%2F2017-06-15%2Fgo-turn-learning-note%2F</url>
    <content type="text"><![CDATA[[TOC] goturn 的代码是用caffe写的，是我们学caffe，深度学习和目标跟踪的好的学习资料。 gotrun主页 github主页 代码学习编译问题caffe_dir找不到 修改cmakefile.txt vot_test修改cmakefile.txt 下载阅读源码在docker容器里面运行脚本，但是不能显示结果。 在根目录（GOTURN）运行1bash scripts/show_tracker_test.sh VOT2014 可以用训练好的tracker测试vot2014，看看代码是怎么弄的，打开脚本show_tracker_test.sh，最后一句是 build/show_tracker_vot something 说明是调用的goturn里面这个已经编译好的这个工具show_tracker_vot，那个这个工具的源码在哪里呢。让我们来看看cmakefile吧。 执行1grep -nr show_tracker_vot CMakeLists.txt 输出的是： 99:add_executable (show_tracker_vot src/visualizer/show_tracker_vot.cpp)101:target_link_libraries (show_tracker_vot ${PROJECT_NAME}) 说明这个工具由 src/visualizer/show_tracker_vot.cpp编译得到，那么我们就打开这个文件学习一下源码。下面是代码的主要部分： 12345678910111213// Set up the neural network.const bool do_train = false;Regressor regressor(model_file, trained_file, gpu_id, do_train);Tracker tracker(show_intermediate_output);// Get videos.LoaderVOT loader(videos_folder);std::vector&lt;Video&gt; videos = loader.get_videos();// Visualize the tracker performance.TrackerVisualizer tracker_visualizer(videos, &amp;regressor, &amp;tracker);tracker_visualizer.TrackAll(start_video_num, pause_val); 这里有几个重要的类：Regressor/Tracker/TrackerVisualizer，待会我们一一来看，do_train和show_intermediate_output都是false. 跟踪tracker的结果 在GOTRUN/src下执行1grep -nr Tracker 结果为： tracker/tracker_manager.h:10:class TrackerManagertracker/tracker.h:12:class Trackertrain/tracker_trainer.h:12:class TrackerTrainer tracker三个类：tracker/trackermanager/trackerTrainer tracker.h12345678910111213141516171819202122232425262728293031323334353637class Tracker&#123;public: Tracker(const bool show_tracking);//使用当前图像和指定的回归器即可完成Tracking // Estimate the location of the target object in the current image. virtual void Track(const cv::Mat&amp; image_curr, RegressorBase* regressor, BoundingBox* bbox_estimate_uncentered);//使用image和BoundingBox完成tracker的初始化 // Initialize the tracker with the ground-truth bounding box of the first frame. void Init(const cv::Mat&amp; image_curr, const BoundingBox&amp; bbox_gt, RegressorBase* regressor); // Initialize the tracker with the ground-truth bounding box of the first frame. // VOTRegion is an object for initializing the tracker when using the VOT Tracking dataset. void Init(const std::string&amp; image_curr_path, const VOTRegion&amp; region, RegressorBase* regressor);private: // Show the tracking output, for debugging.可视化跟踪结果 void ShowTracking(const cv::Mat&amp; target_pad, const cv::Mat&amp; curr_search_region, const BoundingBox&amp; bbox_estimate) const; // Predicted prior location of the target object in the current image. // This should be a tight (high-confidence) prior prediction area. We will // add padding to this region.预测的当前帧中目标的bbox BoundingBox bbox_curr_prior_tight_; // Estimated previous location of the target object. 前一帧中估计的位置 BoundingBox bbox_prev_tight_; // Full previous image. 前一帧图像 cv::Mat image_prev_; // Whether to visualize the tracking results 是否要可视化结果 bool show_tracking_;&#125;; 从上面的代码和简单的注释可以看出，比较重要的类是init和track, 接下来我们查看和tracker.h在同一个目录里面的tracker.cpp里面的实现。 下面是init的函数实现，初始化的时候认为目标当前帧的位置和上一帧一致。12345678910111213void Tracker::Init(const cv::Mat&amp; image, const BoundingBox&amp; bbox_gt, RegressorBase* regressor) &#123; image_prev_ = image; bbox_prev_tight_ = bbox_gt; // Predict in the current frame that the location will be approximately the same // as in the previous frame. // TODO - use a motion model? bbox_curr_prior_tight_ = bbox_gt; // Initialize the neural network. regressor-&gt;Init();&#125; 下面是tracker的函数实现， 12345678910111213141516171819202122232425262728293031323334353637void Tracker::Track(const cv::Mat&amp; image_curr, RegressorBase* regressor, BoundingBox* bbox_estimate_uncentered) &#123; // Get target from previous image. cv::Mat target_pad; CropPadImage(bbox_prev_tight_, image_prev_, &amp;target_pad); // Crop the current image based on predicted prior location of target. cv::Mat curr_search_region; BoundingBox search_location; double edge_spacing_x, edge_spacing_y; (bbox_curr_prior_tight_, image_curr, &amp;curr_search_region, &amp;search_location, &amp;edge_spacing_x, &amp;edge_spacing_y);//看来主要是这个Regress能够回归处当前帧中的目标的大致位置; // Estimate the bounding box location of the target, centered and scaled relative to the cropped image. BoundingBox bbox_estimate; regressor-&gt;Regress(image_curr, curr_search_region, target_pad, &amp;bbox_estimate); // Unscale the estimation to the real image size.当前帧中估计出的紧密的位置; BoundingBox bbox_estimate_unscaled; bbox_estimate.Unscale(curr_search_region, &amp;bbox_estimate_unscaled); // Find the estimated bounding box location relative to the current crop. bbox_estimate_unscaled.Uncenter(image_curr, search_location, edge_spacing_x, edge_spacing_y, bbox_estimate_uncentered); if (show_tracking_) &#123; ShowTracking(target_pad, curr_search_region, bbox_estimate); &#125; // Save the image. image_prev_ = image_curr; // Save the current estimate as the location of the target. bbox_prev_tight_ = *bbox_estimate_uncentered; // Save the current estimate as the prior prediction for the next image. // TODO - replace with a motion model prediction? bbox_curr_prior_tight_ = *bbox_estimate_uncentered;&#125; 这段代码里有两处比较重要：一处是regressor-&gt;regess用来回归出当前帧中的扩充位置，具体在下面分析，第二处是 BoundingBox bbox_estimate_unscaled;这个BoundingBox结构体。从前面include的头文件可以看出这个boundingbox定义在 #include “helper/bounding_box.h”，这么看来这个helper文件夹里定义的都是辅助函数。我们来看一下这个目录里都定义了什么？12root@aa0afc645153:helper# lsbounding_box.cpp bounding_box.h helper.cpp helper.h high_res_timer.cpp high_res_timer.h image_proc.cpp image_proc.h 具体boundingbox定义了什么，后面分析，先回归正题。 注意到track函数里面，两次调用了CropPadImage函数，我们就来对这个函数分析一下，先追踪一个这个函数在那儿定义的呢？在src目录下执行：1grep -nr CropPadImage 得到结果 helper/image_proc.cpp:58:void CropPadImage(const BoundingBox&amp; bbox_tight, const cv::Mat&amp; image, cv::Mat pad_image,helper/image_proc.cpp:63: ComputeCropPadImageLocation(bbox_tight, image, pad_image_location);helper/image_proc.h:11:void CropPadImage(const BoundingBox&amp; bbox_tight, const cv::Mat&amp; image, cv::Mat pad_image);helper/image_proc.h:12:void CropPadImage(const BoundingBox&amp; bbox_tight, const cv::Mat&amp; image, cv::Mat pad_image,helper/image_proc.h: 18:void ComputeCropPadImageLocation(const BoundingBox&amp; bbox_tight, const cv::Mat&amp; image, BoundingBox pad_image_location); 同样这个辅助函数也定义在helper路径下面，那么既然绕不开，我们就来看一看吧。 跟踪regressor的结果执行：1grep -nr Regressor 结果为: network/regressor.h:17:class Regressor : public RegressorBase {network/regressor.h:22: Regressor(const std::string&amp; train_deploy_proto,network/regressor.h:28: Regressor(const std::string&amp; train_deploy_proto,network/regressor_train_base.cpp:7:RegressorTrainBase::RegressorTrainBase(const std::string&amp; solver_file)network/regressor_base.h:14:class RegressorBasenetwork/regressor_base.h:17: RegressorBase();network/regressor_train_base.h:32:class RegressorTrainBasenetwork/regressor_train_base.h:35: RegressorTrainBase(const std::string&amp; solver_file);network/regressor_train.h:7:class RegressorTrain : public Regressor, public RegressorTrainBasenetwork/regressor_train.h:10: RegressorTrain(const std::string&amp; deploy_proto,network/regressor_train.h:16: RegressorTrain(const std::string&amp; deploy_proto,network/regressor.cpp:16:Regressor::Regressor(const string&amp; deploy_proto,network/regressor.cpp:29:Regressor::Regressor(const string&amp; deploy_proto,network/regressor_train.cpp:10:RegressorTrain::RegressorTrain(const std::string&amp; deploy_proto, 从结果可以看出总共regressor这个类主要在三个头文件里： regressor.h, regressor_train_base.cpp, regressor_train.h。所以我们先从头文件regression.h下手。 1class Regressor : public RegressorBase 从这句话可以看出类的继承关系，regressor继承自regressorbase, 而从上面的头文件可知，regressorbase类定义在network/regressor_base.h里面。 跟踪TrackerVisualizer的结果 12345678grep -nr TrackerVisualizertracker/tracker_manager.h:54:class TrackerVisualizer : public TrackerManagertracker/tracker_manager.h:57: TrackerVisualizer(const std::vector&lt;Video&gt;&amp; videos,tracker/tracker_manager.cpp:71:TrackerVisualizer::TrackerVisualizer(const std::vector&lt;Video&gt;&amp; videos,tracker/tracker_manager.cpp:78:void TrackerVisualizer::ProcessTrackOutput(tracker/tracker_manager.cpp:101:void TrackerVisualizer::VideoInit(const Video&amp; video, const size_t video_num) &#123;visualizer/show_tracker_alov.cpp:63: TrackerVisualizer tracker_visualizer(videos, &amp;regressor, &amp;tracker);visualizer/show_tracker_vot.cpp:60: TrackerVisualizer tracker_visualizer(videos, &amp;regressor, &amp;tracker);]]></content>
      <categories>
        <category>深度学习</category>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>deeplearning</tag>
        <tag>caffe</tag>
        <tag>computervision</tag>
        <tag>tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe-源码学习——只看一篇就够了]]></title>
    <url>%2F2017-06-12%2Fcaffe_code_study1%2F</url>
    <content type="text"><![CDATA[网络模型说caffe代码难懂，其实关键点在于caffe中有很多基础的数学运算代码，如果能够对掌握这些数学运算，剩下的就是推公式了。 激活函数sigmoid看softmax函数之前先看一下简单的sigmoid, 这个sigmoid layer的cpp实现是非常简洁的。 sigmoid的cpp文件里主要给了三个函数的实现，分别是sigmoid函数，forward_cpu, backward_cpu,在cpp文件里只实现了算法的CPU版本，至于GPU版本的函数实现放在.cu文件里面。12345678910111213141516171819202122232425262728293031template &lt;typename Dtype&gt;inline Dtype sigmoid(Dtype x) &#123; return 0.5 * tanh(0.5 * x) + 0.5;&#125;template &lt;typename Dtype&gt;void SigmoidLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123; const Dtype* bottom_data = bottom[0]-&gt;cpu_data(); Dtype* top_data = top[0]-&gt;mutable_cpu_data(); const int count = bottom[0]-&gt;count(); for (int i = 0; i &lt; count; ++i) &#123; top_data[i] = sigmoid(bottom_data[i]); &#125;&#125;template &lt;typename Dtype&gt;void SigmoidLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123; if (propagate_down[0]) &#123; const Dtype* top_data = top[0]-&gt;cpu_data(); const Dtype* top_diff = top[0]-&gt;cpu_diff(); Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff(); const int count = bottom[0]-&gt;count(); for (int i = 0; i &lt; count; ++i) &#123; const Dtype sigmoid_x = top_data[i]; bottom_diff[i] = top_diff[i] * sigmoid_x * (1. - sigmoid_x); &#125; &#125;&#125; sigmoid函数注意这里的sigmoid函数与标准的定义不太一样。参见ufld里面的定义神经网络UFLD 而在这里 sigmoid = 0.5 tanh(0.5 x) + 0.5, sigmoid变化范围为从0-1, tanh从-1到1，乘于0.5再加上0.5两者变化范围就一样了。forward_cpu这个很容易就能看懂，就是对每一个bottom元素计算sigmoid就得到来top的元素。 backward_cpu发现新版的代码真的很好懂，sigmoid函数的到函数是sigmoid*(1-sigmoid) , 所以这里就直接利用来。其中propagate_down表明这一层是否要反传。 softmaxlayer这段代码比较复杂，比较好的注释如下。但是这个注释针对的代码版本比较老。caffe深度学习网络softmax层代码注释 这里我们分析比较新的代码，当前(20170622)比较新的代码是20161202提交的代码，结构如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/template &lt;typename Dtype&gt;void SoftmaxLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123; softmax_axis_ = bottom[0]-&gt;CanonicalAxisIndex(this-&gt;layer_param_.softmax_param().axis()); //softmax层的输出应该与输入层一致 top[0]-&gt;ReshapeLike(*bottom[0]); vector&lt;int&gt; mult_dims(1, bottom[0]-&gt;shape(softmax_axis_)); sum_multiplier_.Reshape(mult_dims); Dtype* multiplier_data = sum_multiplier_.mutable_cpu_data(); caffe_set(sum_multiplier_.count(), Dtype(1), multiplier_data); outer_num_ = bottom[0]-&gt;count(0, softmax_axis_); inner_num_ = bottom[0]-&gt;count(softmax_axis_ + 1); vector&lt;int&gt; scale_dims = bottom[0]-&gt;shape(); // scale_尺寸为：num*1*height*width scale_dims[softmax_axis_] = 1; scale_.Reshape(scale_dims);&#125;//前向计算，得到softmax的值template &lt;typename Dtype&gt;void SoftmaxLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123; const Dtype* bottom_data = bottom[0]-&gt;cpu_data(); Dtype* top_data = top[0]-&gt;mutable_cpu_data(); Dtype* scale_data = scale_.mutable_cpu_data(); int channels = bottom[0]-&gt;shape(softmax_axis_); int dim = bottom[0]-&gt;count() / outer_num_; caffe_copy(bottom[0]-&gt;count(), bottom_data, top_data); // We need to subtract the max to avoid numerical issues, compute the exp, // and then normalize. // 先找到最大值 for (int i = 0; i &lt; outer_num_; ++i) &#123;//outer_num就是num输出数据的数目 // initialize scale_data to the first plane caffe_copy(inner_num_, bottom_data + i * dim, scale_data);//dim表示每个数据有多少个不同类别的值. for (int j = 0; j &lt; channels; j++) &#123; for (int k = 0; k &lt; inner_num_; k++) &#123; scale_data[k] = std::max(scale_data[k],//每个元素表示应该是当前位置中所有类别和channel里面最大的那一个。 bottom_data[i * dim + j * inner_num_ + k]); &#125; &#125; // subtraction 减去最大值 详细分析见后面 caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, inner_num_, 1, -1., sum_multiplier_.cpu_data(), scale_data, 1., top_data); // exponentiation 求指数 caffe_exp&lt;Dtype&gt;(dim, top_data, top_data); // sum after exp 求和 caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, channels, inner_num_, 1., top_data, sum_multiplier_.cpu_data(), 0., scale_data); // division 做除法 for (int j = 0; j &lt; channels; j++) &#123; caffe_div(inner_num_, top_data, scale_data, top_data); top_data += inner_num_; &#125; &#125;&#125;template &lt;typename Dtype&gt;void SoftmaxLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123; const Dtype* top_diff = top[0]-&gt;cpu_diff(); const Dtype* top_data = top[0]-&gt;cpu_data(); Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff(); Dtype* scale_data = scale_.mutable_cpu_data(); int channels = top[0]-&gt;shape(softmax_axis_); int dim = top[0]-&gt;count() / outer_num_; caffe_copy(top[0]-&gt;count(), top_diff, bottom_diff); for (int i = 0; i &lt; outer_num_; ++i) &#123; // compute dot(top_diff, top_data) and subtract them from the bottom diff //计算top_diff与top_data的点集 for (int k = 0; k &lt; inner_num_; ++k) &#123; scale_data[k] = caffe_cpu_strided_dot&lt;Dtype&gt;(channels, bottom_diff + i * dim + k, inner_num_, top_data + i * dim + k, inner_num_); &#125; // subtraction caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, inner_num_, 1, -1., sum_multiplier_.cpu_data(), scale_data, 1., bottom_diff + i * dim); &#125; // elementwise multiplication caffe_mul(top[0]-&gt;count(), bottom_diff, top_data, bottom_diff);&#125;#ifdef CPU_ONLYSTUB_GPU(SoftmaxLayer);#endifINSTANTIATE_CLASS(SoftmaxLayer); caffe_cpu_gemm减法 在forward_cpu函数里做减法的时候调用来caffe_cpu_gemm函数，这个函数的实现在 src/caffe/util/math_functions.cpp里面caffecpugemm-函数12345678910template&lt;&gt;void caffe_cpu_gemm&lt;float&gt;(const CBLAS_TRANSPOSE TransA, const CBLAS_TRANSPOSE TransB, const int M, const int N, const int K, const float alpha, const float* A, const float* B, const float beta, float* C) &#123; int lda = (TransA == CblasNoTrans) ? K : M; int ldb = (TransB == CblasNoTrans) ? N : K; cblas_sgemm(CblasRowMajor, TransA, TransB, M, N, K, alpha, A, lda, B, ldb, beta, C, N);&#125; 功能： C=alphaAB+beta*CA,B,C 是输入矩阵（一维数组格式）CblasRowMajor :数据是行主序的（二维数据也是用一维数组储存的）TransA, TransB：是否要对A和B做转置操作（CblasTrans CblasNoTrans）M： A、C 的行数N： B、C 的列数K： A 的列数， B 的行数lda ： A的列数（不做转置）行数（做转置）ldb： B的列数（不做转置）行数（做转置） 所以这里求减法：12caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, inner_num_, 1, -1., sum_multiplier_.cpu_data(), scale_data, 1., top_data); 就是： $top_data = top_data - 1 sum_multiplier_.cpu_data()scale_data$， 这里用top_data来减而不用bottom一方面是因为bottom是const的，取的是cpu_data(), 而top_data是mutable_cpu_data,另一方面之前已经把数据从bottom拷贝到top里面去了。 而在back_ward函数里面也用到了这个函数。12caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, inner_num_, 1, -1., sum_multiplier_.cpu_data(), scale_data, 1., bottom_diff + i * dim); $$bottom_diff + i dim = bottom_diff + i dim - sum_multiplier_.cpu_data() * scale_data$$ caffe_exp指数caffe_exp函数是用来求指数的，其中一个实现是这样的。1234template &lt;&gt;void caffe_exp&lt;float&gt;(const int n, const float* a, float* y) &#123; vsExp(n, a, y);&#125; 功能： y[i] = exp(a[i] ) 所以， forward_cpu里面的指数就很容易理解了。1caffe_exp&lt;Dtype&gt;(dim, top_data, top_data); top_data[i] = exp(topdata[i])caffe_cpu_gemv求和123456template &lt;&gt;void caffe_cpu_gemv&lt;float&gt;(const CBLAS_TRANSPOSE TransA, const int M, const int N, const float alpha, const float* A, const float* x, const float beta, float* y) &#123; cblas_sgemv(CblasRowMajor, TransA, M, N, alpha, A, N, x, 1, beta, y, 1);&#125; 功能： y=alphaAx+beta*y其中X和Y是向量，A 是矩阵M：A 的行数N：A 的列数cblas_sgemv 中的 参数1 表示对X和Y的每个元素都进行操作 forward_cpu里面的求和就很容易理解了123// sum after exp 求和caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, channels, inner_num_, 1., top_data, sum_multiplier_.cpu_data(), 0., scale_data); $scale_data =\sum top_data[i]*sum_multiplier_.cpu_data()[i];$ caffe_div除法12345template &lt;&gt;void caffe_div&lt;float&gt;(const int n, const float* a, const float* b, float* y) &#123; vsDiv(n, a, b, y);&#125; 功能 y[i] = a[i] / b[i] 1234// division 做除法for (int j = 0; j &lt; channels; j++) &#123; caffe_div(inner_num_, top_data, scale_data, top_data); top_data += inner_num_; $top_data[i] = top_data[i] / scale_data[i];$ caffe_cpu_strided_dot12345template &lt;&gt;double caffe_cpu_strided_dot&lt;double&gt;(const int n, const double* x, const int incx, const double* y, const int incy) &#123; return cblas_ddot(n, x, incx, y, incy);&#125; 功能： 返回 vector X 和 vector Y 的内积。incx， incy ： 步长，即每隔incx 或 incy 个element 进行操作。 caffe_mul12345template &lt;&gt;void caffe_mul&lt;float&gt;(const int n, const float* a, const float* b, float* y) &#123; vsMul(n, a, b, y);&#125; 功能 $y[i]=a[i] * b[i]$1caffe_mul(top[0]-&gt;count(), bottom_diff, top_data, bottom_diff); bottom_diff[i] = bottom_diff[i] * top_data[i] 反向传播公式推导Caffe Softmax层的实现原理,知乎 看完softmax layer的实现，我们再来看一下SoftmaxWithLossLayer的代码实现。 卷积层计算量与参数量每个样本做一次前向传播时卷积计算量为： $ i jMNKL $ ，其中$ij$是卷积核的大小，$M*L$是输出特征图的大小，K是输入特征图通道数，L是输出特征图通道数。 参数量为：$ iJK*L $ 所以有个比例叫做计算量参数量之比 CPR，如果在前馈时每个批次batch_size = B, 则表示将B个输入合并成一个矩阵进行计算，那么相当于每次的输出特征图增大来B倍，所以CPR提升来B倍，也就是，每次计算的时候参数重复利用率提高来B倍。 卷积层：局部互连，权值共享， 源码学习先用grep函数在caffe根目录下搜索一下包含ConvolutionLayer的文件有哪些，然后从头文件入手慢慢分析，下面是结果，精简来一些无效成分，在caffe的include文件夹下执行：1grep -n -H -R "ConvolutionLayer" -n表示显示行号，-H表示显示文件名，-R表示递归查找 后面部分表示查找的内容结果如下123456789101112131415161718192021caffe/layer_factory.hpp:31: * (for example, when your layer has multiple backends, see GetConvolutionLayercaffe/layers/base_conv_layer.hpp:15: * ConvolutionLayer and DeconvolutionLayer.caffe/layers/base_conv_layer.hpp:18:class BaseConvolutionLayer : public Layer&lt;Dtype&gt; &#123;caffe/layers/base_conv_layer.hpp:20: explicit BaseConvolutionLayer(const LayerParameter&amp; param)caffe/layers/deconv_layer.hpp:17: * opposite sense as ConvolutionLayer.caffe/layers/deconv_layer.hpp:19: * ConvolutionLayer computes each output value by dotting an input window withcaffe/layers/deconv_layer.hpp:22: * DeconvolutionLayer is ConvolutionLayer with the forward and backward passescaffe/layers/deconv_layer.hpp:24: * parameters, but they take the opposite sense as in ConvolutionLayer (socaffe/layers/deconv_layer.hpp:29:class DeconvolutionLayer : public BaseConvolutionLayer&lt;Dtype&gt; &#123;caffe/layers/deconv_layer.hpp:32: : BaseConvolutionLayer&lt;Dtype&gt;(param) &#123;&#125;caffe/layers/im2col_layer.hpp:14: * column vectors. Used by ConvolutionLayer to perform convolutioncaffe/layers/conv_layer.hpp:31:class ConvolutionLayer : public BaseConvolutionLayer&lt;Dtype&gt; &#123;caffe/layers/conv_layer.hpp:35: * with ConvolutionLayer options:caffe/layers/conv_layer.hpp:64: explicit ConvolutionLayer(const LayerParameter&amp; param)caffe/layers/conv_layer.hpp:65: : BaseConvolutionLayer&lt;Dtype&gt;(param) &#123;&#125;caffe/layers/cudnn_conv_layer.hpp:16: * @brief cuDNN implementation of ConvolutionLayer.caffe/layers/cudnn_conv_layer.hpp:17: * Fallback to ConvolutionLayer for CPU mode.caffe/layers/cudnn_conv_layer.hpp:30:class CuDNNConvolutionLayer : public ConvolutionLayer&lt;Dtype&gt; &#123;caffe/layers/cudnn_conv_layer.hpp:32: explicit CuDNNConvolutionLayer(const LayerParameter&amp; param)caffe/layers/cudnn_conv_layer.hpp:33: : ConvolutionLayer&lt;Dtype&gt;(param), handles_setup_(false) &#123;&#125;caffe/layers/cudnn_conv_layer.hpp:38: virtual ~CuDNNConvolutionLayer(); 主要有三个类包含这个卷积层的实现：base_conv_layer：主要是卷积层基类的实现deconv_layer： 目测是反向传播时候的卷积层的逆向过程cudnn_conv_layer：目测是cudnn实现的卷积层版本继承自BaseConvolutionLayer,GPU版本 接下来我们就打开这三个文件，跳转到相关行，详细看一下。123456789101112131415161718192021222324252627282930313233343536class BaseConvolutionLayer : public Layer&lt;Dtype&gt; &#123; public: explicit BaseConvolutionLayer(const LayerParameter&amp; param) : Layer&lt;Dtype&gt;(param) &#123;&#125; virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); virtual inline int MinBottomBlobs() const &#123; return 1; &#125; virtual inline int MinTopBlobs() const &#123; return 1; &#125; virtual inline bool EqualNumBottomTopBlobs() const &#123; return true; &#125; protected: // Helper functions that abstract away the column buffer and gemm arguments. // The last argument in forward_cpu_gemm is so that we can skip the im2col if // we just called weight_cpu_gemm with the same input. void forward_cpu_gemm(const Dtype* input, const Dtype* weights, Dtype* output, bool skip_im2col = false); void forward_cpu_bias(Dtype* output, const Dtype* bias); void backward_cpu_gemm(const Dtype* input, const Dtype* weights, Dtype* output); void weight_cpu_gemm(const Dtype* input, const Dtype* output, Dtype* weights); void backward_cpu_bias(Dtype* bias, const Dtype* input);#ifndef CPU_ONLY void forward_gpu_gemm(const Dtype* col_input, const Dtype* weights, Dtype* output, bool skip_im2col = false); void forward_gpu_bias(Dtype* output, const Dtype* bias); void backward_gpu_gemm(const Dtype* input, const Dtype* weights, Dtype* col_output); void weight_gpu_gemm(const Dtype* col_input, const Dtype* output, Dtype* weights); void backward_gpu_bias(Dtype* bias, const Dtype* input);#endif 这里给出来CPU和GPU版本的代码的声明，这些代码比较底层，先放一放以后再看。forward_cpu_gemm:猜测可能是前馈过程计算weight部分，来看看CPP里面的实现吧。 在BaseConvolutionLayer中的卷积的实现中有一个重要的函数就是im2col以及col2im，im2colnd以及col2imnd。前面的两个函数是二维卷积的正向和逆向过程，而后面的两个函数是n维卷积的正向和逆向过程。1234567891011121314151617181920void BaseConvolutionLayer&lt;Dtype&gt;::forward_cpu_gemm(const Dtype* input, const Dtype* weights, Dtype* output, bool skip_im2col) &#123; const Dtype* col_buff = input; if (!is_1x1_) &#123; if (!skip_im2col) &#123; // 如果没有1x1卷积，也没有skip_im2col // 则使用conv_im2col_cpu对使用卷积核滑动过程中的每一个kernel大小的图像块 // 变成一个列向量，形成一个height=kernel_dim_ // width = 卷积后图像heght*卷积后图像width conv_im2col_cpu(input, col_buffer_.mutable_cpu_data()); &#125; col_buff = col_buffer_.cpu_data(); &#125; for (int g = 0; g &lt; group_; ++g) &#123; caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, conv_out_channels_ / group_, conv_out_spatial_dim_, kernel_dim_, (Dtype)1., weights + weight_offset_ * g, col_buff + col_offset_ * g, (Dtype)0., output + output_offset_ * g); &#125;&#125; 参考资料 caffe代码阅读10：Caffe中卷积的实现细节 数据集生成数据集的均值文件这里计算图像的均值使用的是pper_image_mean方法，在natural images上训练的时候，这种方式比较好，以imagenet数据集为例，caffe在使用imagenet数据集的时候需要计算均值文件，详细见 python-caffe caffe-blob【Caffe代码解析】Blobcaffe源码分析–Blob类代码研究Caffe源码解析1：Blob 结构体分析Blob 是Caffe作为数据传输的媒介，无论是网络权重参数，还是输入数据，都是转化为Blob数据结构来存储，网络，求解器等都是直接与此结构打交道的。 4纬的结构体（包含数据和梯度)，其4维结构通过shape属性得以计算出来. 成员变量 123456protected: shared_ptr&lt;SyncedMemory&gt; data_;// 存放数据 shared_ptr&lt;SyncedMemory&gt; diff_;//存放梯度 vector&lt;int&gt; shape_; //存放形状 int count_; //数据个数 int capacity_; //数据容量 成员函数 12345678910111213141516171819202122232425262728293031323334 const Dtype* cpu_data() const; //cpu使用的数据 void set_cpu_data(Dtype* data); //用数据块的值来blob里面的data。 const Dtype* gpu_data() const; //返回不可更改的指针，下同 const Dtype* cpu_diff() const; const Dtype* gpu_diff() const; Dtype* mutable_cpu_data(); //返回可更改的指针，下同 Dtype* mutable_gpu_data(); Dtype* mutable_cpu_diff(); Dtype* mutable_gpu_diff(); int offset(const int n, const int c = 0, const int h = 0,const int w = 0) const// 通过n,c,h,w 4个参数来计算一维向量的偏移量。Dtype data_at(const int n, const int c, const int h,const int w) const//通过n,c,h,w 4个参数来来获取该向量位置上的值。Dtype diff_at(const int n, const int c, const int h,const int w) const//同上inline const shared_ptr&lt;SyncedMemory&gt;&amp; data() const &#123; CHECK(data_); return data_; //返回数据，不能修改 &#125;inline const shared_ptr&lt;SyncedMemory&gt;&amp; diff() const &#123; CHECK(diff_); return diff_; //返回梯度，不能修改 &#125;Reshape(...)//reshape 有多种多态的实现，可以是四个数字，长度为四的vector，其它blob等。if (count_ &gt; capacity_) &#123; capacity_ = count_; data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype))); diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype))); &#125;//当空间不够的时候，需要扩大容量，reset。 函数名中带mutable的表示可以对返回的指针内容进行修改。 caffe学习资料收集深度学习Caffe系列教程集合 caffe代码阅读1：blob的实现细节-2016.3.14 甘宇飞 计算机视觉战队 caffe源码简单解析——Layer层 Caffe代码导读（0）：路线图 知乎问题-深度学习caffe的代码怎么读？ Caffe源码解析1：Blob 深度学习大讲堂——深度学习框架Caffe源码解析 Caffe代码夜话1 Caffe源码分析（一）]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>deeplearning</tag>
        <tag>caffe</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零门槛玩坏caffe]]></title>
    <url>%2F2017-05-26%2Fdocker_caffe_gpu_ipython%2F</url>
    <content type="text"><![CDATA[DL如今已经快成为全民玄学了，感觉离民科入侵不远了。唯一的门槛可能是环境不好配，特别是caffe这种依赖数10种其它软件打框架。不过有了docker之后，小学生也能站撸DL了。 从nvidia-docker到docker，需要有这样的操作大致流程如下，入门版通过docker pull一个GPU版本的caffe 的image,然后安装nvidia-docker 和 nvidia-docker-plugin 来映射宿主机的nvidia-driver并通过共享volume的方式来支持容器里面能“看到”宿主机的GPU。进阶版通过curl -s命令列出宿主机的配置显卡配置，并通过docker run的方式来启动。总结完成。纸上得来终觉浅，绝知此事要躬行，光说不练空把式，唯有实践出真知。tensorflow gpu in docker 使用nvidia-docker12sudo nohup nvidia-docker-plugin &gt;/tmp/nvidia-docker.log &amp; 然后nvidia-docker run 使用docker来代替 nvidia-docker1curl -s http://localhost:3476/docker/cli 下面应该是输出：1--device=/dev/nvidiactl --device=/dev/nvidia-uvm --device=/dev/nvidia7 --devic/dev/nvidia6 --device=/dev/nvidia5 --device=/dev/nvidia4 --device=/dev/nvidia3 --device=/dev/nvidia2 --device=/dev/nvidia1 --device=/dev/nvidia0 --volume-driver=nvidia-docker --volume=nvidia_driver_375.39:/usr/local/nvidia:ro 这样其实1sudo docker run -ti `curl -s http://localhost:3476/v1.0/docker/cli` -v /mnt/share:/mnt/share -v /mnt/lustre:/mnt/lustre -v /lib64:/lib64 镜像名 bash 所以如果你想用docker的方式来运行GPU版本 那么你就需要指明你的所有的device信息，如果卸载rc文件里，那么只能这样1234sudo docker run -ti --device=/dev/nvidiactl --device=/dev/nvidia-uvm --device=/dev/nvidia7 --device=/dev/nvidia6 --device=/dev/nvidia5 \--device=/dev/nvidia4 --device=/dev/nvidia3 --device=/dev/nvidia2 --device=/dev/nvidia1 --device=/dev/nvidia0 \--volume-driver=nvidia-docker --volume=nvidia_driver_375.39:/usr/local/nvidia:ro \ -v /mnt/share:/mnt/share -v /mnt/lustre:/mnt/lustre -v /lib64:/lib64 镜像名 bash 当前你也可以有这样风骚的走位1sudo docker run -ti $(ls /dev/nvidia* | xargs -I&#123;&#125; echo &apos;--device=&#123;&#125;&apos;) -v /mnt/share:/mnt/share -v /mnt/lustre:/mnt/lustre -v /lib64:/lib64 镜像名 bash 在镜像里安装ipython notebook，需要这样做把大象装进冰箱分四步，映射端口，开通open-ssh服务器，安装jupyter,配置密码在镜像中执行 映射端口在dock run的时候加-p参数 开通ssh 1sudo apt-get install openssh-server 安装jupyter 12345apt-get update#安装python dev包apt-get install python-dev#安装jupyterpip install jupyter 设置密码分三小步a. 生成jupyter配置文件，这个会生成配置文件.jupyter/jupyter_notebook_config.py 1jupyter notebook --generate-config b. 从密码到ssa密文在命令行输入ipython，进入ipython命令行1234567#使用ipython生成密码In [1]: from notebook.auth import passwdIn [2]: passwd()Enter password: Verify password: Out[2]: &apos;sha1:38a5ecdf288b:c82dace8d3c7a212ec0bd49bbb99c9af3bae076e&apos;` c. 改配置12345#去配置文件.jupyter/jupyter_notebook_config.py中修改以下参数c.NotebookApp.ip=&apos;*&apos; #绑定所有地址c.NotebookApp.password = u&apos;刚才生成的密文也就是sha1:38a5ecdf288b:c82dace8d3c7a212ec0bd49bbb99c9af3bae076e&apos;c.NotebookApp.open_browser = False #启动后是否在浏览器中自动打开，注意F大写c.NotebookApp.port =8888 #指定一个访问端口，默认8888，注意和映射的docker端口对应 然后执行ipython notebook –allow-root就可以在宿主机上用docker里面的环境了，爽歪歪。 把jupyter-notebook装进docker里 参考资料把jupyter-notebook装进docker里tensorflow gpu in docker]]></content>
      <categories>
        <category>深度学习</category>
        <category>虚拟化</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>deeplearning</tag>
        <tag>caffe</tag>
        <tag>nvidia驱动</tag>
        <tag>docker</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[懒人单手配ubuntu大法-快速翻墙装驱动配置开发环境]]></title>
    <url>%2F2017-04-27%2FnvidiaDriverInstall%2F</url>
    <content type="text"><![CDATA[[TOC] 安装Nvidia驱动安装步骤查询NVIDIA驱动首先去官网(http://www.nvidia.com/Download/index.aspx?lang=en-us) 查看适合自己显卡的驱动 安装NVIDIA驱动 安装之前先卸载已经存在的驱动版本： 1sudo apt-get remove --purge nvidia* 若电脑是集成显卡（NVIDIA独立显卡忽略此步骤），需要在安装之前禁止一项： 1sudo service lightdm stop 执行以下指令安装驱动： 123sudo add-apt-repository ppa:xorg-edgers/ppasudo apt-get updatesudo apt-get install nvidia-375 #注意在这里指定自己的驱动版本！ 安装完成之后输入以下指令进行验证： 1sudo nvidia-smi 若列出了GPU的信息列表则表示驱动安装成功。如果没看到，重启再试一下,linux装驱动需要重启才加载吧 可能出的问题 add-apt-repository 命令不存在123sudo apt-get updatesudo apt-get install python-software-propertiessudo apt-get install software-properties-common 然后关掉terminator 输入nvidia-smi 说驱动没装上 重装系统换成英文版ubuntu anaconda安装andaconda会自动安装很多python库和ipython notebook，并且可以提供虚拟机机制，支持多版本python共存。anaconda自动集成了最新版的MKL（math kernel libray）库，这是Intel推出的底层数值计算库。 安装anaconda 在anaconda官网continuum下载64位python3版本 在annaconda下载目录执行命令 1bash Anaconda*.sh anaconda的license文档按q跳过，输入yes确认，按回车使用默认路径 输入yes把anaconda的binary路径加入~/.bashrc anacond的使用用户安装的不同python环境都会被放在目录~/anaconda/envs下 查看已安装环境 1conda info -e anaconda版本 1which conda # 或者 conda -V conda的环境管理 创建一个python2.7的环境 1conda create --name py27 python=2.7 使用activate激活某个环境 12source activate py27 # linux使用此句activate python34 # windows使用此句 激活后，会发现terminal输入的地改成py27，是因为把.bashrc里的path改成python27的路径 若想返回默认的python版本 1source deactivate py27 # 返回原始版本python 删除一个已有的环境 1conda remove --name py27 --all conda的包管理 安装包 1conda install scipy 查看已经安装packages 1conda list 查看某个指定环境的已安装包 1conda list -n py27 查看package信息 1conda search numpy 更新package 1conda update -n py27 numpy 删除package 1conda remove -n py27 numpy 安装cuda先下载cuda然后输入命令进行安装1sudo sh cuda*linux.run --override 启动安装程序，一直按q，输入accept接受条款输入n不安装nvidia图像驱动，之前已经安装过了输入y安装cuda 8.0工具回车确认cuda默认安装路径：/usr/local/cuda-8.0输入y用sudo权限运行安装，输入密码输入y或者n安装或者不安装指向/usr/local/cuda的符号链接输入y安装CUDA 8.0 Samples，以便后面测试 安装cudnn 将下载下来的cudnn-8.0-linux-x64-v5.1.tgz 解压之后，解压后的cuda文件夹先打开里面的include文件夹，在终端输入： 123sudo cp cudnn.h /usr/local/cuda/include/ cd ~/cuda/lib64 sudo cp lib* /usr/local/cuda/lib64/ 继续更新文件链接 1234cd /usr/local/cuda/lib64/ sudo rm -rf libcudnn.so libcudnn.so.5 sudo ln -s libcudnn.so.5.1.10 libcudnn.so.5 sudo ln -s libcudnn.so.5 libcudnn.so 然后设置环境变量 1sudo gedit /etc/profile 在末尾加入 12PATH=/usr/local/cuda/bin:$PATH export PATH 保存之后创建链接文件 1sudo gedit /etc/ld.so.conf.d/cuda.conf 加入 1/usr/local/cuda/lib64 终端下接着输入 1sudo ldconfig 使链接生效 advance profile 工具接口1sudo apt-get install libcupti-dev 安装tensorflow GPU版本 创建一个环境 12conda create -n tensorflowsource activate tensorflow 安装GPU版的tensorflow 1pip install --ignore-installed --upgrade tensorflow的网址 tensorflow的网址 安装tensorlayer安装前需要安装tensorflow.1pip install tensorlayer 安装keraskeras 是一个高度封装的深度学习框架，后端可以是tensorflow,也可以是theno，安装非常简单,安装前需要安装tensorflow 1conda install keras 安装caffecaffe的以来项还是很多的，所以我写了个脚本，把它们一并安了吧12345678910111213141516171819202122#! /bin/bashsudo apt-get install libatlas-base-dev -ysudo apt-get install libprotobuf-dev -ysudo apt-get install libleveldb-dev -ysudo apt-get install libsnappy-dev -ysudo apt-get install libopencv-dev -ysudo apt-get install libboost-all-dev -ysudo apt-get install libhdf5-serial-dev -ysudo apt-get install libgflags-dev -ysudo apt-get install libgoogle-glog-dev -ysudo apt-get install liblmdb-dev -ysudo apt-get install protobuf-compiler -ysudo git clone https://github.com/jayrambhia/Install-OpenCVcd Install-OpenCV/Ubuntu sudo sh dependencies.sh cd 2.4 sudo sh opencv2_4_10.shcd ../../..sudo cp Makefile.config.example Makefile.configmake allsudo echo &apos;/usr/local/cuda/lib64&apos; &gt;&gt; /etc/ld.so.conf.d/caffe.confsudo ldconfig 在make all的时候可能出问题：12compilation terminated.Makefile:575: recipe for target &apos;.build_release/src/caffe/util/hdf5.o&apos; failed 这时候需要 ：在 Makefile.config 中追加 /usr/include/hdf5/serial/ 到 INCLUDE_DIR后面: 在Makefile.config中注释掉：INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/includea下一行加上 INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ 在makefile中把hdf5_hl and hdf5 改成 hdf5_serial_hl and hdf5_serial — LIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_hl hdf5+++ LIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_serial_h 参考iss:4808 docker安装keras/caffe等可以使用别人弄好的，但是在使用GPU的时候有些问题 大杂烩/ 大杂烩镜像 docker与nvidia-dockerubuntu安装docker直接1sudo apt-get install docker.io 安装nvidia-docker12wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.debsudo dpkg -i /tmp/nvidia-docker*.deb &amp;&amp; rm /tmp/nvidia-docker*.deb 测试安装的nvidia-docker nvidia-docker1nvidia-docker run --rm nvidia/cuda nvidia-smi cpu版本caffe可以直接使用bvlc的版本 bvlc/caffe1sudo docker pull bvlc/caffe:cpu GPU版本caffe这个容易出问题，暂时按下不表 ubuntu其他软件安装一键式脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142#!/bin/bashinstall_sougou()&#123;echo &quot;开始安装搜狗输入法&quot;wget -O ~/Downloads/sougoupinyinETC.deb http://pinyin.sogou.com/linux/download.php?f=linux&amp;bit=64 sudo dpkg -i ~/Downloads/sougoupinyinETC.debecho &quot;安装搜狗输入法完成&quot;&#125;hehe()&#123;echo 没安装成&#125;install_chrome()&#123;echo &quot;开始安装chrome&quot;wget -O ~/Downloads/google-chrome.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb sudo dpkg -i ~/Downloads/google-chrome.debecho &quot;安装chrome完成&quot;&#125;install_shadowsocker()&#123;echo &quot;开始安装install_shadowsocker&quot;sudo apt-get install python-pip python-dev build-essentialsudo pip installl pipsudo apt-get install python-m2cryptosudo pip install shadowsockssudo add-apt-repository ppa:hzwhuang/ss-qt5sudo apt-get updatesudo apt-get install shadowsocks-qt5echo &quot;安装shadowsockers安装完成，请打开图形界面进行配置&quot;&#125;install_conda()&#123;echo &quot;开始安装anaconda&quot;bash ~/Downloads/Anaconda*.shecho &quot;安装chrome完成&quot;&#125;install_nvidia()&#123;echo &quot;开始安装nvidiadriver&quot;sudo apt-get remove --purge nvidia*sudo add-apt-repository ppa:xorg-edgers/ppasudo apt-get updatesudo apt-get install nvidia-375sudo nvidia-smiecho &quot;安装shadowsockers安装完成，请确认&quot;&#125;install_cuda()&#123;echo &quot;开始安装cuda,请把安装文件放在downloads下面&quot;echo &quot;输入n不装驱动，输入y安装cuda8工具，回车确认默认安装位置，y使用sudo权限，y创建符号链接，y安装阳历&quot;sudo sh ~/Downloads/cuda*linux.run --overridetar xzvf ~/Downloads/cudnn*tgzcd /usr/local/cuda/lib64/ sudo rm -rf /usr/local/cuda/lib64/libcudnn.so /usr/local/cuda/lib64/libcudnn.so.5 sudo ln -s /usr/local/cuda/lib64/libcudnn.so.5.1.10 /usr/local/cuda/lib64/libcudnn.so.5 sudo ln -s /usr/local/cuda/lib64/libcudnn.so.5 /usr/local/cuda/lib64/libcudnn.sosudo echo PATH=/usr/local/cuda/bin:$PATH &gt;&gt; /etc/profilesudo echo export PATH &gt;&gt; /etc/profilesudo echo /usr/local/cuda/lib64 &gt;&gt; /etc/ld.so.conf.d/cuda.confsudo ldconfigsudo apt-get install libcupti-devecho &quot;安装cuda安装完成，请确认&quot;&#125;ubuntu_etc()&#123;echo &quot;下面准备安装搜狗输入法，是否安装？(y|n)&quot;read anscase $ans in y|Y|yes|Yes) install_sougou ;; n|N|no|No) hehe ;;esacecho &quot;下面准备安装chrome，是否安装？(y|n)&quot;read anscase $ans in y|Y|yes|Yes) install_chrome ;; n|N|no|No) hehe ;;esacecho &quot;下面准备安装shadowsocker，是否安装？(y|n)&quot;read anscase $ans in y|Y|yes|Yes) install_shadowsocker ;; n|N|no|No) hehe ;;esacecho &quot;下面准备安装anaconda，是否安装？(y|n)&quot;read anscase $ans in y|Y|yes|Yes) install_conda ;; n|N|no|No) hehe ;;esacecho &quot;下面准备安装nvidia_driver，是否安装？(y|n)&quot;read anscase $ans in y|Y|yes|Yes) install_nvidia ;; n|N|no|No) hehe ;;esacecho &quot;下面准备安装cuda，是否安装？(y|n)&quot;read anscase $ans in y|Y|yes|Yes) install_cuda ;; n|N|no|No) hehe ;;esac&#125;ubuntu_etc 参考资料《TensorFlow实战》 Anaconda使用总结 Anaconda使用教程（使用Anaconda配置多python开发环境） tensorlayer安装教程中文版 tensorflow安装教程 keras中文教程 keras速查表]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>deeplearning</tag>
        <tag>caffe</tag>
        <tag>nvidia驱动</tag>
        <tag>docker</tag>
        <tag>tensorflow</tag>
        <tag>cuda</tag>
        <tag>anaconda</tag>
        <tag>tensorlayer</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MDNet学习笔记]]></title>
    <url>%2F2017-04-24%2FMDNet-lerning-note%2F</url>
    <content type="text"><![CDATA[[TOC] MDNet:paper-project sitegithub repository作者做的presentationpython版实现 introduction把CNN用在跟踪问题中，最大的问题是数据量太少（1），这样训练出来的网络容易过拟合，之前王乃岩等的做法是用imageNet来训练（2），但是很难保证数据的一致性，所以用视频跟踪的数据来训练,学习这些序列里目标运动的共性（3）。用一个目标可能在某个视频序列里是要跟踪的目标而在另一个环境中就成了背景（4），跟踪问题一个网络只需要分两类：目标和背景，不需要很大的网络(5). MDNet的网络结构是前面一个是共享的CNN网络用来提取图像的特征，在训练的过程中主要训练前面共享网络的参数，而后面的是针对不用视频序列的分支的二值分类器，训练过程调整前面共享网络的参数，使得二值分类器的结果与groundtruth的结果一致。而在跟踪过程中，新建一个后面专用的domain，然后在线训练这块儿的网络。MDNET另一个特点是层数比resnet和alexnet少。 算法包括两个阶段：多域表示学习和在线视觉跟踪，在线视觉跟踪阶段fine-tune的是共享部分的全连接层和后面的分类层。 王乃岩2015年的文章用的方法：Structured output CNN[Wang et al. Arxiv’15] 文章名字叫做： Transferring rich feature hierarchies for robust visual tracking（放在axiv2015上没中）这篇文章是典型的用imagenet训练网络来做追踪的，效果不好的原因是分类与跟踪问题最基本的不同：分类是对目标打标签，而跟踪是定位任意目标的位置。 本文的贡献在于： 提出了一个基于CNN的多域学习框架，把域无关的信息从域相关的信用中分出来用来做共享的表示层。 第一次使用跟踪的数据做训练，效果比较好 早期的CNN方法针对特定目标做跟踪，比如2010年智能跟踪人，而2014年的缺少数据,2015年的王乃岩和HongS[20]使用imagenet效果并不好,hongS表明了更深的网络会丢失一些空间信息，对目标定位不好。multi_domain_learning最早是用在NLP领域的。在视觉中之前只用在一些域自适应的场景中。 MDNet网络结构的离线训练如上面的图所示，包含了共享的层和K个域专用的分支层，黄色和蓝色的包围框表示的分别是正样本和负样本。其中卷积层与VGG-M 网络一致,论文为：M. Danelljan, G. Hager, F. Khan, and M. Felsberg. Accurate scale estimation for robust visual tracking. In BMVC, 2014。 而全连接层的输出为512层，用了relu和dropout。K个分支包含一个二值分类器用softmax做交叉熵损失函数。在共享层学习的是一些共有的东西，比如对光照变化、运动抖动、尺度变化的鲁棒性。在训练网络的过程中每一个iteration中，来自某个branch的数据做minibatch，同时只有这个branch工作来完成此次迭代的训练，整个过程是一个随机梯度下降法的过程。 MDNet的在线跟踪网络更新跟踪控制和网络更新，网络更新有长时更新(使用长时间累积的正样本)和短时更新（用在跟踪出错的时候）的区分，这里不是很懂。 难例挖掘在tracking-by-detection的方法中大部分的负样本是不重要的或者是冗余的，平均低对待这些样本容易造成漂移。[35] Example-based learning for viewbased human face detection 给了难例挖掘的方法，随着训练的进行，*负样本越来越难以被分类。 包围框回归由于CNN的特征抽象层次高并且数据增广策略在选择正样本的时候是在目标的周围选的一些样本，这样会导致最终找到的包围框不能最小的包围到那个目标，所以我们要进行bounding box regression。最早用在目标检测领域,[13,11] DPM算法也这么用。Rich featurehierarchies for accurate object detection and semantic segmentation。 参考资料目标跟踪算法五：MDNet: Learning Multi-Domain Convolutional Neural Networks for Visual Tracking 深度学习在目标跟踪中的应用 目标跟踪之NIUBILITY的相关滤波 cvpr论文阅读笔记 CNN-tracking-文章导读 物体跟踪-CVPR16-tracking 一个人的论文阅读笔记]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>objectTracking</tag>
        <tag>目标跟踪</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标跟踪算法之深度学习方法]]></title>
    <url>%2F2017-04-19%2Fobject-tracking-correlation-filter%2F</url>
    <content type="text"><![CDATA[[TOC] tracking常用的数据库OTB吴毅的两篇论文给出了统一的数据库，在公共平台上进行对比。 Wu Y, Lim J, Yang M H. Online object tracking: A benchmark [C]// CVPR, 2013. Wu Y, Lim J, Yang M H. Object tracking benchmark [J]. TPAMI, 2015. OTB50包括50个序列，后来扩展到OTB100. 2012年以后相关滤波和深度学习崛起了。 VOT 竞赛数据库2013年开始的比赛， Kristan M, Pflugfelder R, Leonardis A, et al. The visual object tracking vot2013 challenge results [C]// ICCV, 2013. Kristan M, Pflugfelder R, Leonardis A, et al. The Visual Object Tracking VOT2014 Challenge Results [C]// ECCV, 2014. Kristan M, Matas J, Leonardis A, et al. The visual object tracking vot2015 challenge results [C]// ICCV, 2015. Kristan M, Ales L, Jiri M, et al. The Visual Object Tracking VOT2016 Challenge Results [C]// ECCV, 2016. OTB和VOT区别： OTB包括25%的灰度序列，但VOT都是彩色序列，这也是造成很多颜色特征算法性能差异的原因； 两个库的评价指标不一样，具体请参考论文； VOT库的序列分辨率普遍较高，这一点后面分析会提到。 最大差别在于：OTB是随机帧开始，或矩形框加随机干扰初始化去跑，作者说这样更加符合检测算法给的框框；而VOT是第一帧初始化去跑，每次跟踪失败(预测框和标注框不重叠)时，5帧之后再次初始化，VOT以short-term为主，且认为跟踪检测应该在一起永不分离，detecter会多次初始化tracker。 目标跟踪算法分类生成模型在当前帧对目标区域建模，下一帧寻找与模型最相似的区域就是预测位置，比较著名的有卡尔曼滤波，粒子滤波，mean-shift等。举个例子，从当前帧知道了目标区域80%是红色，20%是绿色，然后在下一帧，搜索算法就像无头苍蝇，到处去找最符合这个颜色比例的区域，推荐算法ASMS Vojir T, Noskova J, Matas J. Robust scale-adaptive mean-shift for tracking [J]. Pattern Recognition Letters, 2014. 判别模型 （也叫 tracking-by-detection）用深度学习做目标跟踪：Naiyan Wang, Dit-Yan Yeung. Learning a Deep Compact Image Representation for Visual Tracking . Proceedings of Twenty-Seventh Annual Conference on Neural Information Processing Systems (NIPS) 2013 第一篇？ Naiyan Wang, Siyi Li, Abhinav Gupta, Dit-Yan Yeung, Transferring Rich Feature Hierarchies for Robust Visual Tracking. arXiv:1501.04587 2015 Chao Ma（沈春华澳洲学生）, Jia-Bin Huang, Xiaokang Yang and Ming-Hsuan Yang, “Hierarchical Convolutional Features for Visual Tracking”, ICCV 2015Github 主页 https://github.com/jbhuang0604/CF2http://www.cnblogs.com/wangxiaocvpr/p/5925851.html 一篇学习笔记 Lijun Wang, Wanli Ouyang, Xiaogang Wang, Huchuan Lu（卢湖川大连理工）, Visual Tracking with Fully Convolutional Networks, ICCV2015Project site: http://scott89.github.io/FCNT/ Lijun Wang, Wanli Ouyang, Xiaogang Wang, Huchuan Lu ,STCT: Sequentially Training Convolutional Networks for Visual Tracking.cvpr 2016Github repository: https://github.com/scott89/STCT论文笔记：http://blog.csdn.net/u012905422/article/details/52396372王柳军的博客：https://scott89.github.io/ Zhizhen Chi, Hongyang Li, Huchuan Lu, Ming-Hsuan Yang,Dual Deep Network for Visual Tracking. TIP 2017Github repository: https://github.com/chizhizhen/DNT迟至真的博客：http://chizhizhen.github.io/ Seunghoon Hong, Tackgeun You, Suha Kwak and Bohyung Han. Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural NetworkICML - International Conference on Machine Learning, 2015Project site: http://cvlab.postech.ac.kr/research/CNN_SVM/ H. Li, Y. Li, and F. Porikli, DeepTrack: Learning Discriminative Feature Representations by Convolutional Neural Networks for Visual Tracking, BMVC 2014 Hyeonseob Namand Bohyung Han, Learning Multi-Domain Convolutional Neural Networks for Visual TrackingProject site: http://cvlab.postech.ac.kr/research/mdnet/ Heng Fan, Haibin Ling. “SANet: Structure-Aware Network for Visual Tracking.” arXiv (2016).凌海滨个人主页：http://www.dabi.temple.edu/~hbling/ Ran Tao, Efstratios Gavves, Arnold W.M. Smeulders. Siamese Instance Search for Tracking. cvpr2016博客笔记 http://www.dongcoder.com/detail-155114.html Luca Bertinetto , Jack Valmadre , João F. Henriques, Andrea Vedaldi, Philip H.S. Torr, Fully-Convolutional Siamese Networks for Object Tracking, ECCV 2016Project site: http://www.robots.ox.ac.uk/~luca/siamese-fc.htmlAuthor homepage: http://www.robots.ox.ac.uk/~luca/index.htmlhttp://www.robots.ox.ac.uk/~joao/# 牛津大学课题组 CF理论推进者github repository: https://github.com/bertinetto/siamese-fc David Held, Sebastian Thrun, Silvio Savarese,Learning to Track at 100 FPS with Deep Regression Networks, ECCV2016.Stanford课题组Github repository: https://github.com/davheld/GOTURNProject site: http://davheld.github.io/GOTURN/GOTURN.html Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao, Qingming Huang, Jongwoo Lim, Ming-Hsuan Yang. Hedged Deep Tracking. Cvpr2016计算所智能实验室黄庆明组。Project site: https://sites.google.com/site/yuankiqi/hdt/ Guanghan Ning, Zhi Zhang, Chen Huang, Zhihai He, Xiaobo Ren, Haohong Wang, Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking.arxiv: 2016.Github repository: https://github.com/Guanghan/ROLOProject site: http://guanghan.info/projects/ROLO/ 宁广汉的主页 安徽大学phd.王潇的论文笔记：http://www.cnblogs.com/wangxiaocvpr/p/5774840.html王潇的github: https://github.com/wangxiao5791509 用相关滤波做跟踪：第一篇文章：MOSSED. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui. Visual object tracking using adaptive correlation filters. In CVPR, 2010. 瑞典林雪平大学phd三年级学生 Martin Danelljan ECO CCOT作者ECO: Efficient Convolution Operators for Tracking.CVPR2017Learning Continuous Convolution Operators for Visual Tracking.ECCV2016.Github reposityory: https://github.com/martin-danelljan/Continuous-ConvOpProject site: https://theinformationageblog.wordpress.com/2017/01/12/eccv-2016-videos-beyond-correlation-filters-learning-continuous-convolution-operators-for-visual-tracking/ benckmark:吴毅：Annan Li, Min Lin, Yi Wu, Ming-Hsuan Yang and Shuicheng Yan. NUS-PRO: A New Visual Tracking Challenge.PAMI 2016.Project site: https://sites.google.com/site/li00annan/nus-proCNT: Kaihua Zhang, Qingshan Liu, Yi Wu, Minghsuan Yang. “Robust Visual Tracking via Convolutional Networks Without Training.” TIP (2016)TIP:Transaction Image Processing IEEE期刊 王强：在这里可以看到作者收集了很多新的方法：计算所的Github repository: https://github.com/foolwood/benchmark_resultsVOT的使用方法：http://blog.csdn.net/carrierlxksuper/article/details/47054231 Matthias Mueller, Neil Smith, Bernard Ghanem,A Benchmark and Simulator for UAV Tracking.阿卜杜拉国王科技大学做的一个无人机航拍数据集。 Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, Vincent Vanhoucke,YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in VideoProject site: https://research.google.com/youtube-bb/Github repository: https://github.com/mbuckler/youtube-bb 参考资料目标跟踪算法五：MDNet: Learning Multi-Domain Convolutional Neural Networks for Visual Tracking 深度学习在目标跟踪中的应用 目标跟踪之NIUBILITY的相关滤波 cvpr论文阅读笔记 CNN-tracking-文章导读 物体跟踪-CVPR16-tracking 一个人的论文阅读笔记]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>objectTracking</tag>
        <tag>目标跟踪</tag>
        <tag>相关滤波</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习中译本-节选]]></title>
    <url>%2F2017-04-08%2FDL-learningNote2-deeplearning-chinese%2F</url>
    <content type="text"><![CDATA[[TOC] 前言一个人的日常生活需要关于世界的巨量知识。很多这方面的知识是主观的、直观的，因此很难通过形式化的方式表达清楚。计算机需要获取同样的知识才能表现出智能。AI 系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。这种能力被称为 机器学习。 对表示的依赖都是一个普遍现象。在计算机科学中，如果数据集合被精巧地结构化并被智能地索引，那么诸如搜索之类的操作的处理速度就可以成指数级地加快。 使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为 表示学习（representation learning）。学习到的表示往往比手动设计的表示表现得更好。并且它们只需最少的人工干预，就能让AI系统迅速适应新的任务。表示学习算法的典型例子是 自编码器(autoencoder)。 深度学习（deep learning）通过其他较简单的表示来表达复杂表示，解决了表示学习中的核心问题。下图展示了深度学习系统如何通过组合较简单的概念，获得不同层次的抽象（例如转角和轮廓，它们转而由边线定义）来表示图像中人的概念。第一层可以轻易地通过比较相邻像素的亮度来识别边缘。有了第一隐藏层描述的边缘，第二隐藏层可以容易地搜索可识别为角和扩展轮廓的边集合。给定第二隐藏层中关于角和轮廓的图像描述，第三隐藏层可以找到轮廓和角的特定集合来检测特定对象的整个部分。最后，根据图像描述中包含的对象部分，可以识别图像中存在的对象。 深度学习与其他学习的区别。 应用数学与机器学习基础线性代数部分坐标超过两维度的数组称为张量（tensor），一个数组中的元素分布在若干维坐标的规则网络中。 一组向量的生成子空间是原始向量线性组合后所能抵达的点的集合。确定 $ Ax=b$是否有解相当于确定向量b是否在A列向量的生成子空间中。又叫A的值域 如果一个矩阵的列空间涵盖整个 $R^m$，那么该矩阵必须包含至少一组 m 个线性无关的向量。这是式 (2.11) 对于每一个向量 b 的取值都有解的充分必要条件。值得注意的是，这个条件是说该向量集恰好有 m 个线性无关的列向量，而不是至少 m 个。 一个列向量线性相关的方阵被称为 奇异的（singular）。 范数范数（包括 $L^p$ 范数）是将向量映射到非负值的函数。直观上来说，向量 x 的范数衡量从原点到点 x 的距离。比如L2范数衡量的就是欧氏距离 当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用 L1 范数。特别是求梯度的时候。 最大范数$L^{\infty }$，衡量的是向量中具有最大幅值的元素的绝对值：$\left | x \right |_\infty =max_i|x_i|$ 衡量矩阵的大小需要用到Frobenius范数：$$\left | A \right |{F }= \sqrt{ \sum{i,j}A_{i,j}^2} = \sqrt{Tr(AA\top) }$$ .其类似于向量的$L^2$.，F范数可以用于定义最小平方误差。 特殊类型的矩阵和向量对角矩阵是只在主对角线上含有非零元素的矩阵。单位矩阵就是对角阵。对角阵受欢迎主要是因为其乘法计算很高效。计算乘法$ diag(v)x $，只需要把x中的每个元素xi放大vi倍就可以了。并且对角方阵的逆矩阵。 正交矩阵是指行向量和列向量都分别标准正交(都是单位向量然后各自正交)的方阵：$A^\top A = I$ 这以为着 $A^{-1} = A^\top$.所以正交矩阵收到关注是因为求逆代价小。 特征分解特征分解是广泛使用的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值。$Av = \lambda v$ 矩阵分解，假设A有n个线性无关的特征向量{v1,v2,..vn},它们对应的特征值是{$\lambda_1,\lambda_2,…\lambda_n$},把每一个特征向量做一个列向量，然后组合起来形成一个矩阵，然后把相应的特征值组成对角阵。就是特征分解，把特征向量进行归一化（L2范数为1，单位向量），可以记做$ A = Vdiag(\lambda)V^{-1} $因为我们这里只讨论实对称矩阵，所以A的特征向量组成的矩阵就是正交矩阵，所以求逆就是求转置。$ A = Vdiag(\lambda) V^\top$ 矩阵是奇异的，当且仅当含有零特征向量。实对称矩阵的特征分解也可以用于优化二次方程：$f(x)=x^{\top}A x$，其中限制$ \left | x \right |_2 = 1 $， 当$x$是$A$的特征值的时候，$f$将返回对应的特征值，f的最大值是最大的特征值，最小值是最小的特征值。 所有特征值都是正数的矩阵被称为正定，所有特征值都是非负数的矩阵被称为半正定。半正定矩阵受到关注是因为它们保证$x^{\top}Ax&gt;=0$。 奇异值分解奇异值分解，singular value decomposition,SVD，将矩阵分解为奇异向量和奇异值。每个实数矩阵都有奇异值分解，但不一定有特征分解。 $ A = UDV^{\top} $ A是mxn矩阵，则U是mxm正交矩阵，V是nxn正交矩阵 ，D是mxn对角矩阵（不一定是方阵）SVD分解可以把矩阵求逆拓展到非方矩阵上面。 行列式行列式的绝对值可以衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少。如果行列式是0，那么空间至少沿着某一维完全收缩了，使其失去了所有的体积。 概率与信息论概率分布用来描述随机变量在每一个可能取值的可能性的大小。 边缘概率离散变量：$P(X=x) = \sum_y P(X=x,Y=y) $.连续变量：$p(x) = \int p(x,y)dy.$ 条件概率的链式法则：$P(a,b,c) = P(a|b,c)P(b|c)P(c) $ 协方差两个变量的协方差如果是正的，那么两个变量都倾向于同时取得相对较大的值。 中心极限定理表明很多独立随机变量的和近似服从正态分布，正态分布是对模型加入的先验知识量最少的分布。 multinouli分布多努力分布，又叫范畴分布，是指由一个随机变量在多个分类上的分布，与伯努利的区别在于伯努利指的是两个类。伯努利函数的概率用sigmoid函数来预测，而多努力函数是用softmax函数与做预测的。 $ softmax(x)_i = \frac{exp(x_i)}{\sum _{j=1}^nexp(x_j)} $ softmax的理解与应用 常用函数的有用性质logistic sigmoid函数：$ \sigma(x) = \frac{1}{1+exp(-x)} $，在变量的绝对值很大的情况下回出现饱和现象，这时候就会对输入的微小变化不敏感。 另一个经常遇到的函数是softplus函数：$\zeta(x) = log(1+exp(x))$，它的值域是$(0,\infty)$,他是对max函数的平滑max(0,x). 一些性质：$\sigma(x) = \frac{exp(x)}{exp(x)+exp(0)}$$\frac{d}{dx}\sigma(x)=\sigma(x)(1-\sigma(x))$$1-\sigma(x)=\sigma(-x)$$log\sigma(x)=-\zeta(-x)$ 数值计算梯度下降法 鞍点是拐点的一种。其二阶导数为0 jacobian矩阵和hessian矩阵f的Jacobian矩阵定义为 $ J_{i,j} = \frac{\partial }{\partial x_j}f(x)i $当我们的函数有多维输入时，把二阶导数合并成一个矩阵，称为Hessian矩阵。$ H(f)(x){i,j} = \frac{\partial ^2}{\partial x_i \partial x_j} f(x) $ Hessian矩阵大多数都是实对称矩阵。所以可以进行特征分解并写成下面的表达式：$d^\top Hd$,当d是H的特征向量时，表达式的值为d对应的特征值，也就是d这个方向上的二阶导数。当d是H的一个特征向量时，这个方向的二阶导数就是对应的特征值。对于其他的方向d，方向二阶导数就是所有特征值的加权平均，权重在0和1之间，且与d夹角越小的特征向量的权重越大。最大特征值来确定最大二阶导数，最小特征值确定最小二阶导数。 在临界点（一阶偏导都为0处），我们通过检测Hessian的特征值来判断该临界点是一个局部极大点、局部极小点还是鞍点。当Hessian是正定的（所有特征值都是正的），则该临界点是局部极小点。因为方向二阶导数在任何方向都是正的，当Hessian矩阵是负定的，这个点就是局部极大点。如果hessian的特征值中至少一个是正的且至少一个是负的， 参考资料bengio 深度学习中译本 名称 京东报价 李季报价 vCPU：I7 5930k 4299 4850 v主板华硕x99 deluxe II 3999 4350 v内存条：金士顿Fury DDR4 2400 8Gx4 1796 2000 v固态硬盘：三星850EVO 250G M.2接口 669 795 v机械硬盘：希捷2T7200 SATA3 449 515 机箱 ttthernaltake core v51 台式机中塔机箱 649 700 主机电源：海韵x1250w 全模电源 1999 2200 v显卡 titan x pascal 11100 14400 v显示器：戴尔U2414H 1549 1700 键鼠罗技MK520 239 280 其他 157 185 合计 26905 31975 深度学习学习资料爱可可爱生活搬运的cs231N课程 cs231N课程笔记翻译 网友的cs231n课程作业与课程内容回顾 cs231n课程课件 李宏毅2017课程,深度学习偏语音 李宏毅2016课程 UFLDL教程中文版 杨立坤的deeplearning tensorlayer中文版 莫凡 tensorflow tensorfly Neural Networks and Deep Learning中文翻译 一文弄懂神经网络中的反向传播法——BackPropagation 皮果提的深度学习笔记 邹晓艺汇总的深度学习学习资料 Deep Learning（深度学习）学习笔记整理系列之（一）]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DL从入门到放弃]]></title>
    <url>%2F2017-03-20%2FDL-learningNote1%2F</url>
    <content type="text"><![CDATA[[TOC]Yann Lecun, Geoffrey Hinton, Yoshua Bengio, Andrew Ng 参考资料-nature上的DL综述 Conventional machine-learning techniques were limited in their ability to process natural data in their raw form 传统的机器学习不能处理原始形式的自然数据， constructing a pattern-recognition or machine-learning system requiredcareful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector 需要一些工程学和相当领域的专业知识设计特征提取器来把原始数据转为合适的内部表示或者特征向量。 Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. 深度学习使用非线性模块对原始数据进行多层级的转换，得到多层次的数据表示和更高层的抽象，能够学习非常复杂的函数。 learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. The weight vector is then adjusted in the opposite direction to the gradient vector shallow classifiers require a good feature extractor that solves the selectivity–invariance dilemma — one that produces representations that are selective to the aspects of the image that are important for discrimination, but that are invariant to irrelevant aspects 好的特征描述堆无关的方面具有不变性 make classifiers more powerful, one can use generic non-linear features, as with kernel methods20, but generic features such as those arising with the Gaussian kernel do not allow the learner to generalize well far from the training examples 由高斯核产生的通用的非线性特征对远离训练样本的数据扩展性差 A deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning, and many of which compute non-linear input–output mappings. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation 能同时增加模型的区分能力和无关性 multilayer architectures can be trained by simple stochastic gradient descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure 有了反传机制，就能用简单的SGD来训练多层网络架构。反传机制计算目标函数关于多层模型权重的梯度是基于链式法则算的。 The key insight is that the derivative (or gradient)of the objective with respect to the input of a module can be computed by working backwards from the gradient with respect to the output of that module 梯度下降法 目标函数关于模型输入的梯度可以通过关于输出的梯度逆向算出。反正在用损失函数求权重偏导的时候由于激活函数的存在，导致梯度消失. 参考自深度学习如何入门？ - 回答作者: Deeper 梯度下降优化算法综述 An overview of gradient descent optimization algorithms 梯度下降算法是通过沿着目标函数J(θ)参数θ∈ℜ的梯度(一阶导数)相反方向−∇θJ(θ)来不断更新模型参数来到达目标函数的极小值点（收敛），更新步长为η。 随机梯度下降最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动) 波动的特点可能会使得优化的方向从当前的局部极小值点跳到另一个更好的局部极小值点，这样便可能对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于全量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。mini-batch梯度下降常用于神经网络中。 而梯度下降法虽然效果很好但还是存在一些问题 学习速率值的选取较难、 学习速率调整难、 不同参数用不同的学习速率(很少出现的特征的速率要打)、 容易陷入局部极值点 梯度下降优化方法（深度学习）牛顿法也能用，但不能用在深度学习中，这些方法都是替代学习率的 Momentum 动量On the momentum term in gradient descent learning algorithms. 解决SGD在某些极值点附近震荡，导致收敛速度慢，可以让SGD有机会离开局部极值点达到更好的极值点。在更新模型参数时，对于那些当前的梯度方向与上一次梯度方向相同的参数，那么进行加强，即这些方向上更快了；对于那些当前的梯度方向与上一次梯度方向不同的参数，那么进行削减，即这些方向上减慢了。因此可以获得更快的收敛速度与减少振荡。 NAG不仅增加了动量项，并且在计算参数的梯度时，在损失函数中减去了动量项. 假设动量因子参数γ=0.9，首先计算当前梯度项，如上图小蓝色向量，然后加上动量项，这样便得到了大的跳跃，如上图大蓝色的向量。这便是只包含动量项的更新。而NAG首先来一个大的跳跃（动量项)，然后加上一个小的使用了动量计算的当前梯度（上图红色向量）进行修正得到上图绿色的向量。这样可以阻止过快更新来提高响应 上面两个方法每次更新的时候所有的模型参数都使用同一个学习速率，能够根据损失函数的斜率做到自适应更新来加速SGD的收敛。 adagradadagrad能够对每个参数自适应不同的学习速率，对稀疏特征，得到大的学习更新，对非稀疏特征，得到较小的学习更新，因此该优化算法适合处理稀疏特征数据缺点在于需要计算参数梯度序列平方和，并且学习速率趋势是不断衰减最终达到一个非常小的值. adadelta和RMSpropadadelta是对adagrad的一种扩展，为了降低adagrad中学习速率衰减过快问题 Adamadaptive moment estimation 也是不同参数自适应不同学习速率的方法 在鞍点（saddle points）处(即某些维度上梯度为零，某些维度上梯度不为零)，SGD、Momentum与NAG一直在鞍点梯度为零的方向上振荡，很难打破鞍点位置的对称性；Adagrad、RMSprop与Adadelta能够很快地向梯度不为零的方向上转移。 如何选择SGD优化器如果你的数据特征是稀疏的，那么你最好使用自适应学习速率SGD优化方法(Adagrad、Adadelta、RMSprop与Adam)，因为你不需要在迭代过程中对学习速率进行人工调整。adam可能是目前最好的SGD优化方法。 最近很多论文都是使用原始的SGD梯度下降算法，并且使用简单的学习速率退火调整（无动量项）。现有的已经表明：SGD能够收敛于最小值点，但是相对于其他的SGD，它可能花费的时间更长，并且依赖于鲁棒的初始值以及学习速率退火调整策略，并且容易陷入局部极小值点，甚至鞍点 更多的SGD策略Shuffling and Curriculum Learning 训练集随机洗牌与课程学为了使得学习过程更加无偏，应该在每次迭代中随机打乱训练集中的样本。 另一方面，在很多情况下，我们是逐步解决问题的，而将训练集按照某个有意义的顺序排列会提高模型的性能和SGD的收敛性 Batch normalization为了方便训练，我们通常会对参数按照0均值1方差进行初始化，随着不断训练，参数得到不同程度的更新，这样这些参数会失去0均值1方差的分布属性，这样会降低训练速度和放大参数变化随着网络结构的加深。Batch normalization[18]在每次mini-batch反向传播之后重新对参数进行0均值1方差标准化。这样可以使用更大的学习速率，以及花费更少的精力在参数初始化点上。Batch normalization充当着正则化、减少甚至消除掉Dropout的必要性。 Early stopping在验证集上如果连续的多次迭代过程中损失函数不再显著地降低，那么应该提前结束训练 Gradient noise在每次迭代计算梯度中加上一个高斯分布的随机误差 。会增加模型的鲁棒性，即使初始参数值选择地不好，并适合对特别深层次的负责的网络进行训练。其原因在于增加随机噪声会有更多的可能性跳过局部极值点并去寻找一个更好的局部极值点，这种可能性在深层次的网络中更常见。 机器学习中防止过拟合的处理方法stFunction，针对Octave而言，我们可以将这个函数作为参数传入到 fminunc 系统函数中（fminunc 用来求函数的最小值，将@costFunction作为参数代进去，注意 @costFunction 类似于C语言中的函数指针），fminunc返回的是函数 costFunction 在无约束条件下的最小值，即我们提供的代价函数 jVal 的最小值，当然也会返回向量 θ 的 为什么正则化项就可以防止过拟合-邓子明 如果发生了过拟合问题，我们应该如何处理？ 过多的变量（特征），同时只有非常少的训练数据，会导致出现过度拟合的问题。因此为了解决过度拟合，有以下两个办法。 尽量减少 机器学习中防止过拟合的处理方法 在统计学习中，假设数据满足独立同分布（i.i.d，independently and identically distributed），即当前已产生的数据可以对未来的数据进行推测与模拟。但是一般独立同分布的假设往往不成立，即数据的分布可能会发生变化（distribution drift），并且可能当前的数据量过少，不足以对整个数据集进行分布估计，因此往往需要防止模型过拟合，提高模型泛化能力。而为了达到该目的的最常见方法便是：正则化，即在对模型的目标函数（objective function）或代价函数（cost function）加上正则项。 为了防止过拟合，我们需要用到一些方法，如：early stopping、数据集扩增（Data augmentation）、正则化（Regularization）、Dropout等。 early stopping 提早结束Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。Early stopping方法的具体做法是，在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。这种策略也称为“No-improvement-in-n”，n即Epoch的次数，可以根据实际情况取，如10、20、30…… 数据集扩增更多的数据有时候更优秀。但是往往条件有限，如人力物力财力的不足，而不能收集到更多的数据，需要采用一些计算的方式与策略在已有数据集上进行操作，来得到更多的数据。 复制原有数据并加上随机噪声 重采样 根据当前数据集估计数据分布参数，使用该分布产生更多数据等 正则化方法邹晓艺专栏 机器学习中的范数规则化之（一）L0、L1与L2范数 监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。 规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。 一般有L1正则与L2正则最小化损失函数，就用梯度下降，最大化似然函数，就用梯度上升 监督学习可以看做最小化下面的目标函数： !$ w^* = arg min_w \sum_iL(y_i,f(x_i,f(x_ilw)))+\lambda\Omega(w) $ 其中第一项是损失函数，第二项是正则项或者叫惩罚项。 第一项Loss函数，如果是Square loss,那就是最小二乘；如果是Hinge-Loss就是SVM；如果是exp-Loss，那就是boosting了;如果是log-loss，那就是Logistic Regression。 L1正则 L0范数是指向量中非0的元素的个数。L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。价值一个亿的问题：为什么L1范数会使权值稀疏？“它是L0范数的最优凸近似，而且它比L0范数要容易优化求解。”。实际上，还存在一个更美的回答：任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。 参数稀疏有什么好处呢？为什么要稀疏 特征选择 可解释性 L1正则是基于L1范数，在目标函数后面加上参数的L1范数和项，即参数绝对值和!$ C = C_0+\frac{\lambda}{n}\sum_w |W|$其中!$ C_0$ 代表原始的代价函数，n是样本的个数，!$\lambda $就是正则项系数，权衡正则项与!$C_0 $的比重 在计算梯度时，w的梯度变为：!$ \frac{\partial c}{\partial \omega} = \frac{\partial c_0}{\partial \omega}+\frac{\lambda}{n}sgn(\omega) $其中，sgn是符号函数，那么使用下式堆参数进行更新：!$ \omega = \omega + \alpha\frac{\partial c_0}{\partial \omega}+\beta\frac{\lambda}{n}sgn(\omega) $ 在梯度下降法中：!$ \alpha &lt; 0， \beta &lt;0 $ ,所以当w为正时，更新后w会变小，当w为负时更新后w会变大；因此L1正则项是为了使得那些原先处于零（即|w|≈0）附近的参数w往零移动，使得部分参数为零，从而降低模型的复杂度（模型的复杂度由参数决定），从而防止过拟合，提高模型的泛化能力。 L1正则中有个问题，便是L1范数在0处不可导，即|w|在0处不可导，因此在w为0时，使用原来的未经正则化的更新方程来对w进行更新，即令sgn(0)=0!$sgn(w)_{w&gt;0}=1,sgn(w)_{w&lt;0}=−1,sgn(w)_{w=0}=0$ L1正则线性回归即为Lasso回归 L2正则在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。为它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。 L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。为什么越小的参数说明模型越简单？限制了参数很小，实际上就限制了多项式某些分量的影响很小 L2正则是基于L2范数，即在目标函数后面加上参数的L2范数和项，即参数的平方和与参数的积项，即：!$ C = C_0 + \frac{\lambda}{2n}\sum_ww^2 $在计算梯度的时候 ，模型更新为!$ w = w + \alpha\frac{\partial C_0}{\partial \omega} + \frac{\lambda}{n}\sum_ww$ L2正则线性回归即为 Ridge回归，岭回归 L2正则项起到使得参数w变小加剧的效果，但是为什么可以防止过拟合呢？一个通俗的理解便是：更小的参数值w意味着模型的复杂度更低，对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据，从而使得不会过拟合，以提高模型的泛化能力。机器学习中的范数规则化之（一）L0、L1与L2范数 总结机器学习中使用「正则化来防止过拟合」到底是一个什么原理？为什么正则化项就可以防止过拟合？ 比如L2正则，相当于给模型参数W添加了一个协方差为!$ \frac{n}{\lambda} $的零均值高斯分布先验，而对于!$ \lambda=0 $,则表示方差为无穷大，表明模型参数的变化可以很大不受约束。而随着!$\alpha$的增大，模型约束变大，模型就更稳定了。（因为模型参数选择范围小了，模型参数的已知条件变多了，模型复杂度就降低了） L2与L1的区别在于，L1正则是拉普拉斯先验，而L2正则则是高斯先验。它们都是服从均值为0，协方差为1λ。当λ=0时，即没有先验上图中的模型是线性回归，有两个特征，要优化的参数分别是w1和w2，左图的正则化是l2，右图是l1。蓝色线就是优化过程中遇到的等高线，一圈代表一个目标函数值，圆心就是样本观测值（假设一个样本），半径就是误差值，受限条件就是红色边界（就是正则化那部分），二者相交处，才是最优参数。可见右边的最优参数只可能在坐标轴上，所以就会出现0权重参数，使得模型稀疏。 加入正则化是 在bias和variance之间做一个tradeoff. bias就是训练误差呀,variance就是各个特征之间的权值差别了。 作者：刑无刀 dropoutImageNet Classification with Deep Convolutional Neural Networks 通过修改神经网络本身结构来实现的训练开始时，随机得删除一些（可以设定为一半，也可以为1/3，1/4等）隐藏层神经元，即认为这些神经元不存在，同时保持输入层与输出层神经元的个数不变，这样便得到如下的ANN： 按照BP学习算法对ANN中的参数进行学习更新（虚线连接的单元不更新，因为认为这些神经元被临时删除了）。这样一次迭代更新便完成了。下一次迭代中，同样随机删除一些神经元，与上次不一样，做随机选择 激活函数神经网络激励函数的作用是什么？有没有形象的解释 激活函数可以引入非线性因素，解决线性模型所不能解决的问题。 从直觉上讲，SVM的核函数与神经网络的激活函数做了同样的事情，实现非线性分类。但是SVM是在输入层特征处进行核变换做的，而激活函数则是在输出层做的。 深度学习中的激活函数导引 新型激活函数ReLU克服了梯度消失，使得深度网络的直接监督式训练成为可能 激活函数的作用神经网络中激活函数的主要作用是提供网络的非线性建模能力。假设一个示例神经网络中仅包含线性卷积和全连接运算，那么该网络仅能够表达线性映射，即便增加网络的深度也依旧还是线性映射，难以有效建模实际环境中非线性分布的数据。 Sigmoid sigmoid 在定义域内处处可导，且两侧导数逐渐趋近于0。benjo将这类激活函数定义为 软饱和激活函数，饱和就像极限一样，也分为左饱和和右饱和。 常见的 ReLU 就是一类左侧硬饱和激活函数。由于在后向传递过程中，sigmoid向下传导的梯度包含了一个f’(x) 因子（sigmoid关于输入的导数），因此一旦输入落入饱和区，f’(x) 就会变得接近于0，导致了向底层传递的梯度也变得非常小。此时，网络参数很难得到有效训练。这种现象被称为梯度消失。一般来说， sigmoid 网络在 5 层之内就会产生梯度消失现象sigmoid (0, 1) 的输出还可以被表示作概率，或用于输入的归一化 tanh 函数 !$ tanh(x) = 2sigmoid(2x) - 1 $,也具有软饱和性，收敛速度比sigmoid快。 ReLU2006年Hinton教授提出通过逐层贪心预训练解决深层网络训练困难的问题，但是深度网络的直接监督式训练的最终突破，最主要的原因是采用了新型激活函数ReLU与传统的sigmoid激活函数相比，ReLU能够有效缓解梯度消失问题，从而直接以监督的方式训练深度神经网络 ReLU 在!$x&lt;0$ 时硬饱和。由于 !$x&gt;0$时导数为 1，所以，ReLU 能够在!$x&gt;0$时保持梯度不衰减，从而缓解梯度消失问题。但随着训练的推进，部分输入会落入硬饱和区，导致对应权重无法更新。这种现象被称为“神经元死亡”。 ReLU还经常被“诟病”的一个问题是输出具有偏移现象7，即输出均值恒大于零。偏移现象和 神经元死亡会共同影响网络的收敛性。 还有 PReLU 、maxout、elu等，不同的激活函数搭配不同的参数初始化策略 深度学习学习资料UFLDL教程中文版 杨立坤的deeplearning 莫凡 tensorflow tensorfly Neural Networks and Deep Learning中文翻译 一文弄懂神经网络中的反向传播法——BackPropagation 一个浙大直博生推荐的学习资料 一个外文的tensorflow入门教程 别人的论文笔记 皮果提的深度学习笔记 邹晓艺汇总的深度学习学习资料 Deep Learning（深度学习）学习笔记整理系列之（一） 深度学习论文集]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>梯度下降法</tag>
        <tag>正则化</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TLD算法学习]]></title>
    <url>%2F2017-01-10%2FTLD_algorithm%2F</url>
    <content type="text"><![CDATA[[TOC] 2010年发表的论文《Tracking-Learning-Detection》 , GitHub上有很多C++版本的TLD，比如arthurv，注释比较详细，但速度很慢。 对openTLD的注释 TLD算法的构成TLD算法主要由三个模块构成：追踪器（tracker），检测器（detector）和机器学习（learning）。TLD算法成功的原因就在于它将检测器和跟踪器有机的整合在一起，从而实现了长线跟踪。 TLD是对视频中未知物体的长时间跟踪的算法。“未知物体”指的是任意的物体，在开始追踪之前不知道哪个物体是目标。“长时间跟踪”又意味着需要算法实时计算，在追踪中途物体可能会消失再出现，而且随着光照、背景的变化和由于偶尔的部分遮挡，物体在像素上体现出来的“外观”可能会发生很大的变化。从这几点要求看来，单独使用追踪器或检测器都无法胜任这样的工作。所以作者提出把追踪器和检测器结合使用，同时加入机器学习来提高结果的准确度。 12![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-f518840d2338852d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 追踪器的作用是跟踪连续帧间的运动，当物体始终可见时跟踪器才会有效。追踪器根据物体在前一帧已知的位置估计在当前帧的位置，这样就会产生一条物体运动的轨迹，从这条轨迹可以为学习模块产生正样本（Tracking-&gt;Learning）。 检测器的作用是估计追踪器的误差，如果误差很大就改正追踪器的结果。 检测器对每一帧图像都做全面的扫描，找到与目标物体外观相似的所有位置，从检测产生的结果中产生正样本和负样本，交给学习模块（Detection-&gt;Learning）。 算法从所有正样本中选出一个最可信的位置作为这一帧TLD的输出结果，然后用这个结果更新追踪器的起始位置（Detection-&gt;Tracking） 学习模块根据追踪器和检测器产生的正负样本，迭代训练分类器，改善检测器的精度（Learning-&gt;Detection）。 追踪模块TLD使用作者自己提出的Median-Flow光流追踪算法，采用的是Lucas-Kanade追踪器 作者假设一个“好”的追踪算法应该具有正反向连续性（forward-backward consistency），即无论是按照时间上的正序追踪还是反序追踪，产生的轨迹应该是一样的。作者根据这个性质规定了任意一个追踪器的FB误差（forward-backward error）：从时间t的初始位置x(t)开始追踪产生时间t+p的位置x(t+p)，再从位置x(t+p)反向追踪产生时间t的预测位置x`(t)，初始位置和预测位置之间的欧氏距离就作为追踪器在t时间的FB误差。 跟踪点的选择前面提到TLD跟踪的不是关键点，它跟踪的是更简单的点：能稳定存在的点，那哪些点是稳定的呢？Median-Flow tracker的基本思想是，看反向跟踪后的残差，用所有点的残差中值作为稳定点的筛选条件。如上图中的黄色点就因为残差太大，被pass掉了，既然稳定点是可以筛选出来的，那么就不必煞费苦心的寻找那些关键点，可以直接将所有的点都作为初始跟踪点，好吧所，有的点毕竟还是太多了，于是作者是选取网格交叉点作为初始跟踪点（见下图框框中黄色的点点）。 在上一帧t的物体包围框里均匀地产生一些点，然后用Lucas-Kanade追踪器正向追踪这些点到t+1帧，再反向追踪到t帧，计算FB误差，筛选出FB误差最小的一半点作为最佳追踪点。最后根据这些点的坐标变化和距离的变化计算t+1帧包围框的位置和大小（平移的尺度取中值，缩放的尺度取中值。取中值的光流法，估计这也是名称Median-Flow的由来吧） 可以用NCC（Normalized Cross Correlation，归一化互相关）和SSD（Sum-of-Squared Differences，差值平方和）作为筛选追踪点的衡量标准。(都是越小越好) NCC: SSD: 学习模块P-N学习是一种半监督的机器学习算法，它针对检测器对样本分类时产生的两种错误提供了两种“专家”进行纠正： P专家（P-expert）：检出漏检（false negative，正样本误分为负样本）的正样本； N专家（N-expert）：改正误检（false positive，负样本误分为正样本）的正样本。 样本的产生算法已经确定物体在t+1帧的位置（实际上是确定了相应包围框的位置），从检测器产生的包围框中筛选出10个与它距离最近的包围框（两个包围框的交的面积除以并的面积大于0.7），对每个包围框做微小的仿射变换（平移10%、缩放10%、旋转10°以内），产生20个图像元，这样就产生200个正样本。再选出若干距离较远的包围框（交的面积除以并的面积小于0.2），产生负样本。这样产生的样本是已标签的样本，把这些样本放入训练集，用于更新分类器的参数。 作者认为，算法的结果应该具有“结构性”：每一帧图像内物体最多只出现在一个位置；相邻帧间物体的运动是连续的，连续帧的位置可以构成一条较平滑的轨迹。比如像上图c图那样每帧只有一个正的结果，而且连续帧的结果构成了一条平滑的轨迹，而不是像b图那样有很多结果而且无法形成轨迹。还应该注意在整个追踪过程中，轨迹可能是分段的，因为物体有可能中途消失，之后再度出现。 P专家(修正专家)的作用是寻找数据在时间上的结构性，它利用追踪器的结果预测物体在t+1帧的位置。如果这个位置（包围框）被检测器分类为负，P专家就把这个位置改为正。也就是说P专家要保证物体在连续帧上出现的位置可以构成连续的轨迹； N专家(挑一专家)的作用是寻找数据在空间上的结构性，它把检测器产生的和P专家产生的所有正样本进行比较，选择出一个最可信的位置，保证物体最多只出现在一个位置上，把这个位置作为TLD算法的追踪结果。同时这个位置也用来重新初始化追踪器。 比如在这个例子中，目标车辆是下面的深色车，每一帧中黑色框是检测器检测到的正样本，黄色框是追踪器产生的正样本，红星标记的是每一帧最后的追踪结果。在第t帧，检测器没有发现深色车，但P专家根据追踪器的结果认为深色车也是正样本，N专家经过比较，认为深色车的样本更可信，所以把浅色车输出为负样本。第t+1帧的过程与之类似。第t+2帧时，P专家产生了错误的结果，但经过N专家的比较，又把这个结果排除了，算法仍然可以追踪到正确的车辆。 检测模块检测模块使用一个级联分类器，对从包围框boundingbox获得的样本进行分类。级联分类器包含三个级别： 图像元方差分类器（Patch Variance Classifier）。计算图像元像素灰度值的方差，把方差小于原始图像元方差一半的样本标记为负。论文提到在这一步可以排除掉一半以上的样本。 集成分类器（Ensemble Classifier）。实际上是一个随机蕨分类器（Random Ferns Classifier），类似于随机森林（Random Forest），区别在于随机森林的树中每层节点判断准则不同，而随机蕨的“蕨”中每层只有一种判断准则。所以这个分类器其实不怎么行。 如上图所示，把左面的树每层节点改成相同的判断条件，就变成了右面的蕨。所以蕨也不再是树状结构，而是线性结构。随机蕨分类器根据样本的特征值判断其分类。从图像元中任意选取两点A和B，比较这两点的像素值，若A的像素大于B，则特征值为1，否则为0。每选取一对新位置，就是一个新的特征值。蕨的每个节点就是对一对像素点进行比较。 比如取5对点，红色为A，蓝色为B，样本图像经过含有5个节点的蕨，每个节点的结果按顺序排列起来，每个节点表示一个特征。得到长度为5的二进制序列01011，转化成十进制数字11。这个11就是该样本经过这个蕨得到的结果。 同一类的很多个样本经过同一个蕨，得到了该类结果的分布直方图。高度代表类的先验概率p(F|C)，F代表蕨的结果（如果蕨有s个节点，则共有1+2^s种结果）。 不同类的样本经过同一个蕨，得到不同的先验概率分布。 以上过程可以视为对分类器的训练。当有新的未标签样本加入时，假设它经过这个蕨的结果为00011（即3），然后从已知的分布中寻找后验概率最大的一个。由于样本集固定时，右下角公式的分母是相同的，所以只要找在F=3时高度最大的那一类，就是新样本的分类。只用一个蕨进行分类会有较大的偶然性。另取5个新的特征值就可以构成新的蕨。用很多个蕨对同一样本分类，投票数最大的类就作为新样本的分类，这样在很大程度上提高了分类器的准确度。 最近邻分类器（Nearest Neighbor Classifier）。计算新样本的相对相似度，如大于0.6，则认为是正样本。相似度规定如下：图像元pi和pj的相似度，公式里的N是规范化的相关系数，所以S的取值范围就在[0,1]之间， PN学习半监督学习所以，检测器是追踪器的监督者，因为检测器要改正追踪器的错误；而追踪器是训练检测器时的监督者，因为要用追踪器的结果对检测器的分类结果进行监督。用另一段程序对训练过程进行监督，而不是由人来监督，这也是称P-N学习为“半监督”机器学习的原因。 TLD的工作流程如下图所示。首先，检测器由一系列包围框产生样本，经过级联分类器产生正样本，放入样本集；然后使用追踪器估计出物体的新位置，P专家根据这个位置又产生正样本，N专家从这些正样本里选出一个最可信的，同时把其他正样本标记为负；最后用正样本更新检测器的分类器参数，并确定下一帧物体包围框的位置。 TLD源码理解TLD算法成功的原因就在于它将检测器和跟踪器有机的整合在一起，从而实现了长线跟踪。 程序的运行方式1./run_tld -p ../parameters.yml -s ../datasets/06_car/car.mpg -b ../datasets/06_car/init.txt –r -p 后面跟的是初始化参数 -s 后面的是人工视频的位置 -b 是初始化boundingbox的位置 程序用readBB来读取初始化的bounding box 程序初始化过程在run_tld.cpp的main函数里面 进行了配置文件parameters.yml的读取， buildGrid(frame1, box);检测器采用扫描窗口的策略：扫描窗口步长为宽高的 10%，尺度缩放系数为1.2；此函数构建全部的扫描窗口grid，并计算每一个扫描窗口与输入的目标box的重叠度；重叠度定义为两个box的交集与它们的并集的比； 为各种变量或者容器分配内存空间； getOverlappingBoxes(box, num_closest_init);此函数根据传入的box（目标边界框），在整帧图像中的全部扫描窗口中（由上面4.1得到）寻找与该box距离最小（即最相似，重叠度最大）的num_closest_init（10）个窗口，然后把这些窗口归入good_boxes容器。同时，把重叠度小于0.2的，归入bad_boxes容器；相当于对全部的扫描窗口进行筛选。并通过BBhull函数得到这些扫描窗口的最大边界。 classifier.prepare(scales);准备分类器，scales容器里是所有扫描窗口的尺度，由上面的buildGrid()函数初始化； 这是一种典型的特征比较简单，分类器比较复杂的例子. TLD的分类器有三部分：方差分类器模块、集合分类器模块和最近邻分类器模块；这三个分类器是级联的，每一个扫描窗口依次全部通过上面三个分类器，才被认为含有前景目标。这里prepare这个函数主要是初始化集合分类器模块； 集合分类器（随机森林）基于n个基本分类器（共10棵树），每个分类器（树）都是基于一个pixel comparisons（共13个像素比较集）的，也就是说每棵树有13个判断节点（组成一个pixel comparisons），输入的图像片与每一个判断节点（相应像素点）进行比较，产生0或者1，然后将这13个0或者1连成一个13位的二进制码x（有2^13种可能），每一个x对应一个后验概率P(y|x)= #p/(#p+#n) （也有2^13种可能），#p和#n分别是正和负图像片的数目。那么整一个集合分类器（共10个基本分类器）就有10个后验概率了，将10个后验概率进行平均，如果大于阈值（一开始设经验值0.65，后面再训练优化）的话，就认为该图像片含有前景目标；用的是最简单的blending的组合方法。 后验概率P(y|x)= #p/(#p+#n)的产生方法：初始化时，每个后验概率都得初始化为0；运行时候以下面方式更新：将已知类别标签的样本（训练样本）通过n个分类器进行分类，如果分类结果错误，那么相应的#p和#n就会更新，这样P(y|x)也相应更新了。 pixel comparisons的产生方法：先用一个归一化的patch去离散化像素空间，产生所有可能的垂直和水平的pixel comparisons，然后我们把这些pixel comparisons随机分配给n个分类器，每个分类器得到完全不同的pixel comparisons（特征集合），这样，所有分类器的特征组统一起来就可以覆盖整个patch了。 特征是相对于一种尺度的矩形框而言的，TLD中第s种尺度的第i个特征features[s][i] = Feature(x1, y1, x2, y2); 是两个随机分配的像素点坐标 （就是由这两个像素点比较得到0或者1的） 。计算特征的方法就是求patch在这两个点上的像素的大小。每一种尺度的扫描窗口都含有 totalFeatures = nstructs * structSize个特征 ；nstructs为树木 （由一个特征组构建，每组特征代表图像块的不同视图表示）的个数；structSize为每棵树的特征个数，也即每棵树的判断节点个数；树上每一个特征都作为一个决策节点； prepare函数的工作就是先给每一个扫描窗口初始化了对应的pixel comparisons（两个随机分配的像素点坐标）；然后初始化后验概率为0； generatePositiveData(frame1, num_warps_init);此函数通过对第一帧图像的目标框box（用户指定的要跟踪的目标）进行仿射变换来合成训练初始分类器的正样本集。具体方法如下：先在距离初始的目标框最近的扫描窗口内选择10个bounding box（已经由上面的getOverlappingBoxes函数得到，存于good_boxes里面了，还记得不？），然后在每个bounding box的内部，进行±1%范围的偏移，±1%范围的尺度变化，±10%范围的平面内旋转，并且在每个像素上增加方差为5的高斯噪声（确切的大小是在指定的范围内随机选择的），那么每个box都进行20次这种几何变换，那么10个box将产生200个仿射变换的bounding box，作为正样本。具体实现如下：1getPattern(frame(best_box), pEx, mean, stdev); 此函数将frame图像best_box区域的图像片归一化为均值为0的15*15大小的patch，存于pEx（用于最近邻分类器的正样本）正样本中（最近邻的box的Pattern），该正样本只有一个。1generator(frame, pt, warped, bbhull.size(), rng); 此函数属于PatchGenerator类的构造函数，用来对图像区域进行仿射变换，先RNG一个随机因子，再调用（）运算符产生一个变换后的正样本。1classifier.getFeatures(patch, grid[idx].sidx, fern); 函数得到输入的patch的特征fern（13位的二进制代码）；1pX.push_back(make_pair(fern, 1)); //positive ferns &lt;features, labels=1&gt; 然后标记为正样本，存入pX（用于集合分类器的正样本）正样本库； 以上的操作会循环 num_warps good_boxes.size()即20 10 次，这样，pEx就有了一个正样本，而pX有了200个正样本了； meanStdDev(frame1(best_box), mean, stdev);统计best_box的均值和标准差，var = pow(stdev.val[0],2) * 0.5;作为方差分类器的阈值。 generateNegativeData(frame1);由于TLD仅跟踪一个目标，所以我们确定了目标框了，故除目标框外的其他图像都是负样本，无需仿射变换；具体实现如下： 由于之前重叠度小于0.2的，都归入 bad_boxes了，所以数量挺多，把方差大于var 0.5f的bad_boxes（使得负样本很丰富）都加入负样本，同上面一样，需要classifier.getFeatures(patch, grid[idx].sidx, fern);和nX.push_back(make_pair(fern, 0));得到对应的fern特征和标签的nX负样本（用于集合分类器的负样本）； 然后随机在上面的bad_boxes中取bad_patches（100个）个box，然后用 getPattern函数将frame图像bad_box区域的图像片归一化到15*15大小的patch，存在nEx（用于最近邻分类器的负样本）负样本中。 这样nEx和nX都有负样本了；（box的方差通过积分图像计算用于方差检测器） 然后将nEx的一半作为训练集nEx，另一半作为测试集nExT；同样，nX也拆分为训练集nX和测试集nXT； 将负样本的特征nX和正样本特征pX合并到ferns_data[]中，用于集合分类器的训练； 将上面得到的一个正样本pEx和nEx合并到nn_data[]中，用于最近邻分类器的训练； 训练这两个训练方法就是简单的 模板匹配法，集合分类器训练的是后验概率，而最近邻分类器训练的是啥 用上面的样本训练集训练集合分类器（森林） 和 最近邻分类器：classifier.trainF(ferns_data, 2); //bootstrap = 2 对每一个样本ferns_data[i] ，如果样本是正样本标签， 先用measure_forest函数 返回该 样本所有树的所有特征值对应的后验概率累加值，该累加值如果小于正样本阈值（0.6 nstructs，表示平均值需要大于0.6（0.6 nstructs / nstructs）,0.6是程序初始化时定的集合分类器的阈值，为经验值，后面会用测试集来评估修改，找到最优），同时用update函数更新后验概率。 classifier.trainNN(nn_data); 对每一个样本nn_data，如果标签是正样本，通过NNConf(nn_examples[i], isin, conf, dummy);计算输入图像片与在线模型之间的相关相似度conf，如果相关相似度小于0.65 ，则认为其不含有前景目标，也就是分类错误了；这时候就把它加到正样本库。然后就通过pEx.push_back(nn_examples[i]);将该样本添加到pEx正样本库中；同样，如果出现负样本分类错误，就添加到负样本库。 分类器评价 ？用测试集在上面得到的 集合分类器（森林） 和 最近邻分类器中分类，评价并修改得到最好的分类器阈值。 classifier.evaluateTh(nXT, nExT); 对集合分类器，对每一个测试集nXT，所有基本分类器的后验概率的平均值如果大于thr_fern（0.6），则认为含有前景目标，然后取最大的平均值（如果大于thr_fern）作为该集合分类器的新的阈值。 对最近邻分类器，对每一个测试集nExT，最大相关相似度如果大于nn_fern（0.65），则认为含有前景目标，然后取最大的最大相关相似度（如果大于nn_fern）作为该最近邻分类器的新的阈值。 处理视频1processFrame(last_gray, current_gray, pts1, pts2, pbox, status, tl, bb_file); 逐帧读入图片序列，进行算法处理。processFrame共包含四个模块（依次处理）：跟踪模块、检测模块、综合模块和学习模块； 跟踪模块http://docs.opencv.org/3.1.0/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323 其中normCrossCorrelation其中normCrossCorrelation(img1,img2,points1,points2)是对光流法跟踪的结果不放心，因此希望通过对比前后两点周围的小块的相似性，来进一步去掉不稳定的点。这次的相似性不是相关系数，而是normalized cross-correlation (NCC)： 模板匹配 matchTemplateopencv document matchTemplate 这个是opnecv中的一个函数 模板匹配是一项在一幅图像中寻找与另一幅模板图像最匹配(相似)部分的技术. 需要2幅图像:原图像 (I): 在这幅图像里,我们希望找到一块和模板匹配的区域模板 (T): 将和原图像比照的图像块目标是检测最匹配的区域: 通过 滑动, 图像块一次移动一个像素 (从左往右,从上往下). 在每一个位置, 都进行一次度量计算来表明它是 “好” 或 “坏” 地与那个位置匹配 (或者说块图像和原图像的特定区域有多么相似).对于 T 覆盖在 I 上的每个位置,你把度量值 保存 到 结果图像矩阵 (R) 中. 在 R 中的每个位置 (x,y) 都包含匹配度量值(以此点开始的块的相似度) opencv提供的模板匹配相似度计算方法有6类 标准相关匹配 method=CV_TM_CCORR_NORMED 参考资料TLD2010年论文Tracking-Learning-Detection.pdf?ich_u_r_i=9dc166e19827cc6c08b5afa9f474c60f&amp;ich_s_t_a_r_t=0&amp;ich_e_n_d=0&amp;ich_k_e_y=1745018919750763292479&amp;ich_t_y_p_e=1&amp;ich_d_i_s_k_i_d=4&amp;ich_u_n_i_t=1) Forward-Backward Error: Automatic Detection of Tracking Failures 计算机视觉、机器学习相关领域论文和源代码大集合(持续更新) TLD源码分析 庖丁解牛TLD（一）——开篇 这个讲得比较清楚 —— TLD之学习篇（四）]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标跟踪</tag>
        <tag>代码</tag>
        <tag>TLD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[再见啦2016]]></title>
    <url>%2F2016-12-29%2Fgoodbye2016%2F</url>
    <content type="text"><![CDATA[2016年结束啦，回想年初的时候，H老师对我说：“去年让你受了不少委屈，新的方向就是这样，今年就不会这样了”。这让我多少对今年有所规划。然而我却从未预测到今年会发生这么多事，让我在长达数周的时间里处于精神深度崩溃之中，让我狂躁不安。最终这一切随着时间尘埃落定，成为了这令人感慨万千的2016。 真的是日久见人心，你永远也想不到一个刚开始看到乞丐躺在天桥上还要表示关切的傻白甜，最终表现出来的是什么样子。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KCF算法-1论文学习]]></title>
    <url>%2F2016-12-28%2Fkcf_theory%2F</url>
    <content type="text"><![CDATA[KCF全称为Kernel Correlation Filter 核相关滤波算法，是在2014年提出的，算法在跟踪效果和跟踪速度上都有十分亮眼的表现。算法主页有论文和代码可供下载。opencv3.1开始嵌入KCF算法 相关滤波算法算是判别式跟踪，主要是使用给出的样本去训练一个判别分类器，判断跟踪到的是目标还是周围的背景信息。使用轮转矩阵对样本进行采集，使用快速傅里叶变化对算法进行加速计算。 相关（correlation filter）应用在tracking方面的最为直观的方法：相关就是衡量两个信号相似值。如果两个信号相关值越大，就表明这两个信号的越相似。在tracking的应用当中，我们的目的就是设计这么一个模版，使得当模版和输入的图片目标做相关时，能在目标的中心位置得到最大的响应 由卷积定理的correlation版本可以知道，函数互相关傅里叶变换等于函数傅里叶变换的乘积。如公式②所示。F(h)★f=（F(h)）*⊙F(f)——————–②而已知FFT变换的时间复杂度为O(nlogn),由此可知，②式的时间复杂度也为O(nlogn)。明白为什么相关滤波类型的tracking algorithm的计算时间比较快 论文大致内容负样本对训练一个分类器是一个比较重要的存在，但是在训练的时候负样本的数量是比较少的，所以我们本文的算法就是为了更加方便地产生更多的样本，以便于我们能够训练一个更好的分类器。 而Correlation Filter应用于tracking方面最朴素的想法就是：相关是衡量两个信号相似值的度量，如果两个信号越相似，那么其相关值就越高，而在tracking的应用里，就是需要设计一个滤波模板，使得当它作用在跟踪目标上时，得到的响应最大，最大响应值的位置就是目标的位置。 CSK（论文下载地址）是这个算法改进的初级版本，这篇是主要引进了循环矩阵生成样本，使用相关滤波器进行跟踪，本篇KCF是对CSK进行更进一步的改进，引进了多通道特征，可以使用比着灰度特征更好的HOG（梯度颜色直方图）特征或者其他的颜色特征等。 提出了一个快速的效果良好的跟踪算法，把以前只能用单通道的灰度特征改进为现在可以使用多通道的HOG特征或者其他特征，而且在现有算法中是表现比较好的，使用HOG替换掉了灰度特征，对实验部分进行了扩充，充分证明自己的效果是比较好的。使用核函数，对偶相关滤波去计算。 循环移位 cyclic shifts循环矩阵是对图像进行平移，这样可以增加样本的个数，训练出的分类器效果比较好 一维的情况下就是矩阵想乘的问题了，就是矩阵分析当中学过的左乘一个单位矩阵和右乘一个单位矩阵。左乘是行变换，右乘列变化。目的就是得到更多的样本，每乘一次都是一个新的样本，这样的话就可以多出来n*n个样本了，这就是循环矩阵在这里最大的用处，制造样本的数量 循环矩阵的计算可以直接把所有的样本都转换为对角矩阵进行处理，因为在循环矩阵对样本进行处理的时候，样本并不是真实存在的样本，存在的只是虚拟的样本，可以直接利用循环矩阵所特有的特性，直接把样本矩阵转换为对角矩阵进行计算， 这样可以大幅度加快矩阵之间的计算，因为对角矩阵的运算只需要计算对角线上非零元素的值即可。 目标跟踪评价标准Visual tracker benchmark是一个相当于比较标准一般的存在，可以对比每个算法在相同的基准之下的性能。这个在CVPR 2013的时候发表的，下面是原文链接：Online Object Tracking: A Benchmark （CVPR 2013) 官方链接：数据集 在2015年的时候数据集由原来的50个序列扩充到现在的100个。现在多数使用的是Benchmark V1.0来运行测试所有tracking的结果，在官网下载完代码之后，里面已经存在了关于各种tracker在不同数据集上的结果了。 想要验证自己的tracker在这个基准上的结果，说来非常的简单。直接的方法：首先将代码先拷到benchmark_v1.0/tackers/这个文件夹下，你会发现里面已有好几个算法的代码文件夹了。 注意把代码拷贝进去之后要自己写一个调用函数，benchmark在运行的时候调用我们的算法的函数，就是每个tracker文件夹当中的run_trackers名字。 参考资料Visual Object Tracking using Adaptive Correlation Filters 论文笔记 目标跟踪算法——KCF入门详解 KCF学习（1）-理论 【目标跟踪】KCF高速跟踪详解 kcf hog源码 kcf跟踪算法学习笔记 2014新跟踪算法KCF笔记 kcf论文集]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标跟踪</tag>
        <tag>论文</tag>
        <tag>kcf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视频智能之——目标跟踪]]></title>
    <url>%2F2016-12-27%2Fobject_tracking%2F</url>
    <content type="text"><![CDATA[单目标跟踪是指：给出目标在跟踪视频第一帧中的初始状态（如位置，尺寸），自动估计目标物体在后续帧中的状态。跟踪过程中会出现目标发生剧烈形变、被其他目标遮挡或出现相似物体干扰等等各种复杂的情况 cvpr2013的论文 Online Object Tracking: A Benchmark 总结了大量的算法 目标跟踪的benchmark视频目标跟踪领域benchmark的视频 以往方法小结 压缩跟踪 compressive tracking 粒子滤波 Particle Filter Object Tracking 别的还有什么质心算法之类的完全没法用，压缩跟踪会好一些，粒子滤波只是指跟踪的过程中帧与帧之间的更跟方法，具体使用中还要配合颜色直方图的特征或者其他特征进行相似度计算。压缩跟踪的话应该指的是一套完整的跟踪过程 这是个很有意思的地方，在很多时候，我们之所以需要跟踪算法，是因为我们的检测算法很慢，跟踪很快。基本上当前排名前几的跟踪算法都很难用在这样的情况下，因为你实际的速度已经太慢了，比如TLD，CT，还有Struct，如果目标超过十个，基本上就炸了。况且还有些跟踪算法自己drift掉了也不知道，比如第一版本的CT是无法处理drift的问题的，TLD是可以的，究其原因还是因为检测算法比较鲁棒啊…… 实际中速度极快，实现也简单的纯跟踪算法居然是NCC和Overlap。 这两年有相关滤波系列的跟踪器，速度与精度都相当给力 TLD跟踪算法PAMI2011 30fps左右 TLD（Tracking-Learning-Detection）学习与源码理解之 Tracking-Learning-Detection原理分析 TLD（Tracking-Learning-Detection）学习与源码理解 压缩感知追踪算法ECCV2012 60fps左右，压缩感知矩阵很重要。压缩跟踪Compressive Tracking综述 压缩跟踪Compressive Tracking KCF tracker(csk tracker)ECCV PAMI都有发，是FPS可以做到300fps, 论文笔记：目标追踪-CVPR2014-Adaptive Color Attributes for Real-time Visual Tracking CSK tracker 有很多改进，主要原因是速度快融合自适应颜色信息的改进算法有：CVPR2014 adaptive color attributes for real time visual tracking.pdf 融合part-based信息的能够抵抗部分遮挡的改进算法有：cvpr2015 real-time part-based visual tracking via adaptive corelation filters.pdf 目标跟踪算法——KCF入门详解 其他算法时空上下文视觉跟踪（STC）算法的解读与代码复现 最简单的目标跟踪（模版匹配） 基于感知哈希算法的视觉目标跟踪 深度学习大讲堂参考内容经典目标跟踪方法目前跟踪算法可以被分为产生式(generative model)和判别式(discriminative model)两大类别。产生式方法着眼于对目标本身的刻画，忽略背景信息，在目标自身变化剧烈或者被遮挡时容易产生漂移。 判别式方法通过训练分类器来区分目标和背景。这种方法也常被称为tracking-by-detection。近年来，各种机器学习算法被应用在判别式方法上，其中比较有代表性的有多示例学习方法(multiple instance learning), boosting和结构SVM(structured SVM)等。判别式方法因为显著区分背景和前景的信息。值得一提的是，目前大部分深度学习目标跟踪方法也归属于判别式框架。 基于相关滤波(correlation filter)的跟踪方法因为速度快,效果好吸引了众多研究者的目光。相关滤波器通过将输入特征回归为目标高斯分布来训练 filters。并在后续跟踪中寻找预测分布中的响应峰值来定位目标的位置。相关滤波器在运算中巧妙应用快速傅立叶变换获得了大幅度速度提升。目前基于相关滤波的拓展方法也有很多，包括核化相关滤波器(kernelized correlation filter, KCF), 加尺度估计的相关滤波器(DSST)等。 基于深度学习的目标跟踪方法深度模型的魔力之一来自于对大量标注训练数据的有效学习，而目标跟踪仅仅提供第一帧的bounding-box作为训练数据。这种情况下，在跟踪开始针对当前目标从头训练一个深度模型困难重重。目前基于深度学习的目标跟踪算法采用了几种思路来解决这个问题。 利用辅助图片数据预训练深度模型，在线跟踪时微调在目标跟踪的训练数据非常有限的情况下，使用辅助的非跟踪训练数据进行预训练，获取对物体特征的通用表示(general representation )，在实际跟踪时，通过利用当前跟踪目标的有限样本信息对预训练模型微调(fine-tune), 使模型对当前跟踪目标有更强的分类性能，这种迁移学习的思路极大的减少了对跟踪目标训练样本的需求，也提高了跟踪算法的性能。 这个方面代表性的作品有DLT和SO-DLT。 DLT(NIPS2013) Learning a Deep Compact Image Representation for Visual Tracking DLT作为第一个将深度网络运用于单目标跟踪的跟踪算法，首先提出了“离线预训练＋在线微调”的思路，很大程度的解决了跟踪中训练样本不足的问题 SO-DLT(arXiv2015)Transferring Rich Feature Hierarchies for Robust Visual Tracking SO-DLT作为large-scale CNN网络在目标跟踪领域的一次成功应用，取得了非常优异的表现.但是SO－DLT离线预训练依然使用的是大量无关联图片，作者认为使用更贴合跟踪实质的时序关联数据是一个更好的选择。 利用现有大规模分类数据集预训练的CNN分类网络提取特征2015年以来，在目标跟踪领域应用深度学习兴起了一股新的潮流。即直接使用ImageNet这样的大规模分类数据库上训练出的CNN网络如VGG-Net获得目标的特征表示，之后再用观测模型(observation model)进行分类获得跟踪结果。这种做法既避开了跟踪时直接训练large-scale CNN 样本不足的困境，也充分利用了深度特征强大的表征能力。 FCNT(ICCV15)Visual Tracking with Fully Convolutional Networks 作为应用CNN特征于物体跟踪的代表作品，FCNT的亮点之一在于对ImageNet上预训练得到的CNN特征在目标跟踪任务上的性能做了深入的分析,并根据分析结果设计了后续的网络结构。 Hierarchical Convolutional Features for Visual Tracking(ICCV15) 这篇是作者在2015年度看到的最简洁有效的利用深度特征做跟踪的论文。其主要思路是提取深度特征，之后利用相关滤波器确定最终的bounding-box。 分类任务预训练的CNN网络本身更关注区分类间物体，忽略类内差别。目标跟踪时只关注一个物体，重点区分该物体和背景信息，明显抑制背景中的同类物体，但是还需要对目标本身的变化鲁棒。分类任务以相似的一众物体为一类，跟踪任务以同一个物体的不同表观为一类，使得这两个任务存在很大差别，这也是两篇文章融合多层特征来做跟踪以达到较理想效果的动机所在。 利用跟踪序列预训练，在线跟踪时微调 MDNet(CVPR2016)Learning Multi-Domain Convolutional Neural Networks for Visual Tracking 意识到图像分类任务和跟踪之间存在巨大差别，MDNet提出直接用跟踪视频预训练CNN获得general的目标表示能力的方法。但是序列训练也存在问题，即不同跟踪序列跟踪目标完全不一样，某类物体在一个序列中是跟踪目标，在另外一个序列中可能只是背景。不同序列中目标本身的表观和运动模式、环境中光照、遮挡等情形相差甚大。这种情况下，想要用同一个CNN完成所有训练序列中前景和背景区分的任务，困难重重。 速度仍较慢。且boundingbox回归也需要单独训练. RTT(CVPR16)Recurrently Target-Attending Tracking 这篇文章的出发点比较有意思，即利用多方向递归神经网络(multi-directional recurrent neural network)来建模和挖掘对整体跟踪有用的可靠目标部分(reliable part)，实际上是二维平面上的RNN建模，最终解决预测误差累积和传播导致的跟踪漂移问题 参考资料深度学习在目标跟踪中的应用-深度学习大讲堂 译文 Online Object Tracking: A Benchmark]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>目标跟踪</tag>
        <tag>深度学习</tag>
        <tag>论文</tag>
        <tag>kcf</tag>
        <tag>ct</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视频智能之——目标检测]]></title>
    <url>%2F2016-12-26%2Fobject_detection%2F</url>
    <content type="text"><![CDATA[2016年的CVPR会议目标检测（在这里讨论的是2D的目标检测，如图1所示）的方法主要是基于卷积神经网络的框架，代表性的工作有ResNet[1]（Kaiming He等）、YOLO[5]（Joseph Redmon等）、LocNet[7]（Spyros Gidaris等）、HyperNet[3]（Tao Kong等）、ION[2]（Sean Bell等）、G-CNN[6]（Mahyar Najibi等） CNN目标检测框架RCNN早期，使用窗口扫描进行物体识别，计算量大。RCNN去掉窗口扫描，用聚类方式，对图像进行分割分组，得到多个侯选框的层次组。 原始图片通过Selective Search提取候选框，约有2k个 侯选框缩放成固定大小 经过CNN 经两个全连接后，分类 基于R-CNN的物体检测论文分析 后面的fast RCNN去掉重复计算，并微调选框位置。 faster RCNN使用CNN来预测候选框。 RCNN系列(RCNN、Fast RCNN、Faster RCNN)中，网络由两个子CNN构成。在图片分类中，只需一个CNN，效率非常高。 YOLOFaster RCNN需要对20k个anchor box进行判断是否是物体，然后再进行物体识别，分成了两步。 YOLO则把物体框的选择与识别进行了结合，一步输出，即变成”You Only Look Once”。You Only Look Once: Unified, Real-Time Object Detection SSDYOLO在 7×7 的框架下识别物体，遇到大量小物体时，难以处理。SSD则在不同层级的feature map下进行识别，能够覆盖更多范围。 SSD: Single Shot MultiBox Detector CVPR2016 目标检测问题检测指标目标检测中，以下几个指标非常重要： 识别精度 识别效率 定位准确性 较好的工作常常在某个指标上有所提高 识别精度目标检测中衡量检测精度的指标mAP(mean average precision)。简单来讲就是在多个类别的检测中，每一个类别都可以根据recall和precision绘制一条曲线，那么AP就是该曲线下的面积，而mAP是多个类别AP的平均值，这个值介于0到1之间，且越大越好。具有代表性的工作是ResNet、ION和HyperNet。 ResNetResNet：何凯明的代表作之一，获得了今年的best paper。 文章不是针对目标检测来做的，但其解决了一个最根本的问题：更有力的特征。检测时基于Faster R-CNN的目标检测框架，使用ResNet替换VGG16网络可以取得更好的检测结果。 IONION（inside-outside-network）：这个工作的主要贡献有两个，第一个是如何在Fast R-CNN的基础之上增加context信息，所谓context在目标检测领域是指感兴趣的ROI周围的信息，可以是局部的，也可以是全局的。为此，作者提出了IRNN的概念，这也就是outside-network。第二个贡献是所谓skip-connection，通过将deep ConvNet的多层ROI特征进行提取和融合，利用该特征进行每一个位置的分类和进一步回归，这也就是inside-network。 HyperNetHyperNet：文章的出发点为一个很重要的观察：神经网络的高层信息体现了更强的语义信息，对于识别问题较为有效；而低层的特征由于分辨率较高，对于目标定位有天然的优势，而检测问题恰恰是识别+定位，因此作者的贡献点在于如何将deep ConvNet的高低层特征进行融合，进而利用融合后的特征进行region proposal提取和进一步目标检测。不同于Faster R-CNN，文章的潜在Anchor是用类似于BING[4]的方法通过扫描窗口的方式生成的，但利用的是CNN的特征，因此取得了更好的性能。 通过以上的改进策略，HyperNet可以在产生大约100个region proposal的时候保证较高的recall，同时目标检测的mAP相对于Fast R-CNN也提高了大约6个百分点。 识别效率YOLOYOLO：这是今年的oral。这个工作在识别效率方面的优势很明显，可以做到每秒钟45帧图像，处理视频是完全没有问题的。YOLO最大贡献是提出了一种全新的检测框架——直接利用CNN的全局特征预测每个位置可能的目标，相比于R-CNN系列的region proposal+CNN 这种两阶段的处理办法可以大大提高检测速度。 G-CNNG-CNN：不管是Fast R-CNN[9]，还是Faster R-CNN，或者像HyperNet这样的变种，都需要考虑数以万计的潜在框来进行目标位置的搜索，这种方式的一个潜在问题是负样本空间非常大，因此需要一定的策略来进行抑制（不管是OHEM[8]（难例挖掘）还是region proposal方法，其本质上还是一种抑制负样本的工作）。G-CNN从另一个角度来克服这个问题。G-CNN在在初始化的时候不需要那么多框，而是通过对图像进行划分（有交叠），产生少量的框（大约180个），通过一次回归之后得到更接近物体的位置。然后以回归之后的框作为原始窗口，不断进行迭代回归调整，得到最终的检测结果。 经过五次调整之后，G-CNN可以达到跟Fast R-CNN相当的识别性能，但速度是Fast R-CNN的5倍（3fps）。 定位准确性在目标检测的评价体系中，有一个参数叫做IoU，简单来讲就是模型产生的目标窗口和原来标记窗口的交叠率。在Pascal VOC中，这个值为0.5。，定位越准确，其得分越高，这也侧面反映了目标检测在评价指标方面的不断进步。 如何产生更准确的目标位置呢？LocNet的解决方案是：针对每一个给定的初始框进行适当的放大，然后用一个CNN的网络回归出这个放大后的框包含的那个正确框的位置。为了达到这个目标，需要定义回归方式，网络以及模型，具体的细节参见[7]。 经过把原始的框（比如selective search生成的）进行再一次回归之后，再放入Fast R-CNN进行检测，效果还是挺惊人的。 相关论文[1] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition. In CVPR 2016 [2] Bell S, Zitnick C L, Bala K, et al. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In CVPR 2016 [3] Kong T, Yao A, Chen Y, et al. HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection. In CVPR 2016 [4] Cheng M M, Zhang Z, Lin W Y, et al. BING: Binarized normed gradients for objectness estimation at 300fps. In CVPR 2014 [5] Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection. In CVPR 2016 [6] Najibi M, Rastegari M, Davis L S. G-CNN: an Iterative Grid Based Object Detector. In CVPR 2016 [7] Gidaris S, Komodakis N. LocNet: Improving Localization Accuracy for Object Detection. In CVPR 2016 [8] Shrivastava A, Gupta A, Girshick R. Training region-based object detectors with online hard example mining. In CVPR 2016 [9] Girshick R. Fast R-CNN. In ICCV 2015 [10] Ren S, He K, Girshick R, et al. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS 2015 [11] Liu W, Anguelov D, Erhan D, et al. SSD: Single Shot MultiBox Detector[J]. arXiv preprint arXiv:1512.02325, 2015. 参考资料对话CVPR2016：目标检测新进展 目标检测-大神 Detection CNN 之 “物体检测” 篇 深度学习（十八）基于R-CNN的物体检测-CVPR 2014]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动作识别]]></title>
    <url>%2F2016-12-25%2Faciton_recognition%2F</url>
    <content type="text"><![CDATA[从CVPR 2014看计算机视觉领域的最新热点 深度学习热潮爆发以来，诸多研究者希望能够把它应用于解决计算机视觉的各种任务上，从高层次（high- level）的识别（recognition），分类（classification）到低层次（low-level）的去噪（denoising）。让人不禁联想起当年的稀疏表达（sparse representation）的热潮，而深度学习如今的风靡程度看上去是有过之而无不及。深度学习也有横扫high-level问题的趋势，high- level的很多方向都在被其不断刷新着数据。 尚未被深度学习渗透的Low-level Vision 计算机视觉与深度学习计算机视觉的问题可以根据他们的研究对象和目标分成三大类，low- level，mid-level, 和high-level。 Low-level问题主要是针对图像本身及其内在属性的分析及处理，比如判断图片拍摄时所接受的光照，反射影响以及光线方向，进一步推断拍摄物体的几何结构；再如图片修复，如何去除图片拍摄中所遇到的抖动和噪声等不良影响。 High-level问题主要是针对图像内容的理解和认知层面的，比如说识别与跟踪图像中的特定物体与其行为；根据已识别物体的深入推断，比如预测物体所处的场景和即将要进行的行为。 Mid-level是介于以上两者之间的一个层面，个人理解是着重于特征表示，比如说如何描述high-level问题中的目标物体，使得这种描述有别于其他的物体。可以大致认为，low-level的内容可以服务于mid-level的问题，而mid-level的内容可以服务于high-level的问题。由于这种分类不是很严格，所以也会出现交叉的情况。 深度学习在计算机视觉界主要是作为一种特征学习的工具，可以姑且认为是mid-level的。所以之前提到的high- level的问题受深度学习的影响很大就是这个原因。相比较而言low-level问题受到深度学习的冲击会小很多，当然也有深度学习用于去噪（denoise）和去模糊（deblur）等low-level问题的研究。对于受到深度学习良好表现困扰的年轻研究者们，也不妨来探寻low- level很多有意思的研究。这些年，MIT的Bill Freeman组就做了一些很有趣的low-level问题，比如放大视频中出现的肉眼难以察觉的细小变化（Eulerian Video Magnification for Revealing Subtle Changes in the World），还有这次CVPR的文章Camouflaging an Object from Many Viewpoints就是讲如何在自然环境中放置和涂染一个立方体，让其产生变色龙般的隐藏效果。诸如此类的研究也让研究这件事变得有趣和好玩。 视频智能内容参考资料 基于Deep Learning 的视频识别技术 人工智能在视频上的应用主要一个课题是视频理解，努力解决“语义鸿沟”的问题，其中包括了 视频结构化分析：即是对视频进行帧、镜头、场景、故事等分割，从而在多个层次上进行处理和表达。 目标检测和跟踪：如车辆跟踪，多是应用在安防领域。 人物识别：识别出视频中出现的人物。 动作识别：Activity Recognition， 识别出视频中人物的动作。 情感语义分析：即观众在观赏某段视频时会产生什么样的心理体验。 四位一体分析 object action scene geometry 短视频、直播视频中大部分承载的是人物+场景+动作+语音的内容信息，如上图所示，如何用有效的特征对其内容进行表达是进行该类视频理解的关键。 传统的手工特征有一大堆，目前效果较好的是iDT(Improved Dense Trajectories) ，在这里就不加讨论了。 深度学习对图像内容的表达能力十分不错，在视频的内容表达上也有相应的方法。下面介绍最近几年主流的几种技术方法。 知乎问答DT行为识别常用哪种特征提取？ - 回答作者: Yongcheng Jing http://zhihu.com/question/41068341/answer/102114782 。 一种分类方式是将其分为两大类，一大类是基于局部描述子的statistical information的方法，像HOG3D等。这一类中目前知道的比较好的是Dense Trajectory（DT）方法，作者在这个方法上下了很大功夫，由DT方法在ICCV,CVPR,IJCV上发表了好几篇文章（文章名字都很像，都是讲DT这一个东西的，只是做了一些改进，像15年的文章里面考虑了相机抖动、用Fisher encoding代替Bag of feature以取得更好效果等），源码有提供LEAR - Improved Trajectories Video Description，我在JHMDB数据集上做过实验，效果还不错。另一大类是基于pose的行为识别方法，pose可以提取更加细节的信息，先用pose estimation方法估计pose，再从pose中提取各个关节之间的角度、距离等等作为特征。但受pose estimation准确率的影响，目前这种方法不是很常用，但实验发现在理想的pose estimation情况下这种方法准确率是很高的（高于DT），所以可能是未来行为识别领域的一个发展趋势，源码见http://jhmdb.is.tue.mpg.de/challenge/JHMDB/datasets。另外，在选择实验数据集的时候可以参考12年的CVPR Tutorial，里面详细介绍了目前的开源数据集以及截止12年各个数据集的分类准确率。 IDT 与deep learning行为识别(action recognition)有哪些论文适合入门？ - 回答作者: Xiaolong Wang http://zhihu.com/question/33272629/answer/60279003 有关action recognition in videos, ideo里主流的： Deep Learning之前最work的是INRIA组的Improved Dense Trajectories(IDT) + fisher vector, paper and code:LEAR - Improved Trajectories Video Description基本上INRIA的东西都挺work 恩.. 然后Deep Learning比较有代表性的就是VGG组的2-stream:其实效果和IDT并没有太大区别，里面的结果被很多人吐槽难复现，我自己也试了一段时间才有个差不多的数字。 然后就是在这两个work上面就有很多改进的方法，目前的state-of-the-art也是很直观可以想到的是xiaoou组的IDT+2-stream: 还有前段时间很火，现在仍然很多人关注的G社的LSTM+2-stream: 然后安利下zhongwen同学的paper:http://www.cs.cmu.edu/~zhongwen/pdf/MED_CNN.pdf 最后你会发现paper都必需和IDT比，然后很多还会把自己的method和IDT combine一下说有提高 恩.. 静态图像的动作识别作者：水哥链接：https://www.zhihu.com/question/33272629/answer/60163859来源：知乎 视频方面的不了解，可以聊一聊静态图像下的~12345678910111213[1] Action Recognition from a Distributed Representation of Pose and Appearance, CVPR,2010[2] Combining Randomization and Discrimination for Fine-Grained Image Categorization, CVPR,2011[3] Object and Action Classification with Latent Variables, BMVC, 2011[4] Human Action Recognition by Learning Bases of Action Attributes and Parts, ICCV, 2011[5] Learning person-object interactions for action recognition in still images, NIPS, 2011[6] Weakly Supervised Learning of Interactions between Humans and Objects, PAMI, 2012[7] Discriminative Spatial Saliency for Image Classification, CVPR, 2012[8] Expanded Parts Model for Human Attribute and Action Recognition in Still Images, CVPR, 2013[9] Coloring Action Recognition in Still Images, IJCV, 2013[10] Semantic Pyramids for Gender and Action Recognition, TIP, 2014[11] Actions and Attributes from Wholes and Parts, arXiv, 2015[12] Contextual Action Recognition with R*CNN, arXiv, 2015[13] Recognizing Actions Through Action-Specific Person Detection, TIP, 2015 在2010年左右的这几年（11,12）主要的思路有3种： 以所交互的物体为线索（person-object interaction），建立交互关系，如文献5,6； 建立关于姿态（pose）的模型，通过统计姿态（或者更广泛的，部件）的分布来进行分类，如文献1,4，还有个poselet上面好像没列出来，那个用的还比较多； 寻找具有鉴别力的区域（discriminative），抑制那些meaningless 的区域，如文献2,7。10和11也用到了这种思想。 文献9,10都利用了SIFT以外的一种特征：color name，并且描述了在动作分类中如何融合多种不同的特征。文献12探讨如何结合上下文（因为在动作分类中会给出人的bounding box）。比较新的工作都用CNN特征替换了SIFT特征（文献11,12,13），结果上来说12是最新的。 静态图像中以分类为主，检测的工作出现的不是很多，文献4,13中都有关于检测的工作。可能在2015之前分类的结果还不够promising。现在PASCAL VOC 2012上分类mAP已经到了89%，以后的注意力可能会更多地转向检测。 视频的个别看过几篇，与静态图像相比，个人感觉最大的区别在于特征不同。到了中层以后，该怎么做剩下的处理，思路还是差的不远。 论文阅读人体动作行为识别研究综述_李瑞峰 CVPR 2014 Tutorial Action recognition with bag-of-featuresCVPR 2014 Tutorial on Emerging Topics in Human Activity Recognition 研究点视频种类：监控视频 电影剪辑 运动场拍 第一视觉视频动作级别分类：姿势-&gt;动作-&gt;人物交互(击球)-&gt;人人交互(握手 推 拥抱)-&gt;群体动作 监控视频： 动作 (Laptev 05) 人物交互(Oh et al. 11) 人人交互[Ryoo and Aggarwal 09][Vahdat, Gao,Ranjbar, Mori11] 群体动作[Ryoo and arwal 08,11][Lan, Wang, Yang, Mori 10] 问题所在 动作在表现中各有不同 人工收集训练样本是很难的 动作词典难以描述 怎么计算两个动作之间的差距 都是open 开门 和 开瓶盖 不一样 研究方法局部特征和特征袋表示 local features and bag of features representations 相关归类论文 Wang, et al., “ Action Recognition by Dense Trajectories”, In CVPR’11. Jain et al., “Better exploiting motion for better action recognition”, In CVPR’13. Wang and Schmid, “Action Recognition with Improved Trajectories“, In ICCV’13. Kantorov and Laptev, “Efficient feature extraction, encoding and classification for action recognition “, In CVPR’14. DeepPose: Human Pose Estimation via Deep Neural Networks, Toshev and Szegedy,CVPR 2014. Mixing Body-Part Sequences for Human Pose Estimation, Cherian,Mairal, Alahari,Schmid, CVPR 2014. shape 形状估计法 背景差分 其实对内部结构变化估计不好 motion 运动 用光流估计运动以此来估计动作 局部特征法： 不需要图像分割 不需要目标检测和追踪 缺少全局的结构信息 bag of features 动作识别法 [Laptev, Marszałek, Schmid, Rozenfeld 2008] 特征提取-&gt; 时空patches -&gt; 特征描述 -&gt; 特征量化 -&gt; 直方图 -&gt; 非线性 SVM 与卡方核 总结结论 中层动作表示 相关归类论文 Liu et al., “Recognizing Human Actions by Attributes”, In CVPR’11 Discovering discriminative action parts from mid-level video representations Raptis, Kokkinos, and Soatto, CVPR 2012. Sadanand and Corso. “Action bank: A high-level representation of activity in video.” In CVPR’12. Representing Videos using Mid-level Discriminative Patches Jain, Gupta, Rodriguez and Davis, CVPR 2013 使用CNN的方法 Large-scale Video Classification using Convolutional Neural Networks Karpathy, Shetty, Toderici, Sukthankar,Leung and Fei-Fei, CVPR 2014 总结问题 可以：定位运动的部分 标注转动 不如state of the art 的方法 Fisher Vector + Improved dense trajectories 有可能被CNNs取代 动作时序结构 temporal structure of action 相关归类论文 Modeling Temporal Structure of Decomposable Motion Segments for Activity Classication, J.C. Niebles, C.-W. Chen and L. Fei-Fei, ECCV 2010 Learning Latent Temporal Structure for Complex Event Detection.Kevin Tang, Li Fei-Fei and Daphne Koller, CVPR 2012 Poselet Key-framing: A Model for Human Activity Recognition.Raptis and Sigal, In CVPR 20132012 Scenario-based video event recognition by constraint flow. Kwak, Han and Han, In CVPR 2011 Amer et al., Monte Carlo Tree Search for Scheduling Activity Recognition, In ICCV 2013. 总结问题 可以对子运动进行时域分析 群体行为识别 Human-human interactions群体识别的应用意义 相关资料CVPR 2014 Tutorial on Emerging Topics in Human Activity Recognition cvpr2016的tutorials]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文</tag>
        <tag>动作识别</tag>
        <tag>视频</tag>
      </tags>
  </entry>
</search>
